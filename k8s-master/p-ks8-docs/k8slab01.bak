=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.13 11:29:29 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Server refused our key
root@172.16.102.22's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Mon Sep 12 09:40:33 2022 from 192.168.200.220
[root@master01 ~]# [root@master01 ~]# kubelet get ctl get nodes
NAME                           STATUS   ROLES           AGE   VERSION
master01.airtel.internal.lab   Ready    control-plane   38h   v1.25.0
worker01.airtel.internal.lab   Ready    <none>          38h   v1.25.0
worker02.airtel.internal.lab   Ready    <none>          38h   v1.25.0
[root@master01 ~]# kubectl get nodespods
No resources found in default namespace.
[root@master01 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS      AGE
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (38h ago)   38h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (38h ago)   38h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (38h ago)   38h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-bk8v6                                       1/1     Running   0             38h
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-vpx59                                       1/1     Running   0             38h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   weave-net-42gjg                                        2/2     Running   3 (38h ago)   38h
kube-system   weave-net-44k9t                                        2/2     Running   1 (38h ago)   38h
kube-system   weave-net-mdv4h                                        2/2     Running   0             38h
[root@master01 ~]# [root@master01 ~]# [root@master01 ~]# kubectl sece exec -it admin-pod --dry-run=clinetent busyboxrun
error: required flag(s) "image" not set
[root@master01 ~]# kubectl run admin-pod --dry-run=client busybox-busybox-busyboxibusyboxmbusyboxabusyboxgbusyboxebusybox=busybox --name=
pod/admin-pod created (dry run)
[root@master01 ~]# kubectl get pod
No resources found in default namespace.
[root@master01 ~]# kubectl get pod a----all-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS      AGE
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (38h ago)   38h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (38h ago)   38h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (38h ago)   38h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-bk8v6                                       1/1     Running   0             38h
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-vpx59                                       1/1     Running   0             38h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   weave-net-42gjg                                        2/2     Running   3 (38h ago)   38h
kube-system   weave-net-44k9t                                        2/2     Running   1 (38h ago)   38h
kube-system   weave-net-mdv4h                                        2/2     Running   0             38h

[root@master01 ~]# kubectl run admin-pod --image=busybox --dry-run=client -o yaml

[root@master01 ~]# vim admin-pod.yaml  

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: admin-pod
  name: admin-pod
spec:
  containers:
  - image: busybox
    name: admin-pod
    command:
    - sleep
    - "3600"
    securityContext:capabilites:add: ["SYS_TIME"]
status: {}

[root@master01 ~]# vim admin-pod.yaml 
[root@master01 ~]# kubectl create -f admin.pod.yaml
pod/admin-pod created
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
admin-pod                          1/1     Running   0          17s
web-project-268-84b655ccd9-jpwnz   1/1     Running   0          9m25s
[root@master01 ~]# [root@master01 ~]# logout

[root@master01 ~]# kubectl delete -f web-project-268.yaml 
deployment.apps "web-project-268" deleted

[root@master01 ~]# [root@master01 ~]# kubectl create deployment --image=nginx:1.16 
deployment.apps/web-project-268 created

[root@master01 ~]# kubectl get deployment.app
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
web-project-268   1/1     1            1           24s
[root@master01 ~]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
admin-pod                          1/1     Running   0          42m
web-project-268-6f74fdbb64-xgzd9   1/1     Running   0          34s

[root@master01 ~]# kubectl describe web-project-268
Name:                   web-project-268
Namespace:              default
CreationTimestamp:      Tue, 13 Sep 2022 12:20:40 +0530
Labels:                 app=web-project-268
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=web-project-268
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=web-project-268
  Containers:
   nginx:
    Image:        nginx:1.16
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-project-268-6f74fdbb64 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m9s  deployment-controller  Scaled up replica set web-project-268-6f74fdbb64 to 1
  
[root@master01 ~]# kubectl set image deployment web-project0-268 ngincx=nginx.1.17 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-project-268 image updated

[root@master01 ~]# kubectl describe deploymnet web-project-268
Name:                   web-project-268
Namespace:              default
CreationTimestamp:      Tue, 13 Sep 2022 12:20:40 +0530
Labels:                 app=web-project-268
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment web-project-268 nginx=nginx:1.17 --record=true
Selector:               app=web-project-268
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=web-project-268
  Containers:
   nginx:
    Image:        nginx:1.17
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-project-268-84b655ccd9 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  3m55s  deployment-controller  Scaled up replica set web-project-268-6f74fdbb64 to 1
  Normal  ScalingReplicaSet  30s    deployment-controller  Scaled up replica set web-project-268-84b655ccd9 to 1
  Normal  ScalingReplicaSet  28s    deployment-controller  Scaled down replica set web-project-268-6f74fdbb64 to 0 from 1
[root@master01 ~]# kubectl rollout histroeyory ory deplouyment web-project-268
deployment.apps/web-project-268 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment web-project-268 nginx=nginx:1.17 --record=true

[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
admin-pod                          1/1     Running   0          46m
web-project-268-84b655ccd9-jpwnz   1/1     Running   0          119s
