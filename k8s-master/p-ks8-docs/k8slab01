=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.13 11:29:29 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Server refused our key
root@172.16.102.22's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Mon Sep 12 09:40:33 2022 from 192.168.200.220
[root@master01 ~]# [root@master01 ~]# kubelet get ctl get nodes
NAME                           STATUS   ROLES           AGE   VERSION
master01.airtel.internal.lab   Ready    control-plane   38h   v1.25.0
worker01.airtel.internal.lab   Ready    <none>          38h   v1.25.0
worker02.airtel.internal.lab   Ready    <none>          38h   v1.25.0
[root@master01 ~]# kubectl get nodespods
No resources found in default namespace.
[root@master01 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS      AGE
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (38h ago)   38h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (38h ago)   38h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (38h ago)   38h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-bk8v6                                       1/1     Running   0             38h
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-vpx59                                       1/1     Running   0             38h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   weave-net-42gjg                                        2/2     Running   3 (38h ago)   38h
kube-system   weave-net-44k9t                                        2/2     Running   1 (38h ago)   38h
kube-system   weave-net-mdv4h                                        2/2     Running   0             38h
[root@master01 ~]# [root@master01 ~]# [root@master01 ~]# kubectl sece exec -it admin-pod --dry-run=clinetent busyboxrun
error: required flag(s) "image" not set
[root@master01 ~]# kubectl run admin-pod --dry-run=client busybox-busybox-busyboxibusyboxmbusyboxabusyboxgbusyboxebusybox=busybox --name=
pod/admin-pod created (dry run)
[root@master01 ~]# kubectl get pod
No resources found in default namespace.
[root@master01 ~]# kubectl get pod a----all-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS      AGE
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (38h ago)   38h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (38h ago)   38h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (38h ago)   38h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-bk8v6                                       1/1     Running   0             38h
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (38h ago)   38h
kube-system   kube-proxy-vpx59                                       1/1     Running   0             38h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (38h ago)   38h
kube-system   weave-net-42gjg                                        2/2     Running   3 (38h ago)   38h
kube-system   weave-net-44k9t                                        2/2     Running   1 (38h ago)   38h
kube-system   weave-net-mdv4h                                        2/2     Running   0             38h

[root@master01 ~]# kubectl run admin-pod --image=busybox --dry-run=client -o yaml

[root@master01 ~]# vim admin-pod.yaml  

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: admin-pod
  name: admin-pod
spec:
  containers:
  - image: busybox
    name: admin-pod
    command:
    - sleep
    - "3600"
    securityContext:capabilites:add: ["SYS_TIME"]
status: {}

[root@master01 ~]# kubectl create -f admin.pod.yaml
pod/admin-pod created

[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
admin-pod                          1/1     Running   0          17s
web-project-268-84b655ccd9-jpwnz   1/1     Running   0          9m25s

[root@master01 ~]#

[root@master01 ~]# kubectl delete -f web-project-268.yaml 
deployment.apps "web-project-268" deleted

[root@master01 ~]# [root@master01 ~]# kubectl create deployment --image=nginx:1.16 
deployment.apps/web-project-268 created

[root@master01 ~]# kubectl get deployment.app
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
web-project-268   1/1     1            1           24s
[root@master01 ~]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
admin-pod                          1/1     Running   0          42m
web-project-268-6f74fdbb64-xgzd9   1/1     Running   0          34s

[root@master01 ~]# kubectl describe web-project-268
Name:                   web-project-268
Namespace:              default
CreationTimestamp:      Tue, 13 Sep 2022 12:20:40 +0530
Labels:                 app=web-project-268
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=web-project-268
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=web-project-268
  Containers:
   nginx:
    Image:        nginx:1.16
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-project-268-6f74fdbb64 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m9s  deployment-controller  Scaled up replica set web-project-268-6f74fdbb64 to 1
  
[root@master01 ~]# kubectl set image deployment web-project0-268 ngincx=nginx.1.17 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/web-project-268 image updated

[root@master01 ~]# kubectl describe deploymnet web-project-268
Name:                   web-project-268
Namespace:              default
CreationTimestamp:      Tue, 13 Sep 2022 12:20:40 +0530
Labels:                 app=web-project-268
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment web-project-268 nginx=nginx:1.17 --record=true
Selector:               app=web-project-268
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=web-project-268
  Containers:
   nginx:
    Image:        nginx:1.17
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-project-268-84b655ccd9 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  3m55s  deployment-controller  Scaled up replica set web-project-268-6f74fdbb64 to 1
  Normal  ScalingReplicaSet  30s    deployment-controller  Scaled up replica set web-project-268-84b655ccd9 to 1
  Normal  ScalingReplicaSet  28s    deployment-controller  Scaled down replica set web-project-268-6f74fdbb64 to 0 from 1
[root@master01 ~]# kubectl rollout histroeyory ory deplouyment web-project-268
deployment.apps/web-project-268 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment web-project-268 nginx=nginx:1.17 --record=true

[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
admin-pod                          1/1     Running   0          46m
web-project-268-84b655ccd9-jpwnz   1/1     Running   0          119s
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# [root@master01 ~]# kubectl get foo |greep unable -to-access-webiste unable-to-access-webiste- unable-to-access-webistei unable-to-access-webiste
error: the server doesn't have a resource type "foo"
[root@master01 ~]# kubectl get foo |grep -i unable-to-access-webiste > /opt/KULM000201201/foo
-bash: /opt/KULM00201/foo: No such file or directory
error: the server doesn't have a resource type "foo"
[root@master01 ~]# mkdir /opt/KULM00201/
[root@master01 ~]# mkdir /opt/KULM00201/kubectl get foo |grep -i unable-to-access-webiste > /opt/KULM00201/foo
error: the server doesn't have a resource type "foo"
[root@master01 ~]# [root@master01 ~]# vim nfs-pv.yaml
"nfs-pv.yaml" [New File]  ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                0,0-1Alli -- INSERT --0,1AllapiVersion: v1
kind: PersistentVolumemetadata:  name: nfs-pv
spec:  capacity:    storage: 20Gi  volumeMode: Filesystem
  accessModes:    - ReadWriteMany  persistentVolumeReclaim  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs  claimRef:             #Not mandatory
    name: nfs-pvc                                 #Not mandatory
  mountOptions:    - hard    - nfsvers=4.1  nfs:    path: /volume01
    server: 172.16.102.2020,26^[  20,25All::wq!"nfs-pv.yaml" [New] 20L, 409C written
[root@master01 ~]# kubectl create -f nnfs-pv.yaml 
persistentvolume/nfs-pv created
[root@master01 ~]# pvskubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM      STORAGECLASS   REASON   AGE
nfs-pv   20Gi       RWX            Recycle          Available   /nfs-pvc   nfs                     7s
[root@master01 ~]# kubectl get pvc
No resources found in default namespace.
[root@master01 ~]# kubectl get pvccreate -f nfs-pv.yaml delete
persistentvolume "nfs-pv" deleted
[root@master01 ~]# kubectl delete -f nfs-pv.yaml get pvccreate -f nfs-pv.yaml vim nfs-pv.yaml
"nfs-pv.yaml" 20L, 409C  apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 20Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  claimRef: #Not mandatory
    name: nfs-pvc #Not mandatory
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /volume01
    server: 172.16.102.20
~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                20,25All~@k   19,19~@k   8,6 ~@k   7,17~@k   6,10~@k   5,15~@k   4,19-32~@k   3,25   ~@k   2,23~@k   1,25~@k   0,19~@k   9,14 ~@k   8,2~@k   7,17i -- INSERT --7,17All6Gi5Gi45Gi5^[  7,14All~@k   6,11~@k   5,5 ~@k   4,14i -- INSERT --4,14All506175,6 6,127,1789,1510,171234809120^[  14,19All::wq!"nfs-pv.yaml" 20L, 412C written
[root@master01 ~]# vim nfs-pv.yamlkubectl delete -f nfs-pv.yaml get pvccreate -f nfs-pv.yaml 
persistentvolume/nfs-pv01 created
[root@master01 ~]# kubectl create -f nfs-pv.yaml vim nfs-pv.yaml
"nfs-pv.yaml" 20L, 412C  apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv01
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  claimRef: #Not mandatory
    name: nfs-pvc01 #Not mandatory
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /volume01
    server: 172.16.102.20
~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                14,19All~@k   3~@k   2~@k   1~@k   0~@k   9,14 ~@k   8,19~@k   7,16~@k   6,11~@k   5,5 ~@k   4,16~@k   3,9 ~@k   4,16~@k   i -- INSERT --4,16All76275,6 6,127,1789,1510,1712348920 19220^[  14,19All::wq!"nfs-pv.yaml" 20L, 412C written
[root@master01 ~]# vim nfs-pv.yamlkubectl create -f nfs-pv.yaml vim nfs-pv.yamlkubectl delete -f nfs-pv.yaml vim nfs-pv.yamlkubectl create -f nfs-pv.yaml 
persistentvolume/nfs-pv02 created
[root@master01 ~]# kubectl create -f nfs-pv.yaml vim nfs-pv.yamlkubectl create -f nfs-pv.yaml vim nfs-pv.yaml
"nfs-pv.yaml" 20L, 412C  apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv02
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  claimRef: #Not mandatory
    name: nfs-pvc02 #Not mandatory
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /volume01
    server: 172.16.102.20
~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                14,19All~@k   3~@k   2~@k   1~@k   0~@k   9,14 ~@k   8,19~@k   7,16~@k   6,11~@k   5,5 ~@k   4,16~@k   3,9 ~@k   2,19~@k   3,9 ~@k   4,16i -- INSERT --4,16All76375,6 6,127,178765Gi41Gi50Gi689,1510,16123478920 19320^[  14,19All^[  ^[  ::wq!"nfs-pv.yaml" 20L, 413C written
[root@master01 ~]# vim nfs-pv.yamlkubectl create -f nfs-pv.yaml 
persistentvolume/nfs-pv03 created
[root@master01 ~]# kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01   5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     43s
nfs-pv02   5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     23s
nfs-pv03   10Gi       RWX            Recycle          Available   /nfs-pvc03   nfs                     5s
[root@master01 ~]# kubectl get pvvc
No resources found in default namespace.
[root@master01 ~]# [root@master01 ~]# kubectl get --sort-by=s.spec.capacity.storage > /opt/KULM00201/volume_list
You must specify the type of resource to get. Use "kubectl api-resources" for a complete list of supported resources.

error: Required resource not specified.
Use "kubectl explain <resource>" for a detailed description of that resource (e.g. kubectl explain pods).
See 'kubectl get -h' for help and examples
[root@master01 ~]# kubectl get --sort-by=.spec.capacity.storage > /opt/KULM00201/volume_list pv
[root@master01 ~]# ls -l cat /opt/KULM00201/
foo          volume_list  
[root@master01 ~]# cat /opt/KULM00201/volume_list 
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01   5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     2m30s
nfs-pv02   5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     2m10s
nfs-pv03   10Gi       RWX            Recycle          Available   /nfs-pvc03   nfs                     112s
[root@master01 ~]# kubectl create dammenset ds-kusc00201eo --image =nginx --dry-run-=client -o wideyaml -n kube-system
error: unknown flag: --image
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create daemonset -n kube-system ds-kusc00201 --image=nginx --dry-run=client -o yaml kubectl create daemonset -n kube-system ds-kusc00201 --image nginx --dry-run=client -o yaml=nginx
error: unknown flag: --image
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create daemonset -n kube-system ds-kusc00201 --image=nginx --dry-run=client -o yaml 
Error: must specify one of -f and -k

error: unknown command "daemonset ds-kusc00201"
See 'kubectl create -h' for help and examples
[root@master01 ~]# kubectl create daemonset -n kube-system ds-kusc00201 --dry-run=client -o yaml -k
error: Unexpected args: [daemonset]
See 'kubectl create -h' for help and examples
[root@master01 ~]# [root@master01 ~]# kubectl create daemonset -n kube-system -k ds-kusc00201 --dry-run=client -o yaml-k .yaml ds-kusc00201.yaml ds-kusc00201.yamlds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yamlds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yamlds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yamlds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yamlds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yaml ds-kusc00201.yamlv ds-kusc00201.yamli ds-kusc00201.yamlm ds-kusc00201.yaml
"ds-kusc00201.yaml" [New File]  ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                0,0-1Alli -- INSERT --0,1AllapiVersion: apps/v1
kind: DaemonSetmetadata:  name: fluentd-elasticsearch
  namespace: kube-system  labels:    k8s-ap    k8s-app: fluentd-logging
spec:  selector:    matchLabels:      namname: fluentd-elasticsearch
  template:    metadata:      labelslabels:
        name: fluentd-elasticsearch    spec:      toleratiotolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists        effect: NoSchedule
      - key: node-role.kubernetes.io/master        operoperator: Exists
        effect: NoSchedule      containers:      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.228,605987654321049876543210398765432102987654321019876n7g8i9n20x17234567893012345654321029876543210198765n6g7i8n9x20^[  27,19All~@k   6,17~@k   5,19~@k   4~@k   3~@k   2~@k   1~@k   0dd  
~                                                                                                                                                                                20,9Alldd  
~                                                                                                                                                                                20,9Alldd  
~                                                                                                                                                                                20,7All~@k   1~@k   2~@k   3~@k   4~@k   5~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   4~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,7 ~@k   8,5~@k   7,7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8,5~@k   9,7~@k   10,7~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   4~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,7 ~@k   8,5~@k   7,7~@k   6~@k   5~@k   4~@K   29i -- INSERT --4,29All3029876543210198765432109 ds-kusc0020121^[  4,20All~@k   5~@k   6,9 ~@k   7,20~@k   8,5 ~@k   9,11~@k   10,16~@k   1,20~@k   2,11~@k   3,13~@k   4~@k   5,20~@k   6,9 ~@k   7,18dd  
~                                                                                                                                                                                17,7Alldd  
~                                                                                                                                                                                17,7Alldd  
~                                                                                                                                                                                17,7Alldd  
~                                                                                                                                                                                17,9Alldd  
~                                                                                                                                                                                17,9Alldd  
~                                                                                                                                                                                17,7All~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,7 ~@k   8,5~@k   7,7~@k   6dd  
~                                                                                                                                                                                6,5Alldd  
~                                                                                                                                                                                6,1All~@k   5dd  
~                                                                                                                                                                                5,1All~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ^[  ^[  ::wq!"ds-kusc00201.yaml" [New] 16L, 281C written
[root@master01 ~]# kubectl create -f ds-kusc00201.yaml 
daemonset.apps/ds-kusc00201 created
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS              RESTARTS      AGE    IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running             2 (11m ago)   123m   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 0/1     ContainerCreating   0             6s     <none>      worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 0/1     ContainerCreating   0             6s     <none>      worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running             0             132m   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS      AGE    IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running   2 (11m ago)   123m   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running   0             13s    10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running   0             13s    10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running   0             133m   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide --all-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS      AGE    IP              NODE                           NOMINATED NODE   READINESS GATES
default       admin-pod                                              1/1     Running   2 (11m ago)   123m   10.44.0.1       worker02.airtel.internal.lab   <none>           <none>
default       ds-kusc00201-xgb9l                                     1/1     Running   0             20s    10.36.0.1       worker01.airtel.internal.lab   <none>           <none>
default       ds-kusc00201-xkdtw                                     1/1     Running   0             20s    10.44.0.2       worker02.airtel.internal.lab   <none>           <none>
default       web-project-268-84b655ccd9-jpwnz                       1/1     Running   0             133m   10.36.0.2       worker01.airtel.internal.lab   <none>           <none>
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (41h ago)   42h    10.32.0.3       master01.airtel.internal.lab   <none>           <none>
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (41h ago)   42h    10.32.0.2       master01.airtel.internal.lab   <none>           <none>
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (41h ago)   42h    172.16.102.22   master01.airtel.internal.lab   <none>           <none>
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (41h ago)   42h    172.16.102.22   master01.airtel.internal.lab   <none>           <none>
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (41h ago)   42h    172.16.102.22   master01.airtel.internal.lab   <none>           <none>
kube-system   kube-proxy-bk8v6                                       1/1     Running   0             41h    172.16.102.24   worker02.airtel.internal.lab   <none>           <none>
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (41h ago)   42h    172.16.102.22   master01.airtel.internal.lab   <none>           <none>
kube-system   kube-proxy-vpx59                                       1/1     Running   0             41h    172.16.102.23   worker01.airtel.internal.lab   <none>           <none>
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (41h ago)   42h    172.16.102.22   master01.airtel.internal.lab   <none>           <none>
kube-system   weave-net-42gjg                                        2/2     Running   3 (41h ago)   42h    172.16.102.22   master01.airtel.internal.lab   <none>           <none>
kube-system   weave-net-44k9t                                        2/2     Running   1 (41h ago)   41h    172.16.102.23   worker01.airtel.internal.lab   <none>           <none>
kube-system   weave-net-mdv4h                                        2/2     Running   0             41h    172.16.102.24   worker02.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide --all-namespaces--all-namespaces--all-namespaces--all-namespaces--all-namespaces--all-namespaces--all-namespaces--all-namespacesall-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS      AGE
default       admin-pod                                              1/1     Running   2 (12m ago)   124m
default       ds-kusc00201-xgb9l                                     1/1     Running   0             29s
default       ds-kusc00201-xkdtw                                     1/1     Running   0             29s
default       web-project-268-84b655ccd9-jpwnz                       1/1     Running   0             133m
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (41h ago)   42h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (41h ago)   42h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (41h ago)   42h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (41h ago)   42h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (41h ago)   42h
kube-system   kube-proxy-bk8v6                                       1/1     Running   0             41h
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (41h ago)   42h
kube-system   kube-proxy-vpx59                                       1/1     Running   0             41h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (41h ago)   42h
kube-system   weave-net-42gjg                                        2/2     Running   3 (41h ago)   42h
kube-system   weave-net-44k9t                                        2/2     Running   1 (41h ago)   41h
kube-system   weave-net-mdv4h                                        2/2     Running   0             41h
[root@master01 ~]# kubectl get ds
NAME           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ds-kusc00201   2         2         2       2            2           <none>          57s
[root@master01 ~]# vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" [New File]  ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                ~                                                                                                                                                                                0,0-1Alli -- INSERT --0,1All~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          ~                                                                          -- INSERT --0,1Alla1,2p3i4V5e6r7s8i9o10n1apiVersion:23v4152,1 k2i3n4d5kind:67P8o9d103,1 m2e3t4a5d6a7t8a9metadata:10
        4,9 n10a1m2e3name:45h6u7n8g9r20y1-2b3e4a5r6
        5,9 87654321s2p3e4c5spec:6
        6,9876543v4o5l6u7m8e9s10  volumes:1
          709 876545-6- 7n8a9m10e1name:23w4o5r6k7d8i9r206,115,6 4,20198765432109 89name: hungry-bear8name: hungry-bear7name: hungry-bear6name: hungry-bear5name: hungry-bear4name: hungry-bear356720
      8,7 e8m9p10t1h2y3D4i5r6empthyDir:7
              9,15432109 876543c4o5n6t7a8i9n10e1r2s3  containers:4
          10,1109 876543-4- 5n6a7m8e9name:101c2h3e4c5k6e7r8
    1,5 i6m7a8g9e10image:12a3l4p5i6n7e8
    2,5 c6o7m8m9a10n1d2    command:34[5][]65"]["6/]7b]8i]9n]20/]1s]2h]3"][]4 ] ]5]]4,],]5 ] ]6"]["7"][]8[]7-"]8e"]9,"]30"]29[]30,],]1 ] ]2"]["3i]4f]5 ]6[]7]87-]]8f]]9 ]]40/]]1w]]2o]]3r]]4k]]5d]]6i]]7r]]8/]]9c]]50a]]1l]]2m]]3.]]4t]]5x]]6t]]78"][]960;[]1
    3,5 6789101234545t6h7e8n920s1l2e3e4p561708090300102100000;34e5l6a7s8e940e1x2i3t415;6f7i8"925012345678[]9];              then sleep 100000; elase exit1;fi"[;83,4987656 fi"7654 1; fi"56789501
              4,15432109 876545v6o7l8u9m10e1M2o3u4n5t6s7    volumeMounts:8
            5,132109 8765-67n8a9m10e1name:23w4o5r6k7d8i9r20
      6,7 m8o9u10n1t2P3a4t5h6mountPath:78/9w20o1r2k3d4i5r6
      7,7 654i5n6i7t8C9o10n1t2a3i4n5e6r7s8   initContainers:9
           8,12109 87654-56n7a8m9e10name:12c3r4e5a6t7e8
     9,6 i7m8a9g10e121image:23a4;54l5p6i7n8e9
     20,6 c7o8m9m10a1n2d3     command:45[6][]76"]["7"][]8[]7/"]8b"]9i"]20n"]1/"]2s"]3h"]4[]565,],]6 ] ]7"]["8"][]9[]8-"]9c"]30[]1,],]2 ] ]3"]["4t]5o]6u]7c]8h]9 ]40/]1w]2o]3r]4k]5d]6i]7r]8/]9c]50a]1l]2m]3.]4t]5x]6t]7"][]89
     []1,6 v7o8l9u10m1e2M3o4u5n6t7s8     volumeMounts:9
             2,1432109 876-78n9a10m1e2name:34w5o6r7k8d9i20r1
       3,8 m9o10u1n2t3P4a5t6h7mountPath:89/20w1o2r3k4f5i6r7654d5i6r7^[  23,26All~@k   2,20~@k   1,18~@k   0,26~@k   19,18~@k   8,17~@k   7,18~@k   6,25~@k   5,19~@k   4,17~@k   3,26~@k   2~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7i -- INSERT --12,37All -f /workdir/calm.txt]];8^[  12,37All^[  ^[  ::wq!"pod-spec-KUCC00108.yaml"<-spec-KUCC00108.yaml" [New] 23L, 517C written
[root@master01 ~]# kubectl ctrereate -f pod-spec-KUCC00108.yaml 
error: error parsing pod-spec-KUCC00108.yaml: error converting YAML to JSON: yaml: line 13: did not find expected ',' or ']'
[root@master01 ~]# [root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 23L, 517C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-e", "if [ -f /workdir/calm.txt]];
              then sleep 100000; elase exit 1; fi"
    volumeMounts:
    - name: workdirmountPath: /workdir
   initContainers:
   - name: createimage: alpinecommand: ["/bin/sh", "-c", "touch /workdir/calm.txt"]volumeMounts:- name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      12,37All::1313,15Alli -- INSERT --13,15Allthen sleep 100000; elase exit 1; fi"4then sleep 100000; elase exit 1; fi"3then sleep 100000; elase exit 1; fi"2then sleep 100000; elase exit 1; fi"1then sleep 100000; elase exit 1; fi"0then sleep 100000; elase exit 1; fi"9 then sleep 100000; elase exit 1; fi"8then sleep 100000; elase exit 1; fi"7then sleep 100000; elase exit 1; fi"6then sleep 100000; elase exit 1; fi"5then sleep 100000; elase exit 1; fi"4then sleep 100000; elase exit 1; fi"3then sleep 100000; elase exit 1; fi"2then sleep 100000; elase exit 1; fi"1
    command: ["/bin/sh", "-e", "if [ -f /workdir/calm.txt]];then sleep 100000; elase exit 1; fi"~                                                                                                                                                                                      12,61All then sleep 100000; elase exit 1; fi"2^[  12,61All~@K   97~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   89~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3x dl  se exit 1; fi"^[  ^[  ::wq!"pod-spec-KUCC00108.yaml" 22L, 502C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
error: error parsing pod-spec-KUCC00108.yaml: error converting YAML to JSON: yaml: line 12: did not find expected ',' or ']'
[root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 22L, 502C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-e", "if [ -f /workdir/calm.txt]]; then sleep 100000; else exit 1; fi"
    volumeMounts:
    - name: workdirmountPath: /workdir
   initContainers:
   - name: createimage: alpinecommand: ["/bin/sh", "-c", "touch /workdir/calm.txt"]volumeMounts:- name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      12,83All::1212,5All~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9i -- INSERT --12,29All", "if [ -f /workdir/calm.txt]]; then sleep 100000; else exit 1; fi"8c", "if [ -f /workdir/calm.txt]]; then sleep 100000; else exit 1; fi"9301234567894012345678950123456789601234567897012345678980123456789901234567i87}87]- - - []87[]65432108987654321079876543210698765432105960; then sleep 100000; else exit 1; fi"]59; then sleep 100000; else exit 1; fi"]8 ; then sleep 100000; else exit 1; fi"]9l; then sleep 100000; else exit 1; fi"]60^[  12,59All::wq!"pod-spec-KUCC00108.yaml" 22L, 503C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
error: error parsing pod-spec-KUCC00108.yaml: error converting YAML to JSON: yaml: line 15: did not find expected key
[root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 22L, 503C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit 1; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
   initContainers:
   - name: createimage: alpinecommand: ["/bin/sh", "-c", "touch /workdir/calm.txt"]volumeMounts:- name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      12,59All::1515,7All^[  ^[  ::wq!"pod-spec-KUCC00108.yaml" 22L, 503C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 22L, 503C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit 1; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
   initContainers:
   - name: createimage: alpinecommand: ["/bin/sh", "-c", "touch /workdir/calm.txt"]volumeMounts:- name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      15,7All~@k   4~@k   3~@k   2~@k   1~@k   2~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   []4~@k   []5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   40~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   50~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   60~@k   59~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   49~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   39~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3,17~@k   4,19~@k   5,25~@k   6,18~@k   7,17~@k   8,18~@k   9,34~@k   20,18~@k   1,20~@k   2,26~@k   ::wq!"pod-spec-KUCC00108.yaml" 22L, 503C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
error: error parsing pod-spec-KUCC00108.yaml: error converting YAML to JSON: yaml: line 15: did not find expected key
[root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 22L, 503C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit 1; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
   initContainers:
   - name: createimage: alpinecommand: ["/bin/sh", "-c", "touch /workdir/calm.txt"]volumeMounts:- name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      22,26All~@k   1,20~@k   0,18~@k   19,26~@k   8,18~@k   7,17~@k   6,18~@k   5,25~@k   4,19~@k   3,17~@k   2,26~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   40~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   50~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   60i -- INSERT --12,60All; then sleep 100000; else exit 1; fi"]59l; then sleep 100000; else exit 1; fi"]6012345678970123456789801234567899012; fi"]1l; fi"]2^[  12,91All::wq!"pod-spec-KUCC00108.yaml" 22L, 503C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
error: error parsing pod-spec-KUCC00108.yaml: error converting YAML to JSON: yaml: line 15: did not find expected key
[root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 22L, 503C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
   initContainers:
   - name: createimage: alpinecommand: ["/bin/sh", "-c", "touch /workdir/calm.txt"]volumeMounts:- name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      12,91All~@k   3,17~@k   4,19~@k   5,25~@k   6,18~@k   7,17~@k   8,18~@k   []9,5~@k   []20,1~@k   1,20~@k   2,26~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   dd  
~                                                                                                                                                                                      1,6 dd  
~                                                                                                                                                                                      0dd  
~                                                                                                                                                                                      19dd  
~                                                                                                                                                                                      8dd  
~                                                                                                                                                                                      7,4dd  
~                                                                                                                                                                                      6dd  
~                                                                                                                                                                                      5,7dd  
~                                                                                                                                                                                      4,5dd  
~                                                                                                                                                                                      3dd  
~                                                                                                                                                                                      2dd  
~                                                                                                                                                                                      1dd  
~                                                                                                                                                                                      0,3dd  
~                                                                                                                                                                                      9,3 dd  
~                                                                                                                                                                                      8,7dd  
~                                                                                                                                                                                      7,5dd  
~                                                                                                                                                                                      6,3dd  
~                                                                                                                                                                                      5,1dd  
~                                                                                                                                                                                      4,3dd  
~                                                                                                                                                                                      3,1dd  
~                                                                                                                                                                                      2dd  
~                                                                                                                                                                                      1dd  --No lines in buffer--0,0-1Alldd  dd  dd  dd  dd  dii   i -- INSERT --0,1AllapiVersion: v1
kind: Podmetadata:  name: hungry-bearspec:  volumes:
    - name: workdir      empthyDir:  containers:  - name: checker
    image: alpine    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi"]    volumeMounts:
    - name: workdir      mountPath: /workdir  initContainers:
  - name: create    image: alpine    command    command: ["/bin/sh", "-c", "touch /workdir/calm.txt"]
    volumeMounts    volumeMounts:
    - name: workdir      mountPath: /workdir22,26^[  22,25All::wq!"pod-spec-KUCC00108.yaml" 22L, 496C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
Error from server (BadRequest): error when creating "pod-spec-KUCC00108.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.volumes[0].empthyDir"
[root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 22L, 496C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
  initContainers:
  - name: create
    image: alpine
    command: ["/bin/sh", "-c", "touch /workdir/calm.txt"]
    volumeMounts:
    - name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      22,25All~@k   1,19~@k   0,17~@k   19,25~@k   8,17~@k   7,16~@k   6,17~@k   5,25~@k   4,19~@k   3,17~@k   2,25~@k   1,17~@k   0~@k   9,13 ~@k   8,16~@k   7,19~@k   6,10~@k   5,5 ~@k   4,19~@k   3,9 ~@k   2~@k   1,14~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   o -- INSERT --2,1Top2,1AllapiVersion: v15^[  2,14All~@k   1dd  
~                                                                                                                                                                                      1,1Alli -- INSERT --1,1AllapiVersion: v12,1All1^[  1,0-1All::wq!"pod-spec-KUCC00108.yaml" 23L, 497C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
Error from server (BadRequest): error when creating "pod-spec-KUCC00108.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.volumes[0].empthyDir"
[root@master01 ~]# kubectl create -f pod-spec-KUCC00108.yaml vim pod-spec-KUCC00108.yaml
"pod-spec-KUCC00108.yaml" 23L, 497C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdirempthyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
  initContainers:
  - name: create
    image: alpine
    command: ["/bin/sh", "-c", "touch /workdir/calm.txt"]
    volumeMounts:
    - name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      1,0-1All~@k   2,1  ~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   19~@k   8~@k   7~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,9 ~@k   10~@k   1~@k   2i -- INSERT --9,12AllyDir:1^[  9,10All::wq!"pod-spec-KUCC00108.yaml" 23L, 496C written
[root@master01 ~]# vim pod-spec-KUCC00108.yamlkubectl create -f pod-spec-KUCC00108.yaml 
pod/hungry-bear created
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS     RESTARTS      AGE
admin-pod                          1/1     Running    2 (41m ago)   153m
ds-kusc00201-xgb9l                 1/1     Running    0             30m
ds-kusc00201-xkdtw                 1/1     Running    0             30m
hungry-bear                        0/1     Init:0/1   0             6s
web-project-268-84b655ccd9-jpwnz   1/1     Running    0             162m
[root@master01 ~]# ls-ls -l 
total 24
-rw-r--r--  1 root root  284 Sep 13 12:33 admin-pod.yaml
-rw-------. 1 root root 1471 Jun  9 16:46 anaconda-ks.cfg
-rw-r--r--  1 root root  281 Sep 13 14:36 ds-kusc00201.yaml
-rw-r--r--  1 root root  413 Sep 13 14:22 nfs-pv.yaml
-rw-r--r--  1 root root    0 Sep 12 09:41 nodes.txt
-rw-r--r--  1 root root  496 Sep 13 15:06 pod-spec-KUCC00108.yaml
-rw-r--r--  1 root root  338 Sep 13 11:59 web-project-268.yaml
[root@master01 ~]# ls -l /
total 24
lrwxrwxrwx.   1 root root    7 Oct 11  2021 bin -> usr/bin
dr-xr-xr-x.   5 root root 4096 Jun  9 16:46 boot
drwxr-xr-x   19 root root 3020 Sep 11 20:44 dev
drwxr-xr-x.  93 root root 8192 Sep 11 20:45 etc
drwxr-xr-x.   2 root root    6 Oct 11  2021 home
lrwxrwxrwx.   1 root root    7 Oct 11  2021 lib -> usr/lib
lrwxrwxrwx.   1 root root    9 Oct 11  2021 lib64 -> usr/lib64
drwxr-xr-x.   2 root root    6 Oct 11  2021 media
drwxr-xr-x.   2 root root    6 Oct 11  2021 mnt
drwxr-xr-x.   5 root root   71 Sep 13 12:53 opt
dr-xr-xr-x  204 root root    0 Sep 11 20:44 proc
dr-xr-x---.   4 root root 4096 Sep 13 15:06 root
drwxr-xr-x   30 root root  920 Sep 11 20:44 run
lrwxrwxrwx.   1 root root    8 Oct 11  2021 sbin -> usr/sbin
drwxr-xr-x.   2 root root    6 Oct 11  2021 srv
dr-xr-xr-x   13 root root    0 Sep 11 20:44 sys
drwxrwxrwt.  11 root root  243 Sep 13 15:07 tmp
drwxr-xr-x.  13 root root  158 Jun  9 16:41 usr
drwxr-xr-x.  21 root root 4096 Jun  9 16:46 var
[root@master01 ~]# ls -l /root/
total 24
lrwxrwxrwx.   1 root root    7 Oct 11  2021 bin -> usr/bin
dr-xr-xr-x.   5 root root 4096 Jun  9 16:46 boot
drwxr-xr-x   19 root root 3020 Sep 11 20:44 dev
drwxr-xr-x.  93 root root 8192 Sep 11 20:45 etc
drwxr-xr-x.   2 root root    6 Oct 11  2021 home
lrwxrwxrwx.   1 root root    7 Oct 11  2021 lib -> usr/lib
lrwxrwxrwx.   1 root root    9 Oct 11  2021 lib64 -> usr/lib64
drwxr-xr-x.   2 root root    6 Oct 11  2021 media
drwxr-xr-x.   2 root root    6 Oct 11  2021 mnt
drwxr-xr-x.   5 root root   71 Sep 13 12:53 opt
dr-xr-xr-x  204 root root    0 Sep 11 20:44 proc
dr-xr-x---.   4 root root 4096 Sep 13 15:06 root
drwxr-xr-x   30 root root  920 Sep 11 20:44 run
lrwxrwxrwx.   1 root root    8 Oct 11  2021 sbin -> usr/sbin
drwxr-xr-x.   2 root root    6 Oct 11  2021 srv
dr-xr-xr-x   13 root root    0 Sep 11 20:44 sys
drwxrwxrwt.  11 root root  243 Sep 13 15:07 tmp
drwxr-xr-x.  13 root root  158 Jun  9 16:41 usr
drwxr-xr-x.  21 root root 4096 Jun  9 16:46 var
[root@master01 ~]# ls -l /tmp/
total 8
-rw-r--r--  1 root root 2079 Sep 11 20:44 _cafenv-appconfig_
drwx------. 2 root root 4096 Sep 11 20:44 vmware-root
drwx------  2 root root    6 Sep 11 20:19 vmware-root_1297-4257135027
drwx------  2 root root    6 Sep 11 20:22 vmware-root_1322-2999591881
drwx------  2 root root    6 Sep 11 20:44 vmware-root_1438-2689143972
[root@master01 ~]# ls -l /tmp/kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE    IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (42m ago)     154m   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               30m    10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               30m    10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   2 (6m48s ago)   48s    10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               163m   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (44m ago)     156m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               32m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               32m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   4 (7m31s ago)   2m53s   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               165m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# ssh 172.16.102.24
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Sun Sep 11 20:40:51 2022 from 192.168.200.221
[root@worker02 ~]# logout
Connection to 172.16.102.24 closed.
[root@master01 ~]# ssh 172.16.102.243
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Sun Sep 11 20:49:58 2022 from 172.16.102.22
[root@worker01 ~]# ls -l 
total 4
-rw-------. 1 root root 1471 Jun  9 16:46 anaconda-ks.cfg
[root@worker01 ~]# mdimkdir /workdir
[root@worker01 ~]# mkdir /workdirlogout
Connection to 172.16.102.23 closed.
[root@master01 ~]# ssh 172.16.102.234kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS       AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running   2 (45m ago)    157m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running   0              33m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running   0              33m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     Error     5 (8m8s ago)   3m30s   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running   0              166m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widedescribe pods hungry-bear
Name:             hungry-bear
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Tue, 13 Sep 2022 15:00:23 +0530
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.36.0.3
IPs:
  IP:  10.36.0.3
Init Containers:
  create:
    Container ID:  containerd://c0b38d83b1f06cce1cd22755f9f1c54e77066037e4378bc58d9ad453f39f985a
    Image:         alpine
    Image ID:      docker.io/library/alpine@sha256:bc41182d7ef5ffc53a40b044e725193bc10142a1243f395ee852a8d9730fc2ad
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      touch /workdir/calm.txt
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 13 Sep 2022 15:00:30 +0530
      Finished:     Tue, 13 Sep 2022 15:00:30 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sklvw (ro)
      /workdir from workdir (rw)
Containers:
  checker:
    Container ID:  containerd://c57c4c6574d870277e53d06ef8a7a8e40dfeefd26d0b68327ea3f4b74ab43f33
    Image:         alpine
    Image ID:      docker.io/library/alpine@sha256:bc41182d7ef5ffc53a40b044e725193bc10142a1243f395ee852a8d9730fc2ad
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Tue, 13 Sep 2022 15:03:48 +0530
      Finished:     Tue, 13 Sep 2022 15:03:48 +0530
    Ready:          False
    Restart Count:  5
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sklvw (ro)
      /workdir from workdir (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  workdir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-sklvw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  3m45s                default-scheduler  Successfully assigned default/hungry-bear to worker01.airtel.internal.lab
  Normal   Pulling    10m                  kubelet            Pulling image "alpine"
  Normal   Pulled     10m                  kubelet            Successfully pulled image "alpine" in 5.936855323s
  Normal   Created    10m                  kubelet            Created container create
  Normal   Started    10m                  kubelet            Started container create
  Normal   Pulled     10m                  kubelet            Successfully pulled image "alpine" in 2.193607766s
  Normal   Pulled     10m                  kubelet            Successfully pulled image "alpine" in 2.235494893s
  Normal   Pulled     9m45s                kubelet            Successfully pulled image "alpine" in 2.262974517s
  Normal   Pulling    9m18s (x4 over 10m)  kubelet            Pulling image "alpine"
  Normal   Created    9m16s (x4 over 10m)  kubelet            Created container checker
  Normal   Pulled     9m16s                kubelet            Successfully pulled image "alpine" in 2.042899551s
  Normal   Started    9m16s (x4 over 10m)  kubelet            Started container checker
  Warning  BackOff    9m15s (x5 over 10m)  kubelet            Back-off restarting failed container
[root@master01 ~]# kubectl describe pods hungry-bearget pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (46m ago)     158m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               34m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               34m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   5 (7m45s ago)   4m39s   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               167m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (46m ago)     158m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               34m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               34m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   5 (7m48s ago)   4m42s   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               167m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (46m ago)     158m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               34m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               34m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   5 (7m51s ago)   4m45s   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               167m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (46m ago)     158m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               34m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               34m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   5 (7m52s ago)   4m46s   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               167m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widedescribe pods hungry-bearget pods -o widedescribe pods hungry-bear
Name:             hungry-bear
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Tue, 13 Sep 2022 15:00:23 +0530
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.36.0.3
IPs:
  IP:  10.36.0.3
Init Containers:
  create:
    Container ID:  containerd://c0b38d83b1f06cce1cd22755f9f1c54e77066037e4378bc58d9ad453f39f985a
    Image:         alpine
    Image ID:      docker.io/library/alpine@sha256:bc41182d7ef5ffc53a40b044e725193bc10142a1243f395ee852a8d9730fc2ad
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      touch /workdir/calm.txt
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 13 Sep 2022 15:00:30 +0530
      Finished:     Tue, 13 Sep 2022 15:00:30 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sklvw (ro)
      /workdir from workdir (rw)
Containers:
  checker:
    Container ID:  containerd://c57c4c6574d870277e53d06ef8a7a8e40dfeefd26d0b68327ea3f4b74ab43f33
    Image:         alpine
    Image ID:      docker.io/library/alpine@sha256:bc41182d7ef5ffc53a40b044e725193bc10142a1243f395ee852a8d9730fc2ad
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
      -c
      if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Tue, 13 Sep 2022 15:03:48 +0530
      Finished:     Tue, 13 Sep 2022 15:03:48 +0530
    Ready:          False
    Restart Count:  5
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sklvw (ro)
      /workdir from workdir (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  workdir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-sklvw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  4m48s              default-scheduler  Successfully assigned default/hungry-bear to worker01.airtel.internal.lab
  Normal   Pulling    11m                kubelet            Pulling image "alpine"
  Normal   Pulled     11m                kubelet            Successfully pulled image "alpine" in 5.936855323s
  Normal   Created    11m                kubelet            Created container create
  Normal   Started    11m                kubelet            Started container create
  Normal   Pulled     11m                kubelet            Successfully pulled image "alpine" in 2.193607766s
  Normal   Pulled     11m                kubelet            Successfully pulled image "alpine" in 2.235494893s
  Normal   Pulled     10m                kubelet            Successfully pulled image "alpine" in 2.262974517s
  Normal   Pulling    10m (x4 over 11m)  kubelet            Pulling image "alpine"
  Normal   Created    10m (x4 over 11m)  kubelet            Created container checker
  Normal   Pulled     10m                kubelet            Successfully pulled image "alpine" in 2.042899551s
  Normal   Started    10m (x4 over 11m)  kubelet            Started container checker
  Warning  BackOff    10m (x5 over 11m)  kubelet            Back-off restarting failed container
[root@master01 ~]# kubectl crun nginx --image=nginx --dry-run=client -o yaml > nginx.yaml
[root@master01 ~]# vim nginx.yaml 
"nginx.yaml" 15L, 232C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   2~@k   1~@k   2~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   2~@k   3~@k   2dd  
~                                                                                                                                                                                      12,3Alldd  
~                                                                                                                                                                                      12,3Alldd  
~                                                                                                                                                                                      12,1All~@k   1~@k   0~@k   2~@k   3yy  2~@k    2~@k   1p - image: nginx12,3All2yy   p - image: nginx
status: {}13,3Allp - image: nginx
status: {}14,3All~@k   3~@k   2o -- INSERT --13,5Top13,5Allb6n7a8m9e10bname:121    bname09 8765n6a7m8e9name:101redis6457654321image09 8765-4321
- image: nginx~                                                                                                                                                                                      14,17All15,5All467891012343256765432redis73,164,1765432memcached2115,5Alln6a7m8e9name:101memcached206,1 {}7,11{}{}8{}{}9^[  19,10All~@k   {}{}8~@k   {}{}7~@k   {}6,0-1dd  
~                                                                                                                                                                                      16,1Alldd  
~                                                                                                                                                                                      16,1Alldd  
~                                                                                                                                                                                      16,1All::wq!"nginx.yaml" 16L, 238C written
[root@master01 ~]# kubectl create -f nginx.yaml 
pod/nginx created
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             2 (48m ago)     160m
ds-kusc00201-xgb9l                 1/1     Running             0               37m
ds-kusc00201-xkdtw                 1/1     Running             0               37m
hungry-bear                        0/1     CrashLoopBackOff    6 (7m22s ago)   7m9s
nginx                              0/3     ContainerCreating   0               4s
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               169m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             2 (48m ago)     160m
ds-kusc00201-xgb9l                 1/1     Running             0               37m
ds-kusc00201-xkdtw                 1/1     Running             0               37m
hungry-bear                        0/1     CrashLoopBackOff    6 (7m27s ago)   7m14s
nginx                              0/3     ContainerCreating   0               9s
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               170m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             2 (48m ago)     160m
ds-kusc00201-xgb9l                 1/1     Running             0               37m
ds-kusc00201-xkdtw                 1/1     Running             0               37m
hungry-bear                        0/1     CrashLoopBackOff    6 (7m30s ago)   7m17s
nginx                              0/3     ContainerCreating   0               12s
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               170m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             2 (48m ago)     160m
ds-kusc00201-xgb9l                 1/1     Running             0               37m
ds-kusc00201-xkdtw                 1/1     Running             0               37m
hungry-bear                        0/1     CrashLoopBackOff    6 (7m31s ago)   7m18s
nginx                              0/3     ContainerCreating   0               13s
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               170m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             2 (48m ago)     161m
ds-kusc00201-xgb9l                 1/1     Running             0               37m
ds-kusc00201-xkdtw                 1/1     Running             0               37m
hungry-bear                        0/1     CrashLoopBackOff    6 (7m32s ago)   7m19s
nginx                              0/3     ContainerCreating   0               14s
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               170m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS             RESTARTS        AGE
admin-pod                          1/1     Running            2 (51m ago)     163m
ds-kusc00201-xgb9l                 1/1     Running            0               39m
ds-kusc00201-xkdtw                 1/1     Running            0               39m
hungry-bear                        0/1     CrashLoopBackOff   6 (9m42s ago)   9m29s
nginx                              3/3     Running            0               2m24s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               172m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS             RESTARTS        AGE
admin-pod                          1/1     Running            2 (51m ago)     163m
ds-kusc00201-xgb9l                 1/1     Running            0               39m
ds-kusc00201-xkdtw                 1/1     Running            0               39m
hungry-bear                        0/1     CrashLoopBackOff   6 (9m51s ago)   9m38s
nginx                              3/3     Running            0               2m33s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               172m
[root@master01 ~]# kubectl get podsdescribe pods nginx
Name:             nginx
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Tue, 13 Sep 2022 15:05:58 +0530
Labels:           run=nginx
Annotations:      <none>
Status:           Running
IP:               10.44.0.3
IPs:
  IP:  10.44.0.3
Containers:
  nginx:
    Container ID:   containerd://49c6946703e1648837ce6eb0123fa540e476ef0e94ea486f5805354d73380651
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:fb18e3e674b78832837ea05854090f3ca692969348937f0fed6721917277737c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 13 Sep 2022 15:06:01 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xbx5p (ro)
  redis:
    Container ID:   containerd://8cbcf9215e4027095c834190e80ed4ec32da021a351dc35c66362502d01f0823
    Image:          redis
    Image ID:       docker.io/library/redis@sha256:4feb642f6382e6abf95c279ad4130a22f2d8b5e2d9c9e47e3a0ce51ed7ac597b
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 13 Sep 2022 15:06:09 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xbx5p (ro)
  memcached:
    Container ID:   containerd://09b69dddb914b766cbdd82a24c42724db0561e68cacd89220d77f9f051e08898
    Image:          memcached
    Image ID:       docker.io/library/memcached@sha256:c0e6637075167ec1cc888a9e5930d77b3be45d257c234b360e5bbede88a15208
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 13 Sep 2022 15:06:18 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xbx5p (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-xbx5p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m45s  default-scheduler  Successfully assigned default/nginx to worker02.airtel.internal.lab
  Normal  Pulling    10m    kubelet            Pulling image "nginx"
  Normal  Pulled     10m    kubelet            Successfully pulled image "nginx" in 2.12535198s
  Normal  Created    10m    kubelet            Created container nginx
  Normal  Started    10m    kubelet            Started container nginx
  Normal  Pulling    10m    kubelet            Pulling image "redis"
  Normal  Created    10m    kubelet            Created container redis
  Normal  Started    10m    kubelet            Started container redis
  Normal  Pulling    10m    kubelet            Pulling image "memcached"
  Normal  Pulled     10m    kubelet            Successfully pulled image "redis" in 7.675954833s
  Normal  Pulled     10m    kubelet            Successfully pulled image "memcached" in 9.517307785s
  Normal  Created    10m    kubelet            Created container memcached
  Normal  Started    10m    kubelet            Started container memcached
[root@master01 ~]# kubectl describe pods nginx --all-namespaces
error: a resource cannot be retrieved by name across all namespaces
[root@master01 ~]# kubectl describe pods nginx --all-namespacesget podsdescribe pods nginxget pods --all-namespaces
NAMESPACE     NAME                                                   READY   STATUS             RESTARTS      AGE
default       admin-pod                                              1/1     Running            2 (52m ago)   164m
default       ds-kusc00201-xgb9l                                     1/1     Running            0             40m
default       ds-kusc00201-xkdtw                                     1/1     Running            0             40m
default       hungry-bear                                            0/1     CrashLoopBackOff   6 (10m ago)   10m
default       nginx                                                  3/3     Running            0             3m29s
default       web-project-268-84b655ccd9-jpwnz                       1/1     Running            0             173m
kube-system   coredns-565d847f94-48j9p                               1/1     Running            1 (42h ago)   42h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running            1 (42h ago)   42h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running            1 (42h ago)   42h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running            1 (42h ago)   42h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running            1 (42h ago)   42h
kube-system   kube-proxy-bk8v6                                       1/1     Running            0             42h
kube-system   kube-proxy-rfb9j                                       1/1     Running            1 (42h ago)   42h
kube-system   kube-proxy-vpx59                                       1/1     Running            0             42h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running            1 (42h ago)   42h
kube-system   weave-net-42gjg                                        2/2     Running            3 (42h ago)   42h
kube-system   weave-net-44k9t                                        2/2     Running            1 (42h ago)   42h
kube-system   weave-net-mdv4h                                        2/2     Running            0             42h
[root@master01 ~]# kuvim nginx.yaml 
"nginx.yaml" 16L, 238C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  - image: redis
    name: redis
  - image: memcached
    name: memcached
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      16,1All~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,1 ~@k   8~@k   7~@k   6~@k   7~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --7,13All^[  7,12All::q![root@master01 ~]# kubectl delete -f nginx.yaml 
pod "nginx" deleted
[root@master01 ~]# kubectl delete -f nginx.yaml vim
"nginx.yaml" 16L, 238C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  - image: redis
    name: redis
  - image: memcached
    name: memcached
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      7,12Alli -- INSERT --7,12All3432109 kucc814^[  7,13All~@k   6dd  
~                                                                                                                                                                                      6,3All~@k   5~@k   4dd  
~                                                                                                                                                                                      4,3Alldd  
~                                                                                                                                                                                      4,3All~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,3~@k   1~@k   2~@k   3~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ::wq 13,3Alldd  
~                                                                                                                                                                                      2,5::wq!"nginx.yaml" 12L, 176C written
[root@master01 ~]# vim nginx.yaml kubectl delete -fvimkubectl get pods --all-namespacesdescribe pods nginxget podsdescribe pods nginxget podscreate -f nginx.yaml vimkubectl create -f
pod/kucc8 created
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             2 (54m ago)     166m
ds-kusc00201-xgb9l                 1/1     Running             0               42m
ds-kusc00201-xkdtw                 1/1     Running             0               42m
hungry-bear                        0/1     CrashLoopBackOff    7 (7m43s ago)   12m
kucc8                              0/3     ContainerCreating   0               5s
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               175m
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS             RESTARTS        AGE
admin-pod                          1/1     Running            2 (54m ago)     166m
ds-kusc00201-xgb9l                 1/1     Running            0               42m
ds-kusc00201-xkdtw                 1/1     Running            0               42m
hungry-bear                        0/1     CrashLoopBackOff   7 (7m56s ago)   12m
kucc8                              3/3     Running            0               18s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               175m
[root@master01 ~]# kubectl get podskubectl describe pod^C
[root@master01 ~]# kubectl describe pod kucc8
Name:             kucc8
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Tue, 13 Sep 2022 15:11:23 +0530
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.44.0.3
IPs:
  IP:  10.44.0.3
Containers:
  nginx:
    Container ID:   containerd://866254a4e5b76ff7172cc97583b493d7bed941f4e5ecf9a1b8fd6f3cf7146327
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:fb18e3e674b78832837ea05854090f3ca692969348937f0fed6721917277737c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 13 Sep 2022 15:11:26 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ndpwf (ro)
  redis:
    Container ID:   containerd://b205f530b62c4e03e41f55807f4ff955ee416100a51abf740552f54bf09119c2
    Image:          redis
    Image ID:       docker.io/library/redis@sha256:4feb642f6382e6abf95c279ad4130a22f2d8b5e2d9c9e47e3a0ce51ed7ac597b
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 13 Sep 2022 15:11:28 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ndpwf (ro)
  memcached:
    Container ID:   containerd://91cf60a82daf5414caaefd128e70ee1a38515c047b93229784df94020a12e1b6
    Image:          memcached
    Image ID:       docker.io/library/memcached@sha256:c0e6637075167ec1cc888a9e5930d77b3be45d257c234b360e5bbede88a15208
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 13 Sep 2022 15:11:31 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ndpwf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-ndpwf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  67s   default-scheduler  Successfully assigned default/kucc8 to worker02.airtel.internal.lab
  Normal  Pulling    9m8s  kubelet            Pulling image "nginx"
  Normal  Pulled     9m5s  kubelet            Successfully pulled image "nginx" in 2.249594957s
  Normal  Created    9m5s  kubelet            Created container nginx
  Normal  Started    9m5s  kubelet            Started container nginx
  Normal  Pulling    9m5s  kubelet            Pulling image "redis"
  Normal  Created    9m3s  kubelet            Created container redis
  Normal  Started    9m3s  kubelet            Started container redis
  Normal  Pulling    9m3s  kubelet            Pulling image "memcached"
  Normal  Pulled     9m3s  kubelet            Successfully pulled image "redis" in 2.295849003s
  Normal  Pulled     9m    kubelet            Successfully pulled image "memcached" in 2.473001018s
  Normal  Created    9m    kubelet            Created container memcached
  Normal  Started    9m    kubelet            Started container memcached
[root@master01 ~]# kubectl get events
LAST SEEN   TYPE      REASON             OBJECT                   MESSAGE
55m         Normal    Pulling            pod/admin-pod            Pulling image "busybox"
55m         Normal    Created            pod/admin-pod            Created container admin-pod
55m         Normal    Started            pod/admin-pod            Started container admin-pod
55m         Normal    Pulled             pod/admin-pod            Successfully pulled image "busybox" in 2.161334761s
50m         Normal    Pulling            pod/ds-kusc00201-xgb9l   Pulling image "nginx"
50m         Normal    Pulled             pod/ds-kusc00201-xgb9l   Successfully pulled image "nginx" in 8.539970646s
50m         Normal    Created            pod/ds-kusc00201-xgb9l   Created container nginx
50m         Normal    Started            pod/ds-kusc00201-xgb9l   Started container nginx
44m         Normal    Scheduled          pod/ds-kusc00201-xgb9l   Successfully assigned default/ds-kusc00201-xgb9l to worker01.airtel.internal.lab
52m         Normal    Pulling            pod/ds-kusc00201-xkdtw   Pulling image "nginx"
52m         Normal    Pulled             pod/ds-kusc00201-xkdtw   Successfully pulled image "nginx" in 8.439861817s
52m         Normal    Created            pod/ds-kusc00201-xkdtw   Created container nginx
52m         Normal    Started            pod/ds-kusc00201-xkdtw   Started container nginx
44m         Normal    Scheduled          pod/ds-kusc00201-xkdtw   Successfully assigned default/ds-kusc00201-xkdtw to worker02.airtel.internal.lab
44m         Normal    SuccessfulCreate   daemonset/ds-kusc00201   Created pod: ds-kusc00201-xkdtw
44m         Normal    SuccessfulCreate   daemonset/ds-kusc00201   Created pod: ds-kusc00201-xgb9l
20m         Normal    Pulling            pod/hungry-bear          Pulling image "alpine"
20m         Normal    Pulled             pod/hungry-bear          Successfully pulled image "alpine" in 5.936855323s
20m         Normal    Created            pod/hungry-bear          Created container create
20m         Normal    Started            pod/hungry-bear          Started container create
19m         Normal    Pulling            pod/hungry-bear          Pulling image "alpine"
20m         Normal    Pulled             pod/hungry-bear          Successfully pulled image "alpine" in 2.193607766s
19m         Normal    Created            pod/hungry-bear          Created container checker
19m         Normal    Started            pod/hungry-bear          Started container checker
20m         Normal    Pulled             pod/hungry-bear          Successfully pulled image "alpine" in 2.235494893s
10m         Warning   BackOff            pod/hungry-bear          Back-off restarting failed container
20m         Normal    Pulled             pod/hungry-bear          Successfully pulled image "alpine" in 2.262974517s
19m         Normal    Pulled             pod/hungry-bear          Successfully pulled image "alpine" in 2.042899551s
14m         Normal    Scheduled          pod/hungry-bear          Successfully assigned default/hungry-bear to worker01.airtel.internal.lab
9m46s       Normal    Pulling            pod/kucc8                Pulling image "nginx"
9m43s       Normal    Pulled             pod/kucc8                Successfully pulled image "nginx" in 2.249594957s
9m43s       Normal    Created            pod/kucc8                Created container nginx
9m43s       Normal    Started            pod/kucc8                Started container nginx
9m43s       Normal    Pulling            pod/kucc8                Pulling image "redis"
9m41s       Normal    Pulled             pod/kucc8                Successfully pulled image "redis" in 2.295849003s
9m41s       Normal    Created            pod/kucc8                Created container redis
9m41s       Normal    Started            pod/kucc8                Started container redis
9m41s       Normal    Pulling            pod/kucc8                Pulling image "memcached"
9m38s       Normal    Pulled             pod/kucc8                Successfully pulled image "memcached" in 2.473001018s
9m38s       Normal    Created            pod/kucc8                Created container memcached
9m38s       Normal    Started            pod/kucc8                Started container memcached
105s        Normal    Scheduled          pod/kucc8                Successfully assigned default/kucc8 to worker02.airtel.internal.lab
15m         Normal    Pulling            pod/nginx                Pulling image "nginx"
15m         Normal    Pulled             pod/nginx                Successfully pulled image "nginx" in 2.12535198s
15m         Normal    Created            pod/nginx                Created container nginx
15m         Normal    Started            pod/nginx                Started container nginx
15m         Normal    Pulling            pod/nginx                Pulling image "redis"
15m         Normal    Pulled             pod/nginx                Successfully pulled image "redis" in 7.675954833s
15m         Normal    Created            pod/nginx                Created container redis
15m         Normal    Started            pod/nginx                Started container redis
15m         Normal    Pulling            pod/nginx                Pulling image "memcached"
14m         Normal    Pulled             pod/nginx                Successfully pulled image "memcached" in 9.517307785s
14m         Normal    Created            pod/nginx                Created container memcached
14m         Normal    Started            pod/nginx                Started container memcached
10m         Normal    Killing            pod/nginx                Stopping container nginx
10m         Normal    Killing            pod/nginx                Stopping container memcached
10m         Normal    Killing            pod/nginx                Stopping container redis
7m9s        Normal    Scheduled          pod/nginx                Successfully assigned default/nginx to worker02.airtel.internal.lab
[root@master01 ~]# [root@master01 ~]# kubectl get eventsdescribe pod kucc8get pods
NAME                               READY   STATUS             RESTARTS      AGE
admin-pod                          1/1     Running            2 (56m ago)   168m
ds-kusc00201-xgb9l                 1/1     Running            0             45m
ds-kusc00201-xkdtw                 1/1     Running            0             45m
hungry-bear                        0/1     CrashLoopBackOff   7 (10m ago)   15m
kucc8                              3/3     Running            0             2m34s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0             177m
[root@master01 ~]# kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" |\
> tr -s '[[:space:]]' '\n' |\
> sort |\
> uniq -c
      1 alpine
      1 busybox
      3 ghcr.io/weaveworks/launcher/weave-kube:2.8.1
      3 ghcr.io/weaveworks/launcher/weave-npc:2.8.1
      1 memcached
      3 nginx
      1 nginx:1.17
      1 redis
      2 registry.k8s.io/coredns/coredns:v1.9.3
      1 registry.k8s.io/etcd:3.5.4-0
      1 registry.k8s.io/kube-apiserver:v1.25.0
      1 registry.k8s.io/kube-controller-manager:v1.25.0
      3 registry.k8s.io/kube-proxy:v1.25.0
      1 registry.k8s.io/kube-scheduler:v1.25.0
[root@master01 ~]# kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" |tr -s '[[:space:]]' '\n' |sort |uniq -ckucc8
[root@master01 ~]# kubectl get pods kucc8 -o jsonpath="{.items[*].spec.containers[*].image}" |tr -s '[[:space:]]' '\n' |sort |uniq -c--all-namespaces
NAME                               READY   STATUS             RESTARTS      AGE
admin-pod                          1/1     Running            2 (57m ago)   169m
ds-kusc00201-xgb9l                 1/1     Running            0             45m
ds-kusc00201-xkdtw                 1/1     Running            0             45m
hungry-bear                        0/1     CrashLoopBackOff   7 (10m ago)   15m
kucc8                              3/3     Running            0             3m5s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0             178m
[root@master01 ~]# kubectl exec -it kucc8 -- bash
Defaulted container "nginx" out of: nginx, redis, memcached
root@kucc8:/# root@kucc8:/# ls l -l
total 12
drwxr-xr-x   2 root root 4096 Sep 12 00:00 bin
drwxr-xr-x   2 root root    6 Sep  3 12:10 boot
drwxr-xr-x   5 root root  360 Sep 13 09:41 dev
drwxr-xr-x   1 root root   41 Sep 13 06:29 docker-entrypoint.d
-rwxrwxr-x   1 root root 1202 Sep 13 06:29 docker-entrypoint.sh
drwxr-xr-x   1 root root   32 Sep 13 09:41 etc
drwxr-xr-x   2 root root    6 Sep  3 12:10 home
drwxr-xr-x   1 root root   45 Sep 12 00:00 lib
drwxr-xr-x   2 root root   34 Sep 12 00:00 lib64
drwxr-xr-x   2 root root    6 Sep 12 00:00 media
drwxr-xr-x   2 root root    6 Sep 12 00:00 mnt
drwxr-xr-x   2 root root    6 Sep 12 00:00 opt
dr-xr-xr-x 219 root root    0 Sep 13 09:41 proc
drwx------   2 root root   37 Sep 12 00:00 root
drwxr-xr-x   1 root root   38 Sep 13 09:41 run
drwxr-xr-x   2 root root 4096 Sep 12 00:00 sbin
drwxr-xr-x   2 root root    6 Sep 12 00:00 srv
dr-xr-xr-x  13 root root    0 Sep 13 09:41 sys
drwxrwxrwt   1 root root    6 Sep 13 06:29 tmp
drwxr-xr-x   1 root root   66 Sep 12 00:00 usr
drwxr-xr-x   1 root root   19 Sep 12 00:00 var
root@kucc8:/# red 
read          readarray     readlink      readonly      readprofile   realpath      remove-shell  renice        reset         resize2fs     resizepart    return        rev
root@kucc8:/# re  ls -l     redisredis
bash: redis: command not found
root@kucc8:/# ls -l 
total 12
drwxr-xr-x   2 root root 4096 Sep 12 00:00 bin
drwxr-xr-x   2 root root    6 Sep  3 12:10 boot
drwxr-xr-x   5 root root  360 Sep 13 09:41 dev
drwxr-xr-x   1 root root   41 Sep 13 06:29 docker-entrypoint.d
-rwxrwxr-x   1 root root 1202 Sep 13 06:29 docker-entrypoint.sh
drwxr-xr-x   1 root root   32 Sep 13 09:41 etc
drwxr-xr-x   2 root root    6 Sep  3 12:10 home
drwxr-xr-x   1 root root   45 Sep 12 00:00 lib
drwxr-xr-x   2 root root   34 Sep 12 00:00 lib64
drwxr-xr-x   2 root root    6 Sep 12 00:00 media
drwxr-xr-x   2 root root    6 Sep 12 00:00 mnt
drwxr-xr-x   2 root root    6 Sep 12 00:00 opt
dr-xr-xr-x 201 root root    0 Sep 13 09:41 proc
drwx------   2 root root   37 Sep 12 00:00 root
drwxr-xr-x   1 root root   38 Sep 13 09:41 run
drwxr-xr-x   2 root root 4096 Sep 12 00:00 sbin
drwxr-xr-x   2 root root    6 Sep 12 00:00 srv
dr-xr-xr-x  13 root root    0 Sep 13 09:41 sys
drwxrwxrwt   1 root root    6 Sep 13 06:29 tmp
drwxr-xr-x   1 root root   66 Sep 12 00:00 usr
drwxr-xr-x   1 root root   19 Sep 12 00:00 var
root@kucc8:/# ls -l root@kucc8:/# ls -l       redisls -lredis
exit
[root@master01 ~]# kubectl exec -it kucc8 -- bashget pods
NAME                               READY   STATUS             RESTARTS        AGE
admin-pod                          1/1     Running            2 (58m ago)     171m
ds-kusc00201-xgb9l                 1/1     Running            0               47m
ds-kusc00201-xkdtw                 1/1     Running            0               47m
hungry-bear                        0/1     CrashLoopBackOff   8 (7m16s ago)   17m
kucc8                              3/3     Running            0               4m49s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               3h
[root@master01 ~]# kubectl get sc
No resources found
[root@master01 ~]# kubectl get scvc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   42h
[root@master01 ~]# kubectl get svcconatiner
error: the server doesn't have a resource type "conatiner"
[root@master01 ~]# kubectl get conatinerpods
NAME                               READY   STATUS             RESTARTS       AGE
admin-pod                          1/1     Running            2 (59m ago)    171m
ds-kusc00201-xgb9l                 1/1     Running            0              48m
ds-kusc00201-xkdtw                 1/1     Running            0              48m
hungry-bear                        0/1     CrashLoopBackOff   8 (8m2s ago)   18m
kucc8                              3/3     Running            0              5m35s
web-project-268-84b655ccd9-jpwnz   1/1     Running            0              3h
[root@master01 ~]# kubectl get pods POD_NAME_HERE -o jsonpath='{.spec.containers[*].name}'kucc8
nginx redis memcached[root@master01 ~]# kubectl get pods kucc8 -o jsonpath='{.spec.containers[*].name}'conatinersvccpods^C
[root@master01 ~]# kubectl get pods kucc8 -o jsonpath='{.spec.containers[*].name}' -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (61m ago)     173m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               49m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               49m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   8 (9m45s ago)   19m     10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
kucc8                              3/3     Running            0               7m18s   10.44.0.3   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               3h2m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# vim hu
admin-pod.yaml           .bash_logout             .cshrc                   nfs-pv.yaml              pod-spec-KUCC00108.yaml  .viminfo                 
anaconda-ks.cfg          .bash_profile            ds-kusc00201.yaml        nginx.yaml               .ssh/                    web-project-268.yaml     
.bash_history            .bashrc                  .kube/                   nodes.txt                .tcshrc                  
[root@master01 ~]# vim ipod-spec-KUCC00108.yaml 
"pod-spec-KUCC00108.yaml" 23L, 496C  apiVersion: v1
kind: Pod
metadata:
  name: hungry-bear
spec:
  volumes:
    - name: workdiremptyDir:
  containers:
  - name: checker
    image: alpine
    command: ["/bin/sh", "-c", "if [ -f /workdir/calm.txt l; then sleep 100000; else exit l; fi"]
    volumeMounts:
    - name: workdirmountPath: /workdir
  initContainers:
  - name: create
    image: alpine
    command: ["/bin/sh", "-c", "touch /workdir/calm.txt"]
    volumeMounts:
    - name: workdirmountPath: /workdir
~                                                                                                                                                                                      ~                                                                                                                                                                                      9,10All::q![root@master01 ~]# vim pod-spec-KUCC00108.yaml kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            2 (63m ago)     175m    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               52m     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               52m     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   9 (6m57s ago)   22m     10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
kucc8                              3/3     Running            0               9m39s   10.44.0.3   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               3h4m    10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# [root@master01 ~]# kubectl create service pvviewer
error: unknown command "pvviewer"
See 'kubectl create service -h' for help and examples
[root@master01 ~]# kubectl create service pvviewer pvviewer pvviewer pvviewer pvviewer pvviewer pvviewera pvviewer
serviceaccount/pvviewer created
[root@master01 ~]# kubectl get sa
NAME       SECRETS   AGE
default    0         43h
pvviewer   0         5s
[root@master01 ~]# kubectl create clusterrole PersistentVolumespvviewer-role --verb=list --resource=PersistentVolumes
clusterrole.rbac.authorization.k8s.io/pvviewer-role created
[root@master01 ~]# kubectl create ClusterRoleBinding pvviewer-role-pvviewer-role-binding clusterrole=pvviewer-role --serviceaccount=default:pvviewer
error: unknown flag: --serviceaccount
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create ClusterRoleBinding pvviewer-role-binding clusterrole=pvviewer-role --serviceaccount=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewera=default:pvviewer
error: unknown flag: --sa
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create ClusterRoleBinding pvviewer-role-binding clusterrole=pvviewer-role --sa=default:pvviewer=default:pvviewere=default:pvviewerr=default:pvviewerv=default:pvvieweri=default:pvviewerc=default:pvviewere=default:pvviewera=default:pvviewerc=default:pvviewerc=default:pvviewero=default:pvvieweru=default:pvviewern=default:pvviewert=default:pvviewer
error: unknown flag: --serviceaccount
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl get serviceaccount
NAME       SECRETS   AGE
default    0         43h
pvviewer   0         4m48s
[root@master01 ~]# kubectl get serviceaccountcreate ClusterRoleBinding pvviewer-role-binding clusterrole=pvviewer-role --serviceaccount=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewerserviceaccount=default:pvviewer---serviceaccount=default:pvviewer-serviceaccount=default:pvviewer
error: unknown flag: --clusterrole
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create ClusterRoleBinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewerserviceaccount=default:pvviewerserviceaccount=default:pvviewer 
Error: must specify one of -f and -k

error: unknown command "ClusterRoleBinding pvviewer-role-binding clusterrole=pvviewer-role serviceaccount=default:pvviewer"
See 'kubectl create -h' for help and examples
[root@master01 ~]# kubectl create ClusterRoleBinding pvviewer-role-binding clusterrole=pvviewer-role serviceaccount=default:pvviewer--serviceaccount=default:pvviewer
error: unknown shorthand flag: 'c' in -clusterrole=pvviewer-role
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create
Error: must specify one of -f and -k

Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

Examples:
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json

Available Commands:
  clusterrole           Create a cluster role
  clusterrolebinding    Create a cluster role binding for a particular cluster role
  configmap             Create a config map from a local file, directory or literal value
  cronjob               Create a cron job with the specified name
  deployment            Create a deployment with the specified name
  ingress               Create an ingress with the specified name
  job                   Create a job with the specified name
  namespace             Create a namespace with the specified name
  poddisruptionbudget   Create a pod disruption budget with the specified name
  priorityclass         Create a priority class with the specified name
  quota                 Create a quota with the specified name
  role                  Create a role with single rule
  rolebinding           Create a role binding for a particular role or cluster role
  secret                Create a secret using specified subcommand
  service               Create a service using a specified subcommand
  serviceaccount        Create a service account with the specified name
  token                 Request a service account token

Options:
    --allow-missing-template-keys=true:
If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to
golang and jsonpath output formats.

    --dry-run='none':
Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
sending it. If server strategy, submit server-side request without persisting the resource.

    --edit=false:
Edit the API resource before creating

    --field-manager='kubectl-create':
Name of the manager used to track field ownership.

    -f, --filename=[]:
Filename, directory, or URL to files to use to create the resource

    -k, --kustomize='':
Process the kustomization directory. This flag can't be used together with -f or -R.

    -o, --output='':
Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath,
jsonpath-as-json, jsonpath-file).

    --raw='':
Raw URI to POST to the server.  Uses the transport specified by the kubeconfig file.

    -R, --recursive=false:
Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests
organized within the same directory.

    --save-config=false:
If true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will
be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.

    -l, --selector='':
Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
objects must satisfy all of the specified label constraints.

    --show-managed-fields=false:
If true, keep the managedFields when printing objects in JSON or YAML format.

    --template='':
Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format
is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

    --validate='strict':
Must be one of: strict (or true), warn, ignore (or false). "true" or "strict" will use a schema to validate
the input and fail the request if invalid. It will perform server side validation if ServerSideFieldValidation
is enabled on the api-server, but will fall back to less reliable client-side validation if not. "warn" will
warn about unknown or duplicate fields without blocking the request if server-side field validation is enabled
on the API server, and behave as "ignore" otherwise. "false" or "ignore" will not perform any schema
validation, silently dropping any unknown or duplicate fields.

    --windows-line-endings=false:
Only relevant if --edit=true. Defaults to the line ending native to your platform.

Usage:
  kubectl create -f FILENAME [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl create clusterrolebinding pvviewer ClusterRoleBinding pvviewer-role-binding -clusterrole=pvviewer-role -serviceaccount=default:pvviewerclusterrolebinding pvviewer clusterrole clusterrolepvviewer-role-binding clusterrole=pvviewer-role service accountaccount=default:pvviewer
error: required flag(s) "clusterrole" not set
[root@master01 ~]# kubectl create clusterrolebinding pvviewer-role-binding clusterrole=pvviewer-role serviceaccount=default:pvviewer ClusterRoleBinding pvviewer-role-binding -clusterrole=pvviewer-role -serviceaccount=default:pvviewer clusterrolebinding pvviewer-role-binding clusterrole=pvviewer-role serviceaccount=default:pvviewerkubectl create clusterrolebinding pvviewer-role-binding clusterrole=pvviewer-role serviceaccount=default:pvviewer--
error: exactly one NAME is required, got 2
See 'kubectl create clusterrolebinding -h' for help and examples
[root@master01 ~]# kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role serviceaccount=default:pvviewer--help
Create a cluster role binding for a particular cluster role.

Examples:
  # Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role
  kubectl create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1

Options:
    --allow-missing-template-keys=true:
If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to
golang and jsonpath output formats.

    --clusterrole='':
ClusterRole this ClusterRoleBinding should reference

    --dry-run='none':
Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
sending it. If server strategy, submit server-side request without persisting the resource.

    --field-manager='kubectl-create':
Name of the manager used to track field ownership.

    --group=[]:
Groups to bind to the clusterrole. The flag can be repeated to add multiple groups.

    -o, --output='':
Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath,
jsonpath-as-json, jsonpath-file).

    --save-config=false:
If true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will
be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.

    --serviceaccount=[]:
Service accounts to bind to the clusterrole, in the format <namespace>:<name>. The flag can be repeated to add
multiple service accounts.

    --show-managed-fields=false:
If true, keep the managedFields when printing objects in JSON or YAML format.

    --template='':
Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format
is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

    --validate='strict':
Must be one of: strict (or true), warn, ignore (or false). "true" or "strict" will use a schema to validate
the input and fail the request if invalid. It will perform server side validation if ServerSideFieldValidation
is enabled on the api-server, but will fall back to less reliable client-side validation if not. "warn" will
warn about unknown or duplicate fields without blocking the request if server-side field validation is enabled
on the API server, and behave as "ignore" otherwise. "false" or "ignore" will not perform any schema
validation, silently dropping any unknown or duplicate fields.

Usage:
  kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname]
[--serviceaccount=namespace:serviceaccountname] [--dry-run=server|client|none] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl create clusterrolebinding --helppvviewer-role-binding --clusterrole=pvviewer-role serviceaccount=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer=default:pvviewer--serviceaccount=default:pvviewer--clusterrole
clusterrolebinding.rbac.authorization.k8s.io/pvviewer-role-binding created
[root@master01 ~]# kubectl get verify auth can-i list ersistentVolumespersistentVolumesersistentVolumesPersistentVolumes -as system:serviceaccount :default:pvviewer
error: unknown shorthand flag: 'a' in -as
See 'kubectl auth can-i --help' for usage.
[root@master01 ~]# kubectl auth can-i list PersistentVolumes -as system:serviceaccount:default:pvviewer-
Warning: resource 'persistentvolumes' is not namespace scoped

yes
[root@master01 ~]# kubectl auth can-i list PersistentVolumes --as system:serviceaccount:default:pvviewercreate clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer--helppvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewerauth can-i list PersistentVolumes -as system:serviceaccount:default:pvviewer-
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# cat /etc/kubernetes/admin.conf 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3hNVEUxTURVeU5Gb1hEVE15TURrd09ERTFNRFV5TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS1NHClVXaFJROWhBNjR3NWNrVDNtUlZlZUJDU1VXSkpUNGRkbVUxU21nT3plcStMVlpPdDEvOVN1QjN6RlFGdXRTcjgKMTBnWFAwR2E5SXFaUGhaNm5kMDluQUx5d3k5b2hsQlFCT0ExcEprU3A4WkhMZXhTQ2QxK2JZLzZYd0xTeFZzdwpmTzdISHdEZGZQZVEvSVNXVEYzWDA2MHdtamtkYVhLY00xejVick1OMXFXc0JWdjlVRlZaa21XN2JWZTVqR1MzCmRPV3kwZjNBb0tKbFU2YzNYOGNzektsekxYck5OcTNucldCL2hEclYrUzFCZnUxbTZPZ1lnOFIraE9MRWY0dlkKOWJvMG9FSDFUVklvTFJBa1FzSFJpb0NnSmRZL01PbjV6emhjcGpLUUVyRDl6S0FrL0diOHpYS3UzdVA0d3BCNgpuNWQyZTZvdzRKNXhGQ053YnpzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDL3lSNHlQQWcrNzBIZldGOFlyUmhFcDI3cVNNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRHU3bDdmWTRGZ09pZm52Q1l3bApQR0twSFhCY05TTUF4bDBwZlN2SGJLZ2xzQWFNUzVGQndLMkZ6VlVaVEZEcVNHYU9ZVkV0R095L092TzFCamU5CkNwUmZXb09vb1FXZnlhZ3hMVU1jNjNUOHhFMERadWc5Y0svMTkyRFpqOUphYzk1SFFGMDlhT3plZ3M1UzVieTYKd3NseEhjTmpENFgyUnJVQVlOZEVJTkVkWmZXTU9hL1hsVTlGcWc4WXhmdkZLb1dWcmRtV1gwWEVMdHg1bWk2UQpaVlhkZU9nQ1U1bTVFZ243cWV0UTlDM0FRMVZwNklER0Nodm1TYXNkNTNDa2hEdVd2WjUwKzYrSksyeTdnbVVWCldZUDZvWDZja1orUFM3VE1WMDJGbGFBSDZKMHlNY2VVLytQZDlPbHFtS0dKWnVvTmQ2WC9nQ0d5Z2RmcUpvTjYKRlFRPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://172.16.102.22:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJWWJBUHVmbDUzZHd3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBNU1URXhOVEExTWpSYUZ3MHlNekE1TVRFeE5UQTFNamhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTI3ZmZRRStoaFhRMVNKcWIKYW1nUHVCNTBaeGVkcndRd0lGTHFDdGRBYnFjRldzeGFKVlQ1ZHYwK3NRd0JhaWR6dUhPb3ZJcTNKOEVLUFcybApoWXJ5cU9oNjcwYmZ0Z3I5NjFibzJmU3M4angxd0h5bWlNVWYxK0V6M0d2WmQ0dWVwUXR3eUJwSThtVTdyZkJiClJYendJcFJhaytOZjU1Q09lZlNzQkZ5Q3A0K3FOWnZXeENoeFNVVjNwbUhKcmhLdHBXSXFWeCtTMWM1WU1zRCsKdHBaRE4vWmgrT0hWTjhQUkZ1RjJRcFBESDE1bTBQSlZubjNveGZtZ1dGVndGT3laN05sWmo0YTVvalFoWkYyZQo0Q2lNaVpDUDZQMVFCMjMvSmREOVhaTHBBbXNwY2U2WGt6ejVwM2JqMUNqWjFzSkVpSWIwQ1liN3hCdXdpL0d0Cjg1bWFCUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JRdjhrZU1qd0lQdTlCMzFoZkdLMFlSS2R1NgprakFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBQWl3NndXYWhHQ3VhbDVRSkU4Rys2YTVJZ25IMGs0OGFRWVM3CmxyYjhtcG9RQWlxWW5GVUdIc1dTTjhVaDRvUmlhZjR1YWdPaWVRbjlvVisxVE5MQVozbjFoRVJmZEJuU1pNOFoKVWhzOVVBTnByTlJYMHV4YXI5aUpPMlh0cHdzNHJxMEFzOEZNZVpnUC95Z0dCWVptQVRCM2ZsNk5jb01zOU9xeApaeVVXMHZmTWhhdjBHbVIvRUhKa3EzZ0dzM3BneDdybVhpTUVqZU5GT2ZmbWtnM3RYU2FCUUJNcjcvQWVQN0h5ClQ2SElxY0pzdWIxYk9SU2xHQ09IS21QV1Z1a21VN3Fka1BvNnNQdVc0RFdaSjk2cldlM0dXMmRKdmNibC9ZMlAKMllJeUdhZ0t1QjU5TkhuQlpyQXhleWdWekhBWHN1MXNjeUFWVFNnQ0dkbmc5UEZMN1E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: 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
[root@master01 ~]# [root@master01 ~]# cat /etc/kubernetes/admin.conf 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3hNVEUxTURVeU5Gb1hEVE15TURrd09ERTFNRFV5TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS1NHClVXaFJROWhBNjR3NWNrVDNtUlZlZUJDU1VXSkpUNGRkbVUxU21nT3plcStMVlpPdDEvOVN1QjN6RlFGdXRTcjgKMTBnWFAwR2E5SXFaUGhaNm5kMDluQUx5d3k5b2hsQlFCT0ExcEprU3A4WkhMZXhTQ2QxK2JZLzZYd0xTeFZzdwpmTzdISHdEZGZQZVEvSVNXVEYzWDA2MHdtamtkYVhLY00xejVick1OMXFXc0JWdjlVRlZaa21XN2JWZTVqR1MzCmRPV3kwZjNBb0tKbFU2YzNYOGNzektsekxYck5OcTNucldCL2hEclYrUzFCZnUxbTZPZ1lnOFIraE9MRWY0dlkKOWJvMG9FSDFUVklvTFJBa1FzSFJpb0NnSmRZL01PbjV6emhjcGpLUUVyRDl6S0FrL0diOHpYS3UzdVA0d3BCNgpuNWQyZTZvdzRKNXhGQ053YnpzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDL3lSNHlQQWcrNzBIZldGOFlyUmhFcDI3cVNNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRHU3bDdmWTRGZ09pZm52Q1l3bApQR0twSFhCY05TTUF4bDBwZlN2SGJLZ2xzQWFNUzVGQndLMkZ6VlVaVEZEcVNHYU9ZVkV0R095L092TzFCamU5CkNwUmZXb09vb1FXZnlhZ3hMVU1jNjNUOHhFMERadWc5Y0svMTkyRFpqOUphYzk1SFFGMDlhT3plZ3M1UzVieTYKd3NseEhjTmpENFgyUnJVQVlOZEVJTkVkWmZXTU9hL1hsVTlGcWc4WXhmdkZLb1dWcmRtV1gwWEVMdHg1bWk2UQpaVlhkZU9nQ1U1bTVFZ243cWV0UTlDM0FRMVZwNklER0Nodm1TYXNkNTNDa2hEdVd2WjUwKzYrSksyeTdnbVVWCldZUDZvWDZja1orUFM3VE1WMDJGbGFBSDZKMHlNY2VVLytQZDlPbHFtS0dKWnVvTmQ2WC9nQ0d5Z2RmcUpvTjYKRlFRPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://172.16.102.22:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJWWJBUHVmbDUzZHd3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBNU1URXhOVEExTWpSYUZ3MHlNekE1TVRFeE5UQTFNamhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTI3ZmZRRStoaFhRMVNKcWIKYW1nUHVCNTBaeGVkcndRd0lGTHFDdGRBYnFjRldzeGFKVlQ1ZHYwK3NRd0JhaWR6dUhPb3ZJcTNKOEVLUFcybApoWXJ5cU9oNjcwYmZ0Z3I5NjFibzJmU3M4angxd0h5bWlNVWYxK0V6M0d2WmQ0dWVwUXR3eUJwSThtVTdyZkJiClJYendJcFJhaytOZjU1Q09lZlNzQkZ5Q3A0K3FOWnZXeENoeFNVVjNwbUhKcmhLdHBXSXFWeCtTMWM1WU1zRCsKdHBaRE4vWmgrT0hWTjhQUkZ1RjJRcFBESDE1bTBQSlZubjNveGZtZ1dGVndGT3laN05sWmo0YTVvalFoWkYyZQo0Q2lNaVpDUDZQMVFCMjMvSmREOVhaTHBBbXNwY2U2WGt6ejVwM2JqMUNqWjFzSkVpSWIwQ1liN3hCdXdpL0d0Cjg1bWFCUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JRdjhrZU1qd0lQdTlCMzFoZkdLMFlSS2R1NgprakFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBQWl3NndXYWhHQ3VhbDVRSkU4Rys2YTVJZ25IMGs0OGFRWVM3CmxyYjhtcG9RQWlxWW5GVUdIc1dTTjhVaDRvUmlhZjR1YWdPaWVRbjlvVisxVE5MQVozbjFoRVJmZEJuU1pNOFoKVWhzOVVBTnByTlJYMHV4YXI5aUpPMlh0cHdzNHJxMEFzOEZNZVpnUC95Z0dCWVptQVRCM2ZsNk5jb01zOU9xeApaeVVXMHZmTWhhdjBHbVIvRUhKa3EzZ0dzM3BneDdybVhpTUVqZU5GT2ZmbWtnM3RYU2FCUUJNcjcvQWVQN0h5ClQ2SElxY0pzdWIxYk9SU2xHQ09IS21QV1Z1a21VN3Fka1BvNnNQdVc0RFdaSjk2cldlM0dXMmRKdmNibC9ZMlAKMllJeUdhZ0t1QjU5TkhuQlpyQXhleWdWekhBWHN1MXNjeUFWVFNnQ0dkbmc5UEZMN1E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBMjdmZlFFK2hoWFExU0pxYmFtZ1B1QjUwWnhlZHJ3UXdJRkxxQ3RkQWJxY0ZXc3hhCkpWVDVkdjArc1F3QmFpZHp1SE9vdklxM0o4RUtQVzJsaFlyeXFPaDY3MGJmdGdyOTYxYm8yZlNzOGp4MXdIeW0KaU1VZjErRXozR3ZaZDR1ZXBRdHd5QnBJOG1VN3JmQmJSWHp3SXBSYWsrTmY1NUNPZWZTc0JGeUNwNCtxTlp2Vwp4Q2h4U1VWM3BtSEpyaEt0cFdJcVZ4K1MxYzVZTXNEK3RwWkROL1poK09IVk44UFJGdUYyUXBQREgxNW0wUEpWCm5uM294Zm1nV0ZWd0ZPeVo3TmxaajRhNW9qUWhaRjJlNENpTWlaQ1A2UDFRQjIzL0pkRDlYWkxwQW1zcGNlNlgKa3p6NXAzYmoxQ2paMXNKRWlJYjBDWWI3eEJ1d2kvR3Q4NW1hQlFJREFRQUJBb0lCQUFTbys3VXpleXVIY1hIZQp4WmtXSis4eHhpcEJsL2lVVUNqL1dUTnRxVkl2ZksrV1Vmc2t1RDNMMVQ2UUVKK2R3cGlRNjIxRVMrb3Q3L2VwCmthOHBhVWduMWlPcER5bWZSZzl0d25hTTkvWVRUalFNOE1OSjJ3a0xIN3gzRlIreCtHVnJ2cktmekJoSUNXSUIKRFE2R1BkbWQ1K1huRnF1a3AwRHk3WXNTSXp0R2lMVU1RcE5xK2RhakROZW5kN2cvVGFkSWpZMmEwYVlWSnVQbworM2tMKyt2d0JoVVVIM2ZzR013U1ArLy9mRGg2MXhUUTdYZ1VKSmxxekxFakpyMHZ3STYvT2dTbTRzOU1BaHZwCjUyNzJqY1NWaVJhTEMwNEowYjBuUmhRSEdKUTJPMXF5VnlicnJ4T2FFVGpmdGNQRjNzb2ZFWVZXelU5VzE0N2oKUzJhTnNTa0NnWUVBK1NOM2RHZm92KzNVNzN2TTVudkREaTVJN1IxcG1pSGh0UWRKMGxmTEU1N1VkYVp5SjVpVwpsbm9jZ0JyL0p2NkFrMXlkNmxSbW1YS2ZZOWcwQm9xY2VtQVNFWFg3RE5yWFJ6T1NCRkhaczNhRVFtcGRud2doCmdKZXFaajZPbkZ0R3c3QU05Mnk3b2htVXdSL2lqYUp4VnFGN3lseTBzdzk3OHo2bWNvc1YvUHNDZ1lFQTRjVDYKMUp0UmdxWU16bVA4Z2dHQVEwVlZmbVM5RzRlVFVaSGpsSnROWnQxSlZSanZ3dm84NzNCZ0NJYTRUSko5Wm56TQowTVIwQWczWWp3V0FHWXdRY2xYdlNsVU5FRExFMWNqclp0UHRlRnU5aVlVNnl2Tk9BajVtMHV2NENFU2xCNmlGCmtTOHhDeEhvblRDYWgvaTJIdVRjRkpxMmllQnQyUHBDVnJHZEZQOENnWUVBdG4yaE1YWXZpZTduSzZpdVZTSFEKcGVHSGI1QTgyemI0UW9jZEx1TTZFSndtR2l1YXNLQlhwYys2Y3FmVHdEZHA3UWRKY096ejNuQ1VpaWxES290Nwp4VFA3NkNoWWhYa0RlZGN2bk5ScnQ4WVhhU0xLZ3dRTXJrbnAra3pXWDNlSXBTeWV3dG8xc3E0MFJTWmE1ODVXCllOT1dmb3RiblZHNVJGcmV4MFpVQytNQ2dZQjlkQml4aFhFZ3VTYUtsVkxkNXZtZjUrb2U1dG1TYjU2MEIrNC8KU0tFVkZNZklDcWhJWHdiM1FpbUs4MWw3NnpiVmVXblNaQ2ZXY0YweTlDcC9oNjRuNm9xaWNtcGs0dUV2cm9ybwpROEZFVFM0Z09iRTJ6MFk4eHZJYUtGZFdPTmEvVDZ5UXhYbHFIbFIwN0dvaVBsanRCOTBuY2VVVzdtZjcxdXlBCkFnZzRmd0tCZ0NiNzdUZmNSQzlIcWEvRUtSbUZCOXZGWEtNNUp6KzRHVmExV2g1Q2JwbEN0ZmVWSXVWc0FEU3UKU1UxY3lsOC9IS1JrMzArU3c4SmhHYXZYN21tZkVuSEE4OGxIZHAzbk96RENQRVBoeHV6dGp2SlY2QlhVeW8yUQo5aUZuVXc1QlVpRHlrV3p5NFFlV0d3MlBnOG0weDVrTThrRFNyZmxXZmwrWGFLNTFvQkJhCi0tLS0tRU5EIFJTQSBQUklWQVRFIEtFWS0tLS0tCg==
[root@master01 ~]# cat /etc/kubernetes/admin.conf 
admin.conf               controller-manager.conf  kubelet.conf             manifests/               pki/                     scheduler.conf           
[root@master01 ~]# cat /etc/kubernetes/kubelet.conf 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3hNVEUxTURVeU5Gb1hEVE15TURrd09ERTFNRFV5TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS1NHClVXaFJROWhBNjR3NWNrVDNtUlZlZUJDU1VXSkpUNGRkbVUxU21nT3plcStMVlpPdDEvOVN1QjN6RlFGdXRTcjgKMTBnWFAwR2E5SXFaUGhaNm5kMDluQUx5d3k5b2hsQlFCT0ExcEprU3A4WkhMZXhTQ2QxK2JZLzZYd0xTeFZzdwpmTzdISHdEZGZQZVEvSVNXVEYzWDA2MHdtamtkYVhLY00xejVick1OMXFXc0JWdjlVRlZaa21XN2JWZTVqR1MzCmRPV3kwZjNBb0tKbFU2YzNYOGNzektsekxYck5OcTNucldCL2hEclYrUzFCZnUxbTZPZ1lnOFIraE9MRWY0dlkKOWJvMG9FSDFUVklvTFJBa1FzSFJpb0NnSmRZL01PbjV6emhjcGpLUUVyRDl6S0FrL0diOHpYS3UzdVA0d3BCNgpuNWQyZTZvdzRKNXhGQ053YnpzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDL3lSNHlQQWcrNzBIZldGOFlyUmhFcDI3cVNNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRHU3bDdmWTRGZ09pZm52Q1l3bApQR0twSFhCY05TTUF4bDBwZlN2SGJLZ2xzQWFNUzVGQndLMkZ6VlVaVEZEcVNHYU9ZVkV0R095L092TzFCamU5CkNwUmZXb09vb1FXZnlhZ3hMVU1jNjNUOHhFMERadWc5Y0svMTkyRFpqOUphYzk1SFFGMDlhT3plZ3M1UzVieTYKd3NseEhjTmpENFgyUnJVQVlOZEVJTkVkWmZXTU9hL1hsVTlGcWc4WXhmdkZLb1dWcmRtV1gwWEVMdHg1bWk2UQpaVlhkZU9nQ1U1bTVFZ243cWV0UTlDM0FRMVZwNklER0Nodm1TYXNkNTNDa2hEdVd2WjUwKzYrSksyeTdnbVVWCldZUDZvWDZja1orUFM3VE1WMDJGbGFBSDZKMHlNY2VVLytQZDlPbHFtS0dKWnVvTmQ2WC9nQ0d5Z2RmcUpvTjYKRlFRPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://172.16.102.22:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:master01.airtel.internal.lab
  name: system:node:master01.airtel.internal.lab@kubernetes
current-context: system:node:master01.airtel.internal.lab@kubernetes
kind: Config
preferences: {}
users:
- name: system:node:master01.airtel.internal.lab
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
[root@master01 ~]# cat /etc/kubernetes/kubelet.conf epki/
apiserver.crt                 apiserver.key                 ca.crt                        front-proxy-ca.crt            front-proxy-client.key        
apiserver-etcd-client.crt     apiserver-kubelet-client.crt  ca.key                        front-proxy-ca.key            sa.key                        
apiserver-etcd-client.key     apiserver-kubelet-client.key  etcd/                         front-proxy-client.crt        sa.pub                        
[root@master01 ~]# cat /etc/kubernetes/pki/etcd/
ca.crt                  healthcheck-client.crt  peer.crt                server.crt              
ca.key                  healthcheck-client.key  peer.key                server.key              
[root@master01 ~]# cat /etc/kubernetes/pki/etcd//
admin.conf               controller-manager.conf  kubelet.conf             manifests/               pki/                     scheduler.conf           
[root@master01 ~]# cat /etc/kubernetes/scheduler.conf 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3hNVEUxTURVeU5Gb1hEVE15TURrd09ERTFNRFV5TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS1NHClVXaFJROWhBNjR3NWNrVDNtUlZlZUJDU1VXSkpUNGRkbVUxU21nT3plcStMVlpPdDEvOVN1QjN6RlFGdXRTcjgKMTBnWFAwR2E5SXFaUGhaNm5kMDluQUx5d3k5b2hsQlFCT0ExcEprU3A4WkhMZXhTQ2QxK2JZLzZYd0xTeFZzdwpmTzdISHdEZGZQZVEvSVNXVEYzWDA2MHdtamtkYVhLY00xejVick1OMXFXc0JWdjlVRlZaa21XN2JWZTVqR1MzCmRPV3kwZjNBb0tKbFU2YzNYOGNzektsekxYck5OcTNucldCL2hEclYrUzFCZnUxbTZPZ1lnOFIraE9MRWY0dlkKOWJvMG9FSDFUVklvTFJBa1FzSFJpb0NnSmRZL01PbjV6emhjcGpLUUVyRDl6S0FrL0diOHpYS3UzdVA0d3BCNgpuNWQyZTZvdzRKNXhGQ053YnpzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZDL3lSNHlQQWcrNzBIZldGOFlyUmhFcDI3cVNNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBRHU3bDdmWTRGZ09pZm52Q1l3bApQR0twSFhCY05TTUF4bDBwZlN2SGJLZ2xzQWFNUzVGQndLMkZ6VlVaVEZEcVNHYU9ZVkV0R095L092TzFCamU5CkNwUmZXb09vb1FXZnlhZ3hMVU1jNjNUOHhFMERadWc5Y0svMTkyRFpqOUphYzk1SFFGMDlhT3plZ3M1UzVieTYKd3NseEhjTmpENFgyUnJVQVlOZEVJTkVkWmZXTU9hL1hsVTlGcWc4WXhmdkZLb1dWcmRtV1gwWEVMdHg1bWk2UQpaVlhkZU9nQ1U1bTVFZ243cWV0UTlDM0FRMVZwNklER0Nodm1TYXNkNTNDa2hEdVd2WjUwKzYrSksyeTdnbVVWCldZUDZvWDZja1orUFM3VE1WMDJGbGFBSDZKMHlNY2VVLytQZDlPbHFtS0dKWnVvTmQ2WC9nQ0d5Z2RmcUpvTjYKRlFRPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://172.16.102.22:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:kube-scheduler
  name: system:kube-scheduler@kubernetes
current-context: system:kube-scheduler@kubernetes
kind: Config
preferences: {}
users:
- name: system:kube-scheduler
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUREVENDQWZXZ0F3SUJBZ0lJU29ENUtrb2I1RU13RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TWpBNU1URXhOVEExTWpSYUZ3MHlNekE1TVRFeE5UQTFNamhhTUNBeApIakFjQmdOVkJBTVRGWE41YzNSbGJUcHJkV0psTFhOamFHVmtkV3hsY2pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCCkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUxXcEVMUVZqOHNyT0EwZ04xUzU5U24wL2tpOHB2NVdtdFhjM09JczBIVlYKN0xsMHk2Rm9Hem9XMUQ2Z1lYRFpjTk52N3ZOb3RMZzlscmNSRlJXdHlEeUp0UDdtbG5RMzRjRnFjWjFGU3o3RAovT2UvQVpJRlR3OHNiNGtjbDQvUHZLWTRkSmRDQnhTR0krYm9yVmpqLzV4aGF0WExJNzFxK3RrQkR1dnpEeTR0CmdQdGR0R1ZOdDRRU28wMlcrQkdqY080SjlTNzRXYy9NdXBORGhxNUZ4Rncra2FXR25ITktMamVmdUR3cmhBMUMKaVNYd0hHTXJQVzZFZG1ObkxyUXBpWlBKOEczenhNRFByaENESWFPeXZ2QzBOc2dhTldFeVdlLzZLOE1xZEVMUgpsdG9XRUphWFUrKytpTis4RmM2NDA0eU14Q1RQK2wvbzAvb0JIc2Z0UGNNQ0F3RUFBYU5XTUZRd0RnWURWUjBQCkFRSC9CQVFEQWdXZ01CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TUNNQXdHQTFVZEV3RUIvd1FDTUFBd0h3WUQKVlIwakJCZ3dGb0FVTC9KSGpJOENEN3ZRZDlZWHhpdEdFU25idXBJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQgpBQjZkNURDWDR6Y2JwVjBmV0VXZXUxbjRuMEJtdTdiUy9Rb0Q4UndoVVQyOEtaWWUvTndjdUdjOVlUUHl2K1JTCjZLTnJqWndvUlV3ZVE0Zkp6aUtENndKWE1EbWdENTc1cVkwV2pncWR5MUtmbGtYU0FaUnhSYXBvdWxDVVhMWlAKRklCMlRWS3pkWkE0V21GOExGZmNRaEltblZEZmdYMWJsVnIxTFlLQlNHOEVLWFl2TWsxam9zVXhkS0U1WlNsego5NHZzdExjbGdtSHRpSHVGdG9hZ0NGZGJGMnRreTdORnV5TXlqRkQ3TDVQbmJjazRiZFJ4MmRKVEJ5eHBaT3lJCmlmRGxDSjY3QmMzdWtROWw0RHJXaEQzeDhqRWczR3ovTWdTRHhPUzU3Qnc5aDQxMkZ0ckxlbFlDVklMdTR2Uk8KR1ZESDBncXpUZURWMzZrME5iR3FScnc9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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
[root@master01 ~]# cd /etc/kubernetes/
[root@master01 kubernetes]# ll
total 32
-rw------- 1 root root 5637 Sep 11 20:35 admin.conf
-rw------- 1 root root 5673 Sep 11 20:35 controller-manager.conf
-rw------- 1 root root 2049 Sep 11 20:35 kubelet.conf
drwxr-xr-x 2 root root  113 Sep 11 20:35 manifests
drwxr-xr-x 3 root root 4096 Sep 11 20:35 pki
-rw------- 1 root root 5621 Sep 11 20:35 scheduler.conf
[root@master01 kubernetes]# ls -l manifests/
etcd.yaml                     kube-apiserver.yaml           kube-controller-manager.yaml  kube-scheduler.yaml           
[root@master01 kubernetes]# ls -l manifests/etcd.yaml vim 
"manifests/etcd.yaml" 81L, 2420C  apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.16.102.22:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://172.16.102.22:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://172.16.102.22:2380
    - --initial-cluster=master01.airtel.internal.lab=https://172.16.102.22:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://172.16.102.22:23791,1Top~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20::q[root@master01 kubernetes]# vim  manifests/etcd.yaml cat
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.16.102.22:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://172.16.102.22:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://172.16.102.22:2380
    - --initial-cluster=master01.airtel.internal.lab=https://172.16.102.22:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://172.16.102.22:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://172.16.102.22:2380
    - --name=master01.airtel.internal.lab
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.4-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}
[root@master01 kubernetes]# "
[root@master01 ~]#    wget https://github.com/etcd-io/etcd/releases/download/v${RELEASE}/etcd-v${RELEASE}-linux-amd64.tar.gz
--2022-09-13 16:28:47--  https://github.com/etcd-io/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz
Resolving github.com (github.com)... 20.207.73.82
Connecting to github.com (github.com)|20.207.73.82|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/11225014/2917d000-6cce-11e9-843f-9aa76ea24cb1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220913T110416Z&X-Amz-Expires=300&X-Amz-Signature=f41d3d835d9e93fb35b3b968a5ae7d460fd70cb5c0b6c32656298257210b49f3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=11225014&response-content-disposition=attachment%3B%20filename%3Detcd-v3.3.13-linux-amd64.tar.gz&response-content-type=application%2Foctet-stream [following]
--2022-09-13 16:28:47--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/11225014/2917d000-6cce-11e9-843f-9aa76ea24cb1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220913%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220913T110416Z&X-Amz-Expires=300&X-Amz-Signature=f41d3d835d9e93fb35b3b968a5ae7d460fd70cb5c0b6c32656298257210b49f3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=11225014&response-content-disposition=attachment%3B%20filename%3Detcd-v3.3.13-linux-amd64.tar.gz&response-content-type=application%2Foctet-stream
Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...
Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10423953 (9.9M) [application/octet-stream]
Saving to: etcd-v3.3.13-linux-amd64.tar.gz

etcd-v3.3.13-linux-amd64.tar.gz                 0%[                                                                                                 ]       0  --.-KB/s               etcd-v3.3.13-linux-amd64.tar.gz                40%[======================================>                                                          ]   4.01M  16.5MB/s               etcd-v3.3.13-linux-amd64.tar.gz                75%[========================================================================>                        ]   7.52M  15.9MB/s               etcd-v3.3.13-linux-amd64.tar.gz                80%[=============================================================================>                   ]   8.01M  11.3MB/s               etcd-v3.3.13-linux-amd64.tar.gz               100%[================================================================================================>]   9.94M  13.6MB/s    in 0.7s    

2022-09-13 16:28:49 (13.6 MB/s) - etcd-v3.3.13-linux-amd64.tar.gz saved [10423953/10423953]

[root@master01 ~]#    tar xvf etcd-v${RELEASE}-linux-amd64.tar.gz
etcd-v3.3.13-linux-amd64/
etcd-v3.3.13-linux-amd64/README.md
etcd-v3.3.13-linux-amd64/Documentation/
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/interacting_v3.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/api_concurrency_reference_v3.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/limit.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/local_cluster.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/api_grpc_gateway.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/_index.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/grpc_naming.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/apispec/
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/apispec/swagger/
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/apispec/swagger/rpc.swagger.json
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/apispec/swagger/v3lock.swagger.json
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/apispec/swagger/v3election.swagger.json
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/experimental_apis.md
etcd-v3.3.13-linux-amd64/Documentation/dev-guide/api_reference_v3.md
etcd-v3.3.13-linux-amd64/Documentation/integrations.md
etcd-v3.3.13-linux-amd64/Documentation/README.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/_index.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md
etcd-v3.3.13-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md
etcd-v3.3.13-linux-amd64/Documentation/rfc/
etcd-v3.3.13-linux-amd64/Documentation/rfc/_index.md
etcd-v3.3.13-linux-amd64/Documentation/production-users.md
etcd-v3.3.13-linux-amd64/Documentation/metrics.md
etcd-v3.3.13-linux-amd64/Documentation/_index.md
etcd-v3.3.13-linux-amd64/Documentation/branch_management.md
etcd-v3.3.13-linux-amd64/Documentation/platforms/
etcd-v3.3.13-linux-amd64/Documentation/platforms/container-linux-systemd.md
etcd-v3.3.13-linux-amd64/Documentation/platforms/freebsd.md
etcd-v3.3.13-linux-amd64/Documentation/platforms/aws.md
etcd-v3.3.13-linux-amd64/Documentation/platforms/_index.md
etcd-v3.3.13-linux-amd64/Documentation/faq.md
etcd-v3.3.13-linux-amd64/Documentation/dl_build.md
etcd-v3.3.13-linux-amd64/Documentation/reporting_bugs.md
etcd-v3.3.13-linux-amd64/Documentation/tuning.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrade_3_4.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrading-etcd.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrade_3_2.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrade_3_1.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/_index.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrade_3_5.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrade_3_0.md
etcd-v3.3.13-linux-amd64/Documentation/upgrades/upgrade_3_3.md
etcd-v3.3.13-linux-amd64/Documentation/dev-internal/
etcd-v3.3.13-linux-amd64/Documentation/dev-internal/logging.md
etcd-v3.3.13-linux-amd64/Documentation/dev-internal/discovery_protocol.md
etcd-v3.3.13-linux-amd64/Documentation/dev-internal/release.md
etcd-v3.3.13-linux-amd64/Documentation/learning/
etcd-v3.3.13-linux-amd64/Documentation/learning/auth_design.md
etcd-v3.3.13-linux-amd64/Documentation/learning/glossary.md
etcd-v3.3.13-linux-amd64/Documentation/learning/client-architecture.md
etcd-v3.3.13-linux-amd64/Documentation/learning/data_model.md
etcd-v3.3.13-linux-amd64/Documentation/learning/api_guarantees.md
etcd-v3.3.13-linux-amd64/Documentation/learning/_index.md
etcd-v3.3.13-linux-amd64/Documentation/learning/why.md
etcd-v3.3.13-linux-amd64/Documentation/learning/api.md
etcd-v3.3.13-linux-amd64/Documentation/learning/client-feature-matrix.md
etcd-v3.3.13-linux-amd64/Documentation/learning/learner.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/
etcd-v3.3.13-linux-amd64/Documentation/op-guide/authentication.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/versioning.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/hardware.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/grafana.json
etcd-v3.3.13-linux-amd64/Documentation/op-guide/monitoring.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/configuration.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/v2-migration.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/maintenance.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/runtime-configuration.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/recovery.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/clustering.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/etcd3_alert.rules
etcd-v3.3.13-linux-amd64/Documentation/op-guide/_index.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/runtime-reconf-design.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/etcd3_alert.rules.yml
etcd-v3.3.13-linux-amd64/Documentation/op-guide/security.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/performance.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/etcd-sample-grafana.png
etcd-v3.3.13-linux-amd64/Documentation/op-guide/grpc_proxy.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/gateway.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/container.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/supported-platform.md
etcd-v3.3.13-linux-amd64/Documentation/op-guide/failures.md
etcd-v3.3.13-linux-amd64/Documentation/demo.md
etcd-v3.3.13-linux-amd64/README-etcdctl.md
etcd-v3.3.13-linux-amd64/etcdctl
etcd-v3.3.13-linux-amd64/READMEv2-etcdctl.md
etcd-v3.3.13-linux-amd64/etcd
[root@master01 ~]#    cd etcd-v${RELEASE}-linux-amd64
[root@master01 etcd-v3.3.13-linux-amd64]#    sudo mv etcdctl /usr/local/bin
[root@master01 etcd-v3.3.13-linux-amd64]# cd
[root@master01 ~]# [root@master01 ~]# cdETCTETCD_API=3 etcdctl --endpoints=login as: root
Server refused our key
root@172.16.102.22's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Sep 13 16:28:31 2022 from 192.168.200.13
[root@master01 ~]# ETCD_API=3 etcdctl --endpoints==~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.13 16:55:57 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Server refused our key
root@172.16.102.22's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Sep 13 16:31:40 2022 from 192.168.200.13
[root@master01 ~]# [root@master01 ~]# cat /etc/kubernetes/pmanifests/
etcd.yaml                     kube-apiserver.yaml           kube-controller-manager.yaml  kube-scheduler.yaml           
[root@master01 ~]# cat /etc/kubernetes/manifests/etcd.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.16.102.22:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://172.16.102.22:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://172.16.102.22:2380
    - --initial-cluster=master01.airtel.internal.lab=https://172.16.102.22:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://172.16.102.22:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://172.16.102.22:2380
    - --name=master01.airtel.internal.lab
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.4-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}
[root@master01 ~]# ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --carcertcert= --cert --key= /opt/etcd-snapshot.db/etc/kubernetes/pki/etcd/ca.crt=/etc/kubernetes/pki/etcd/server.crt ---key= /opt/etcd-snapshot.db
/etc/kubernetes/pki/etcd/server.key /opt/etcd-snapshot.db /opt/etcd-snapshot.dbs /opt/etcd-snapshot.dbav /opt/etcd-snapshot.dbe /opt/etcd-snapshot.db
Error: unknown command "save" for "etcdctl"
Run 'etcdctl --help' for usage.
Error: unknown command "save" for "etcdctl"
[root@master01 ~]# ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt ---key=/etc/kubernetes/pki/etcd/server.key save /opt/etcd-snapshot.dbsnaphot s
Snapshot saved at /opt/etcd-snapshot.db
[root@master01 ~]# ls - l/ope /opt/
cni/              config.toml       containerd/       etcd-snapshot.db  KULM00201/        
[root@master01 ~]# ls - l /opt/
cni/              config.toml       containerd/       etcd-snapshot.db  KULM00201/        
[root@master01 ~]# ls - l /opt/
ls: cannot access '-': No such file or directory
ls: cannot access 'l': No such file or directory
/opt/:
cni  config.toml  containerd  etcd-snapshot.db  KULM00201
[root@master01 ~]# ls - l /opt/
total 2296
drwxr-xr-x 3 root root      17 Sep 11 20:33 cni
-rw-r--r-- 1 root root     886 Aug 26 04:45 config.toml
drwx--x--x 4 root root      28 Sep 11 20:30 containerd
-rw-r--r-- 1 root root 2342944 Sep 13 16:55 etcd-snapshot.db
drwxr-xr-x 2 root root      36 Sep 13 14:24 KULM00201
[root@master01 ~]# ETCD_API=3 caetcdctl --write-out=table snapshot status /opt/setcd-snapshot.db 
Incorrect Usage.

NAME:
   etcdctl - A simple command line client for etcd.

WARNING:
   Environment variable ETCDCTL_API is not set; defaults to etcdctl v2.
   Set environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API.

USAGE:
   etcdctl [global options] command [command options] [arguments...]
   
VERSION:
   3.3.13
   
COMMANDS:
     backup          backup an etcd directory
     cluster-health  check the health of the etcd cluster
     mk              make a new key with a given value
     mkdir           make a new directory
     rm              remove a key or a directory
     rmdir           removes the key if it is an empty directory or a key-value pair
     get             retrieve the value of a key
     ls              retrieve a directory
     set             set the value of a key
     setdir          create a new directory or update an existing directory TTL
     update          update an existing key with a given value
     updatedir       update an existing directory
     watch           watch a key for changes
     exec-watch      watch a key for changes and exec an executable
     member          member add, remove and list subcommands
     user            user add, grant and revoke subcommands
     role            role add, grant and revoke subcommands
     auth            overall auth controls
     help, h         Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                          output cURL commands which can be used to reproduce the request
   --no-sync                        don't synchronize cluster information before sending request
   --output simple, -o simple       output response in the given format (simple, `extended` or `json`) (default: "simple")
   --discovery-srv value, -D value  domain name to query for SRV records describing cluster endpoints
   --insecure-discovery             accept insecure SRV records describing cluster endpoints
   --peers value, -C value          DEPRECATED - "--endpoints" should be used instead
   --endpoint value                 DEPRECATED - "--endpoints" should be used instead
   --endpoints value                a comma-delimited list of machine addresses in the cluster (default: "http://127.0.0.1:2379,http://127.0.0.1:4001")
   --cert-file value                identify HTTPS client using this SSL certificate file
   --key-file value                 identify HTTPS client using this SSL key file
   --ca-file value                  verify certificates of HTTPS-enabled servers using this CA bundle
   --username value, -u value       provide username[:password] and prompt if password is not supplied.
   --timeout value                  connection timeout per request (default: 2s)
   --total-timeout value            timeout for the command execution (except watch) (default: 5s)
   --help, -h                       show help
   --version, -v                    print the version
   
flag provided but not defined: -write-out
[root@master01 ~]# ETCD_API=3 etcdctl --write-out=table snapshot status /opt/etcd-snapshot.db --
Incorrect Usage.

NAME:
   etcdctl - A simple command line client for etcd.

WARNING:
   Environment variable ETCDCTL_API is not set; defaults to etcdctl v2.
   Set environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API.

USAGE:
   etcdctl [global options] command [command options] [arguments...]
   
VERSION:
   3.3.13
   
COMMANDS:
     backup          backup an etcd directory
     cluster-health  check the health of the etcd cluster
     mk              make a new key with a given value
     mkdir           make a new directory
     rm              remove a key or a directory
     rmdir           removes the key if it is an empty directory or a key-value pair
     get             retrieve the value of a key
     ls              retrieve a directory
     set             set the value of a key
     setdir          create a new directory or update an existing directory TTL
     update          update an existing key with a given value
     updatedir       update an existing directory
     watch           watch a key for changes
     exec-watch      watch a key for changes and exec an executable
     member          member add, remove and list subcommands
     user            user add, grant and revoke subcommands
     role            role add, grant and revoke subcommands
     auth            overall auth controls
     help, h         Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --debug                          output cURL commands which can be used to reproduce the request
   --no-sync                        don't synchronize cluster information before sending request
   --output simple, -o simple       output response in the given format (simple, `extended` or `json`) (default: "simple")
   --discovery-srv value, -D value  domain name to query for SRV records describing cluster endpoints
   --insecure-discovery             accept insecure SRV records describing cluster endpoints
   --peers value, -C value          DEPRECATED - "--endpoints" should be used instead
   --endpoint value                 DEPRECATED - "--endpoints" should be used instead
   --endpoints value                a comma-delimited list of machine addresses in the cluster (default: "http://127.0.0.1:2379,http://127.0.0.1:4001")
   --cert-file value                identify HTTPS client using this SSL certificate file
   --key-file value                 identify HTTPS client using this SSL key file
   --ca-file value                  verify certificates of HTTPS-enabled servers using this CA bundle
   --username value, -u value       provide username[:password] and prompt if password is not supplied.
   --timeout value                  connection timeout per request (default: 2s)
   --total-timeout value            timeout for the command execution (except watch) (default: 5s)
   --help, -h                       show help
   --version, -v                    print the version
   
flag provided but not defined: -write-out
[root@master01 ~]# ETCDCTL_API=3 etcdctl --write-out=table snapshot status /opt/etcd-snapshot.db
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| a5b4a236 |   224563 |        844 |     2.3 MB |
+----------+----------+------------+------------+
[root@master01 ~]# ETCDCTL_API=3 etcdctl --write-out=table snapshot status /opt/etcd-snapshot.db_API=3 etcdctl --write-out=table snapshot status /opt/etcd-snapshot.db CTL_API=3 etcdctl --write-out=table snapshot status /opt/etcd-snapshot.dbpkubectl pvc cpcv create pv-analytics --dry-run=client -o yaml
error: unknown command "pv" for "kubectl"

Did you mean this?
cp
[root@master01 ~]# kubectl pv create pv-analytics --dry-run=client -o yaml  pv
Error: must specify one of -f and -k

error: unknown command "pv pv-analytics"
See 'kubectl create -h' for help and examples
[root@master01 ~]# kubectl create pv pv-analytics --dry-run=client -o yamlPersistentVolume 
Error: must specify one of -f and -k

error: unknown command "PersistentVolume"
See 'kubectl create -h' for help and examples
[root@master01 ~]# kubectl create PersistentVolume pv
Error: must specify one of -f and -k

error: unknown command "persistentvolume"
See 'kubectl create -h' for help and examples
[root@master01 ~]# kubectl create persistentvolume  --help
Create a resource from a file or from stdin.

 JSON and YAML formats are accepted.

Examples:
  # Create a pod using the data in pod.json
  kubectl create -f ./pod.json
  
  # Create a pod based on the JSON passed into stdin
  cat pod.json | kubectl create -f -
  
  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  kubectl create -f registry.yaml --edit -o json

Available Commands:
  clusterrole           Create a cluster role
  clusterrolebinding    Create a cluster role binding for a particular cluster role
  configmap             Create a config map from a local file, directory or literal value
  cronjob               Create a cron job with the specified name
  deployment            Create a deployment with the specified name
  ingress               Create an ingress with the specified name
  job                   Create a job with the specified name
  namespace             Create a namespace with the specified name
  poddisruptionbudget   Create a pod disruption budget with the specified name
  priorityclass         Create a priority class with the specified name
  quota                 Create a quota with the specified name
  role                  Create a role with single rule
  rolebinding           Create a role binding for a particular role or cluster role
  secret                Create a secret using specified subcommand
  service               Create a service using a specified subcommand
  serviceaccount        Create a service account with the specified name
  token                 Request a service account token

Options:
    --allow-missing-template-keys=true:
If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to
golang and jsonpath output formats.

    --dry-run='none':
Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
sending it. If server strategy, submit server-side request without persisting the resource.

    --edit=false:
Edit the API resource before creating

    --field-manager='kubectl-create':
Name of the manager used to track field ownership.

    -f, --filename=[]:
Filename, directory, or URL to files to use to create the resource

    -k, --kustomize='':
Process the kustomization directory. This flag can't be used together with -f or -R.

    -o, --output='':
Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath,
jsonpath-as-json, jsonpath-file).

    --raw='':
Raw URI to POST to the server.  Uses the transport specified by the kubeconfig file.

    -R, --recursive=false:
Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests
organized within the same directory.

    --save-config=false:
If true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will
be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.

    -l, --selector='':
Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
objects must satisfy all of the specified label constraints.

    --show-managed-fields=false:
If true, keep the managedFields when printing objects in JSON or YAML format.

    --template='':
Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format
is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

    --validate='strict':
Must be one of: strict (or true), warn, ignore (or false). "true" or "strict" will use a schema to validate
the input and fail the request if invalid. It will perform server side validation if ServerSideFieldValidation
is enabled on the api-server, but will fall back to less reliable client-side validation if not. "warn" will
warn about unknown or duplicate fields without blocking the request if server-side field validation is enabled
on the API server, and behave as "ignore" otherwise. "false" or "ignore" will not perform any schema
validation, silently dropping any unknown or duplicate fields.

    --windows-line-endings=false:
Only relevant if --edit=true. Defaults to the line ending native to your platform.

Usage:
  kubectl create -f FILENAME [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# vim pvana-analytics.yaml
"pv-analytics.yaml" [New File]  ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                0,0-1Alli -- INSERT --0,1AllPersistentVolume1,17^[  1,16Allu1 line less; before #1  2 seconds ago0,0-1Alli -- INSERT --0,1AllapiVersion: v1 kind: PersistentVolume metadata: name: pv-analytics spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytics1,162^[  1,161All::q![root@master01 ~]# vim pv-analytics.yaml pv-analytics.yaml pv-analytics.yaml pv-analytics.yamln pv-analytics.yamla pv-analytics.yamln pv-analytics.yamlo pv-analytics.yaml
[ New File ]  GNU nano 2.9.8                                                           pv-analytics.yaml                                                                     ^G Get Help     ^O Write Out    ^W Where Is     ^K Cut Text     ^J Justify^C Cur PosM-U UndoM-A Mark Text   M-] To Bracket  M- Previous^X Exit^R Read File    ^\ Replace^U Uncut Text   ^T To Spell     ^_ Go To Line   M-E RedoM-6 Copy Text   M-W WhereIs NextM- NextModifiedi$alytics apiVersion: v1 kind: PersistentVolume metadata: name: pv-analytics spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analyti                                                                                                                                              Save modified buffer?  (Answering "No" will DISCARD changes.)                                                                                                     Y Yes N No  C Cancel^G Get HelpM-D DOS FormatM-A AppendM-B Backup File^C Cancel         M-M Mac FormatM-P Prepend^T To FilesFile Name to Write: pv-analytics.yaml                        X [ Cancelled ]^O Write Out    ^W Where Is     ^K Cut Text     ^J Justify^C Cur PosM-U UndoM-A Mark Text   M-] To Bracket  M- PreviousX Exit  ^R Read File    ^\ Replace      ^U Uncut Text   ^T To Spell     ^_ Go To Line   M-E RedoM-6 Copy Text   M-W WhereIs NextM- NextXSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                                                     Y Yes N No  C Cancel^G Get HelpM-D DOS FormatM-A AppendM-B Backup File^C Cancel         M-M Mac FormatM-P Prepend^T To FilesFile Name to Write: pv-analytics.yaml                         [ Cancelled ]^O Write Out    ^W Where Is     ^K Cut Text     ^J Justify^C Cur PosM-U UndoM-A Mark Text   M-] To Bracket  M- PreviousX Exit  ^R Read File    ^\ Replace      ^U Uncut Text   ^T To Spell     ^_ Go To Line   M-E RedoM-6 Copy Text   M-W WhereIs NextM- NextSave modified buffer?  (Answering "No" will DISCARD changes.)                                                                                                     Y Yes N No  C Cancel[root@master01 ~]# nano pv-analytics.yamlvim
"pv-analytics.yaml" [New File]  ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                0,0-1Alli -- INSERT --0,1AllapiVersion: v1 kind: PersistentVolume metadata: name: pv-analytics spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytics1,16210  234567891012892019876kind: PersistentVolume metadata: name: pv-analytics spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic2,1 2345678910123456789201234metadata: name: pv-analytics spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic3,1 2345678910134567892012345678930spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic4,1 23456789101245678765432109 87capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic5,1 capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic2 capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic345678910829301234567876543210298  accessModes: - ReadWriteMany hostPath: path: /pv/data-analytic6,3 45567891012343storage: 100Mi6,3All storage: 100Mi4 storage: 100Mi57432345678910123456789201293012  hostPath: path: /pv/data-analytic8,3 45678910123  path: /pv/data-analytic9,3  path: /pv/data-analytic4 path: /pv/data-analytic5^[  9,4All::wq!"pv-analytics.yaml" [New] 9L, 183C written
[root@master01 ~]# vim pv-analytics.yaml
"pv-analytics.yaml" 9L, 183C  apiVersion: v1
kind: PersistentVolume
metadata: name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  accessModes: - ReadWriteMany
  hostPath:
    path: /pv/data-analytic
~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                9,4All~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1i -- INSERT --3,11Allname: pv-analytics4,1All name: pv-analytics2 name: pv-analytics3^[  4,2All~@k   5~@k   6~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   ~@k   ~@k   ~@k   7~@k   8~@k   9~@k   10,12::wq!"pv-analytics.yaml" 10L, 186C written
[root@master01 ~]# vim pv-analytics.yamlnanovimkubectl create -f pv-analytics.yaml 
error: error parsing pv-analytics.yaml: error converting YAML to JSON: yaml: line 8: block sequence entries are not allowed in this context
[root@master01 ~]# kubectl create -f pv-analytics.yaml vim pv-analytics.yaml
"pv-analytics.yaml" 10L, 186C  apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  accessModes: - ReadWriteMany
  hostPath:
    path: /pv/data-analytic
~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                ~                                                                                                                                                                10,12All::88,3All~@k   9~@k   10,3~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   o -- INSERT --11,5All    4321
    2,54321apiVersion: v1
kind: PersistentVolumemetadata:  name: task-pv-volume
  labels:    type: localspec:  storageClassName: manual
  capacity:    storage: 10Gi  accessModes:    - ReadWriteOnce
  hostPath:    path: "/mnt/data"25,221"0"19"8"7"6"5"4"3"2/pv/data-analytic"294,123,2019876M7a8n9y202,151,18710607100M8i90,1219,19^[  19,18Alldd  
~                                                                                                                                                                19,3All~@k   8~@k   7~@k   6dd  
~                                                                                                                                                                16,5Alldd  
~                                                                                                                                                                16,1All~@k   5~@K   22i -- INSERT --15,22All32101987432109 pv-analytics21^[  15,20All~@k   4,9 ~@k   3,20~@k   2,14~@k   1,0-1~@k   0,20 ~@k   9,12 ~@k   8,20~@k   7,19~@k   6,12~@k   5,6 ~@k   4,20~@k   3,1~@k   2,2~@k   1,15~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   dd  
~                                                                                                                                                                1,1Alldd  
~                                                                                                                                                                1,1Alldd  
~                                                                                                                                                                1,3Alldd  
~                                                                                                                                                                1,1Alldd  
~                                                                                                                                                                1,3Alldd  
~                                                                                                                                                                1,5Alldd  
~                                                                                                                                                                1,3Alldd  
~                                                                                                                                                                1,3Alld~@k    ~                                                                                                                                                                ~                                                                                                                                                                1,0-1All::wq!"pv-analytics.yaml" 12L, 184C written
[root@master01 ~]# vim pv-analytics.yamlkubectl create -f pv-analytics.yaml 
persistentvolume/pv-analytics created
[root@master01 ~]# kubegeectl get pv
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01       5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     3h24m
nfs-pv02       5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     3h24m
nfs-pv03       10Gi       RWX            Recycle          Available   /nfs-pvc03   nfs                     3h24m
pv-analytics   100Mi      RWX            Retain           Available                                        10s
[root@master01 ~]# kubbectl get nodes
NAME                           STATUS   ROLES           AGE   VERSION
master01.airtel.internal.lab   Ready    control-plane   45h   v1.25.0
worker01.airtel.internal.lab   Ready    <none>          44h   v1.25.0
worker02.airtel.internal.lab   Ready    <none>          44h   v1.25.0
[root@master01 ~]# kubectl get nodes -o wide
NAME                           STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                           KERNEL-VERSION              CONTAINER-RUNTIME
master01.airtel.internal.lab   Ready    control-plane   45h   v1.25.0   172.16.102.22   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
worker01.airtel.internal.lab   Ready    <none>          44h   v1.25.0   172.16.102.23   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
worker02.airtel.internal.lab   Ready    <none>          44h   v1.25.0   172.16.102.24   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
[root@master01 ~]# [root@master01 ~]# kubectl get nodes -o wide
NAME                           STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                           KERNEL-VERSION              CONTAINER-RUNTIME
master01.airtel.internal.lab   Ready    control-plane   45h   v1.25.0   172.16.102.22   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
worker01.airtel.internal.lab   Ready    <none>          44h   v1.25.0   172.16.102.23   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
worker02.airtel.internal.lab   Ready    <none>          44h   v1.25.0   172.16.102.24   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
[root@master01 ~]# [root@master01 ~]# kubectl get nodes -o wide
NAME                           STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                           KERNEL-VERSION              CONTAINER-RUNTIME
master01.airtel.internal.lab   Ready    control-plane   45h   v1.25.0   172.16.102.22   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
worker01.airtel.internal.lab   Ready    <none>          44h   v1.25.0   172.16.102.23   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
worker02.airtel.internal.lab   Ready    <none>          44h   v1.25.0   172.16.102.24   <none>        Rocky Linux 8.6 (Green Obsidian)   4.18.0-372.9.1.el8.x86_64   containerd://1.6.8
[root@master01 ~]# logout
=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.14 16:25:59 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Server refused our key
root@172.16.102.22's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Sep 13 16:50:36 2022 from 192.168.200.13
[root@master01 ~]# [root@master01 ~]# kubectl get nopods
NAME                               READY   STATUS             RESTARTS        AGE
admin-pod                          1/1     Running            27 (54m ago)    27h
ds-kusc00201-xgb9l                 1/1     Running            0               25h
ds-kusc00201-xkdtw                 1/1     Running            0               25h
hungry-bear                        0/1     CrashLoopBackOff   298 (10m ago)   25h
kucc8                              3/3     Running            0               25h
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               27h
[root@master01 ~]# [root@master01 ~]# kubectl get pods -o swide
NAME                               READY   STATUS             RESTARTS          AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            28 (12m ago)      28h   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0                 26h   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0                 26h   10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   302 (8m12s ago)   25h   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
kucc8                              3/3     Running            0                 25h   10.44.0.3   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0                 28h   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl taint nodes nodename dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule dedicated=groupName:NoScheduleworker01.airtel.internal.lab dedicated=groupName:NoSchedule dedicated=groupName:NoSchedulee dedicated=groupName:NoSchedulen dedicated=groupName:NoSchedulev dedicated=groupName:NoSchedule_ dedicated=groupName:NoSchedulet dedicated=groupName:NoScheduley dedicated=groupName:NoSchedulep dedicated=groupName:NoSchedulee dedicated=groupName:NoSchedule= dedicated=groupName:NoSchedulep dedicated=groupName:NoScheduler dedicated=groupName:NoScheduleo dedicated=groupName:NoScheduled dedicated=groupName:NoScheduleu dedicated=groupName:NoSchedulec dedicated=groupName:NoSchedulet dedicated=groupName:NoSchedulei dedicated=groupName:NoScheduleo dedicated=groupName:NoSchedulen dedicated=groupName:NoSchedule: dedicated=groupName:NoSchedule dedicated=groupName:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule:NoSchedule#
[root@master01 ~]# kubectl describe node worker01.airtel.internal.lab |more
Name:               worker01.airtel.internal.lab
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker01.airtel.internal.lab
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 11 Sep 2022 20:57:31 +0530
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  worker01.airtel.internal.lab
  AcquireTime:     <unset>
  RenewTime:       Wed, 14 Sep 2022 16:33:32 +0530
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sun, 11 Sep 2022 20:51:49 +0530   Sun, 11 Sep 2022 20:51:49 +0530   WeaveIsUp                    Weave pod has set this
  MemoryPressure       False   Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:01 +0530   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:01 +0530   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:01 +0530   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:22 +0530   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.16.102.23
  Hostname:    worker01.airtel.internal.lab
Capacity:
  cpu:                4
  ephemeral-storage:  49197600Ki
--More--  hugepages-1Gi:      0
--More--  hugepages-2Mi:      0
--More--  memory:             7949636Ki
--More--  pods:               110
--More--Allocatable:
--More--  cpu:                4
--More--  ephemeral-storage:  45340508085
--More--  hugepages-1Gi:      0
--More--  hugepages-2Mi:      0
--More--  memory:             7847236Ki
--More--  pods:               110
--More--System Info:
--More--  Machine ID:                 eb4d5930d89b402d940dcef33ef62837
--More--  System UUID:                8c420f42-08d6-8586-efc4-ce36d830e4b7
--More--  Boot ID:                    45eee788-2e48-4a8b-b5ba-ffec3b70d0ea
--More--  Kernel Version:             4.18.0-372.9.1.el8.x86_64
--More--  OS Image:                   Rocky Linux 8.6 (Green Obsidian)
--More--  Operating System:           linux
--More--  Architecture:               amd64
--More--  Container Runtime Version:  containerd://1.6.8
--More--  Kubelet Version:            v1.25.0
--More--  Kube-Proxy Version:         v1.25.0
--More--Non-terminated Pods:          (5 in total)
--More--  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
--More--  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
--More--  default                     ds-kusc00201-xgb9l                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         26h
--More--  default                     hungry-bear                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         25h
--More--  default                     web-project-268-84b655ccd9-jpwnz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28h
--More--  kube-system                 kube-proxy-vpx59                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d19h
--More--  kube-system                 weave-net-44k9t                     100m (2%)     0 (0%)      200Mi (2%)       0 (0%)         2d19h
--More--Allocated resources:
--More--  (Total limits may be over 100 percent, i.e., overcommitted.)
--More--  Resource           Requests    Limits
--More--  --------           --------    ------
--More--  cpu                100m (2%)   0 (0%)
--More--  memory             200Mi (2%)  0 (0%)
--More--  ephemeral-storage  0 (0%)      0 (0%)
--More--  hugepages-1Gi      0 (0%)      0 (0%)
--More--  hugepages-2Mi      0 (0%)      0 (0%)
--More--Events:              <none>
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# kubectl describe node worker01.airtel.internal.lab |more#kubectl taint nodes worker01.airtel.internal.lab env_type=production:NoSchedule
node/worker01.airtel.internal.lab tainted
[root@master01 ~]# kubectl taint nodes worker01.airtel.internal.lab env_type=production:NoScheduledescribe node worker01.airtel.internal.lab |moremore
Name:               worker01.airtel.internal.lab
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker01.airtel.internal.lab
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 11 Sep 2022 20:57:31 +0530
Taints:             env_type=production:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  worker01.airtel.internal.lab
  AcquireTime:     <unset>
  RenewTime:       Wed, 14 Sep 2022 16:34:23 +0530
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sun, 11 Sep 2022 20:51:49 +0530   Sun, 11 Sep 2022 20:51:49 +0530   WeaveIsUp                    Weave pod has set this
  MemoryPressure       False   Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:01 +0530   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:01 +0530   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:01 +0530   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Wed, 14 Sep 2022 16:31:52 +0530   Sun, 11 Sep 2022 20:51:22 +0530   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.16.102.23
  Hostname:    worker01.airtel.internal.lab
Capacity:
  cpu:                4
  ephemeral-storage:  49197600Ki
--More--  hugepages-1Gi:      0
--More--  hugepages-2Mi:      0
--More--  memory:             7949636Ki
--More--  pods:               110
--More--Allocatable:
--More--  cpu:                4
--More--  ephemeral-storage:  45340508085
--More--  hugepages-1Gi:      0
--More--  hugepages-2Mi:      0
--More--  memory:             7847236Ki
--More--  pods:               110
--More--System Info:
--More--  Machine ID:                 eb4d5930d89b402d940dcef33ef62837
--More--  System UUID:                8c420f42-08d6-8586-efc4-ce36d830e4b7
--More--  Boot ID:                    45eee788-2e48-4a8b-b5ba-ffec3b70d0ea
--More--  Kernel Version:             4.18.0-372.9.1.el8.x86_64
--More--  OS Image:                   Rocky Linux 8.6 (Green Obsidian)
--More--  Operating System:           linux
--More--  Architecture:               amd64
--More--  Container Runtime Version:  containerd://1.6.8
--More--  Kubelet Version:            v1.25.0
--More--  Kube-Proxy Version:         v1.25.0
--More--Non-terminated Pods:          (5 in total)
--More--  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
--More--  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
--More--  default                     ds-kusc00201-xgb9l                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         26h
--More--  default                     hungry-bear                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         25h
--More--  default                     web-project-268-84b655ccd9-jpwnz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28h
--More--  kube-system                 kube-proxy-vpx59                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d19h
--More--  kube-system                 weave-net-44k9t                     100m (2%)     0 (0%)      200Mi (2%)       0 (0%)         2d19h
--More--Allocated resources:
--More--  (Total limits may be over 100 percent, i.e., overcommitted.)
--More--  Resource           Requests    Limits
--More--  --------           --------    ------
--More--  cpu                100m (2%)   0 (0%)
--More--  memory             200Mi (2%)  0 (0%)
--More--  ephemeral-storage  0 (0%)      0 (0%)
--More--  hugepages-1Gi      0 (0%)      0 (0%)
--More--  hugepages-2Mi      0 (0%)      0 (0%)
--More--Events:              <none>
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# kubectl kubectl describe node worker01.airtel.internal.lab |moregrep -i taint
Taints:             env_type=production:NoSchedule
[root@master01 ~]# kubectl create run dev-redis --image=redis:alpine --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: dev-redis
  name: dev-redis
spec:
  containers:
  - image: redis:alpine
    name: dev-redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
[root@master01 ~]# vim dev-redis.yaml-redis.yaml-redis.yaml-redis.yamlp-redis.yamlo-redis.yamld-redis.yaml
"pod-redis.yaml" [New File]  ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       0,0-1Alli -- INSERT --0,1AllapiVersion: v1
kind: Podmetadata:  creationTimestamp: nunull
  labels:    run: dev-redis  name: dev-redisspec:  conta  containers:
  - image: redis:alpine    name: dev-redis    resource    resources: {}
  dnsPolicy: ClusterFirst  restartPolicy: Alwaysstatus: {}{}15,11^[  15,10All~@k   {}4~@k   3~@k   2~@k   1~@k   0~@k   1~@k   2~@k   3~@k   4~@k   3~@k   2dd  
~                                                                                                                                                                       12,3Alldd  
~                                                                                                                                                                       12,3Alldd  
~                                                                                                                                                                       12,1All~@k   1~@k   2~@k   3o -- INSERT --12,5Top12,5All43tolerations:
- key: "key1"
  operator: "Equal"
  value: "v
status: {}(paste) --alue1"
  effect: "NoSchedule"
status: {}--a16,23All5,184,203,142,151,202,153,1432109 87654321        - key: "key1"9- key: "key1"8- key: "key1"7- key: "key1"6- key: "key1"5- key: "key1"4- key: "key1"34 operator: "Equal"4 operator: "Equal"5543 value: "value1"4 value: "value1"5643 effect: "NoSchedule"4 effect: "NoSchedule"5765436456^[  16,5Alldd  
~                                                                                                                                                                       16,1All~@k   5~@k   4~@k   3~@k   2o -- INSERT --13,11Top13,11All^[  13,0-1Allu1 line less; before #11  4 seconds ago
~                                                                                                                                                                       12,1Allp effect: "NoSchedule"13,5All- 2,3i -- INSERT --12,3All345effect: "NoSchedule"4effect: "NoSchedule"3-effect: "NoSchedule"4- effect: "NoSchedule"544 key: "key1"34 key: "key1"567891012345"4"3"2"1e"2n"3v"4_"5t"6y"7p"8e"956"8"7"6"5"4"3p"4r"5o"6d"7u"8c"9t"20i"1o"2n"3^[  16,22All::wq!"pod-redis.yaml" [New] 17L, 289C written
[root@master01 ~]# kubectl create -f pod-redis.yaml 
pod/dev-redis created
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS        AGE
admin-pod                          1/1     Running             28 (20m ago)    28h
dev-redis                          0/1     ContainerCreating   0               6s
ds-kusc00201-xgb9l                 1/1     Running             0               26h
ds-kusc00201-xkdtw                 1/1     Running             0               26h
hungry-bear                        0/1     CrashLoopBackOff    303 (11m ago)   25h
kucc8                              3/3     Running             0               25h
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               28h
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
admin-pod                          1/1     Running            28 (20m ago)    28h   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
dev-redis                          1/1     Running            0               10s   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               26h   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               26h   10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   303 (11m ago)   25h   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
kucc8                              3/3     Running            0               25h   10.44.0.3   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widecreate -f pod-redis.yaml vim pod-redis.yamlkubectl run dev-redis --image=redis:alpine --dry-run=client -o yamldescribe node worker01.airtel.internal.lab |grep -i taintrun dev-redis --image=redis:alpine --dry-run=client -o yamldescribe node worker01.airtel.internal.lab |grep -i taintmoretaint nodes worker01.airtel.internal.lab env_type=production:NoSchedule -
error: invalid taint spec: , name part must be non-empty; name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')
See 'kubectl taint -h' for help and examples
[root@master01 ~]# kubectl taint nodes worker01.airtel.internal.lab env_type=production:NoSchedule --
node/worker01.airtel.internal.lab untainted
[root@master01 ~]# kubectl taint nodes worker01.airtel.internal.lab env_type=production:NoSchedule- -get pods -o wide -o widetaint nodes worker01.airtel.internal.lab env_type=production:NoSchedule -get pods -o widecreate -f pod-redis.yaml vim pod-redis.yamlkubectl run dev-redis --image=redis:alpine --dry-run=client -o yamldescribe node worker01.airtel.internal.lab |grep -i taint
Taints:             <none>
[root@master01 ~]# kubectl drain nide worker01.airtel.internal.lab --ignore-daemonsets
Error from server (NotFound): nodes "nide" not found
[root@master01 ~]# kubectl drain nide worker01.airtel.internal.lab --ignore-daemonsetso
Error from server (NotFound): nodes "node" not found
[root@master01 ~]# kubectl drain node worker01.airtel.internal.lab --ignore-daemonsets --ignore-daemonsets
Error from server (NotFound): nodes "node" not found
[root@master01 ~]# kubectl drain node worker01.airtel.internal.lab --ignore-daemonsets2
Error from server (NotFound): nodes "node" not found
[root@master01 ~]# kubectl drain node worker02.airtel.internal.lab --ignore-daemonsets
node/worker02.airtel.internal.lab cordoned
error: unable to drain node "worker02.airtel.internal.lab" due to error:cannot delete Pods declare no controller (use --force to override): default/admin-pod, default/kucc8, continuing command...
There are pending nodes to be drained:
 worker02.airtel.internal.lab
cannot delete Pods declare no controller (use --force to override): default/admin-pod, default/kucc8
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab --ignore-daemonsets --force
node/worker02.airtel.internal.lab already cordoned
Warning: deleting Pods that declare no controller: default/admin-pod, default/kucc8; ignoring DaemonSet-managed Pods: default/ds-kusc00201-xkdtw, kube-system/kube-proxy-bk8v6, kube-system/weave-net-mdv4h
evicting pod default/kucc8
evicting pod default/admin-pod
pod/kucc8 evicted

pod/admin-pod evicted
node/worker02.airtel.internal.lab drained
[root@master01 ~]# 
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab --ignore-daemonsets --forcenode 1iescribe node worker01.airtel.internal.lab |grep -i tainttaint nodes worker01.airtel.internal.lab env_type=production:NoSchedule- -get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running            0               4m48s   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               26h     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               26h     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   304 (11m ago)   25h     10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h     10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running            0               5m11s   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               26h     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               26h     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   304 (11m ago)   25h     10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h     10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running            0               5m14s   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               26h     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               26h     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   304 (11m ago)   25h     10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h     10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl run non-root-pod --image=redis:alpine --dry-run=client -o wide
error: unable to match a printer suitable for the output format "wide", allowed formats are: go-template,go-template-file,json,jsonpath,jsonpath-as-json,jsonpath-file,name,template,templatefile,yaml
[root@master01 ~]# kubectl run non-root-pod --image=redis:alpine --dry-run=client -o wideyaml ? > non-root-pod.yaml
[root@master01 ~]# vim no
nodes.txt          non-root-pod.yaml  
[root@master01 ~]# vim non-root-pod.yaml 
"non-root-pod.yaml" 15L, 260C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  containers:
  - image: redis:alpine
    name: non-root-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   2~@k   1~@k   2dd  
~                                                                                                                                                                       12,3Alldd  
~                                                                                                                                                                       12,3Alldd  
~                                                                                                                                                                       12,1All~@k   1o -- INSERT --12,5Top12,5All43s4p543securityContext:
    runAsUser: 1000
    runAsGroup: 2000(paste) --fsGroup: 3000--a15,18All432765432109 8765securityContext:4securityContext:3344554^[  14,4All~@k   5dd  
~                                                                                                                                                                       14,5All^[  ^[  ::wq!"non-root-pod.yaml" 15L, 249C written
[root@master01 ~]# vim non-root-pod.yaml kubectl run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root-pod.yamlwideyaml > non-root-pod.yamlwideget pods -o widedrain worker02.airtel.internal.lab --ignore-daemonsets --force --
error: unknown flag: --force-
See 'kubectl drain --help' for usage.
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab --ignore-daemonsets --force- -
Error from server (NotFound): nodes "-" not found
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab --ignore-daemonsets --force --
error: unknown flag: --ignore-daemonsets-
See 'kubectl drain --help' for usage.
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab --ignore-daemonsets--
Error from server (NotFound): nodes "worker02.airtel.internal.lab-" not found
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab- -
Error from server (NotFound): nodes "-" not found
[root@master01 ~]# kubectl drain worker02.airtel.internal.lab -- --ignore-daemonsets- --force -kubectl uncordon
error: unknown flag: --ignore-daemonsets
See 'kubectl uncordon --help' for usage.
[root@master01 ~]# kubectl uncordon worker02.airtel.internal.lab --ignore-daemonsets --force 
error: unknown flag: --force
See 'kubectl uncordon --help' for usage.
[root@master01 ~]# kubectl uncordon worker02.airtel.internal.lab --force 
node/worker02.airtel.internal.lab uncordoned
[root@master01 ~]# kubectl uncordon worker02.airtel.internal.lab --force ignore-daemonsets --drain worker02.airtel.internal.lab -- --ignore-daemonsets- --force --vim non-root-pod.yaml kubectl run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root-pod.yamlwideget pods -o wide
NAME                               READY   STATUS             RESTARTS        AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running            0               10m   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0               26h   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0               26h   10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   305 (11m ago)   25h   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kuubectl create -f non-root-pod.yaml 
pod/non-root-pod created
[root@master01 ~]# kubectl create -f non-root-pod.yaml get pods -o wide
NAME                               READY   STATUS              RESTARTS        AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running             0               10m   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running             0               26h   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running             0               26h   10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     Error               306 (11m ago)   25h   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
non-root-pod                       0/1     ContainerCreating   0               4s    <none>      worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running             0               28h   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS        AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running   0               10m   10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running   0               26h   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running   0               26h   10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     Error     306 (11m ago)   25h   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
non-root-pod                       1/1     Running   0               9s    10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running   0               28h   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# vim policynetwork.yaml
"policynetwork.yaml" [New File]  ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       0,0-1Alli -- INSERT --0,1AllapiVersion: networking.k8s.io/v1kind: NetworkPolicy
metadata:  name: test-network-policy  namespace: default
spec:  podSelector:    matchL    matchLabels:
      role: db  policyTypes:    - Ingress    -- Egress
  ingress:    - from:        - ipBlock:            cidr: 172.17.0.0/16
            except:              - 172.17.1.0/24
        - namespaceSelector:            matchLmatchLabels:
              project: myproject        - podSelector:
            matchLabels:              role: frontend      pports:
        - protocol: TCP          port: 6379  egress:    - to:
        - ipBlock:            cidr: 10.0.0.0/24
(paste) --ports:- protocol: TCPport: 5978--a34,21Bot^[  34,20Bot~@k   3~@k   2,12~@k   1,20~@k   0,18~@k   29,9 ~@k   8~@k   7,20~@k   6~@k   5,12~@k   4,20~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7,19~@k   6,20~@k   5,18~@k   4,11~@k   3,10~@k   2,12~@k   1,13~@k   0,14~@k   9,14 ~@k   metadata:8,1666%~@k   kind: NetworkPolicy7,1433%~@k   apiVersion: networking.k8s.io/v16,5Top~@k   5,20~@k   4~@k   3,9 ~@k   2,19~@k   1,20~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   2,19~@k   3,9 ~@k   4,20~@k   5~@k   6,5 ~@k   7,14~@k   8,16~@k   9,14~@k   10,14~@k   9,14 ~@k   8,16~@k   7,14dd  
ports:7,5Topdd  
- protocol: TCP7,7Topdd  
port: 59787,3All~@k   6~@k   5~@k   4~@k   5dd  
~                                                                                                                                                                       5,1All~@k   4~@K   27i -- INSERT --4,27All876543210198765432109 d10e1f2a3u4l5t6-7d8e9n20y15,6 6,157,148,139,1110,12^[  10,11Alldd  
~                                                                                                                                                                       10,9Alldd  
~                                                                                                                                                                       10,13Alldd  
~                                                                                                                                                                       10,13Alldd  
~                                                                                                                                                                       10,15Alldd  
~                                                                                                                                                                       10,9All
~                                                                                                                                                                       10,13All
~                                                                                                                                                                       10,15All
~                                                                                                                                                                       10,9All
~                                                                                                                                                                       10,13Alldd  
~                                                                                                                                                                       10,15Alldd  
~                                                                                                                                                                       10,7Alldd  
~                                                                                                                                                                       10,9Alldd  
~                                                                                                                                                                       10,11Alldd  
~                                                                                                                                                                       10,3Alldd  
~                                                                                                                                                                       10,5Alld~@k    ~                                                                                                                                                                       ~                                                                                                                                                                       10,13Alldd  
~                                                                                                                                                                       10,7Alldd  
~                                                                                                                                                                       10,9Alldd  
~                                                                                                                                                                       10,11Alldd  
~                                                                                                                                                                       9,3  ::wq!"policynetwork.yaml" [New] 9L, 143C written
[root@master01 ~]# kubectl create -f plolicynetwork.yaml 
networkpolicy.networking.k8s.io/default-deny created
[root@master01 ~]# kubectl getget pods -o wide
NAME                               READY   STATUS             RESTARTS          AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
dev-redis                          1/1     Running            0                 13m     10.36.0.4   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xgb9l                 1/1     Running            0                 26h     10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
ds-kusc00201-xkdtw                 1/1     Running            0                 26h     10.44.0.2   worker02.airtel.internal.lab   <none>           <none>
hungry-bear                        0/1     CrashLoopBackOff   306 (9m18s ago)   25h     10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
non-root-pod                       1/1     Running            0                 2m45s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
web-project-268-84b655ccd9-jpwnz   1/1     Running            0                 28h     10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widens
NAME              STATUS   AGE
default           Active   2d20h
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
[root@master01 ~]# kubectl get ns -o wideetworkpolicy
NAME           POD-SELECTOR   AGE
default-deny   <none>         28s
[root@master01 ~]# kubectl get ns --all-namespaces
NAME              STATUS   AGE
default           Active   2d20h
kube-node-lease   Active   2d20h
kube-public       Active   2d20h
kube-system       Active   2d20h
[root@master01 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                                   READY   STATUS             RESTARTS        AGE
default       dev-redis                                              1/1     Running            0               29m
default       ds-kusc00201-xgb9l                                     1/1     Running            0               26h
default       ds-kusc00201-xkdtw                                     1/1     Running            0               26h
default       hungry-bear                                            0/1     CrashLoopBackOff   309 (10m ago)   26h
default       non-root-pod                                           1/1     Running            0               19m
default       web-project-268-84b655ccd9-jpwnz                       1/1     Running            0               28h
kube-system   coredns-565d847f94-48j9p                               1/1     Running            1 (2d20h ago)   2d20h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running            1 (2d20h ago)   2d20h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running            1 (2d20h ago)   2d20h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running            1 (2d20h ago)   2d20h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running            1 (2d20h ago)   2d20h
kube-system   kube-proxy-bk8v6                                       1/1     Running            0               2d20h
kube-system   kube-proxy-rfb9j                                       1/1     Running            1 (2d20h ago)   2d20h
kube-system   kube-proxy-vpx59                                       1/1     Running            0               2d20h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running            1 (2d20h ago)   2d20h
kube-system   weave-net-42gjg                                        2/2     Running            3 (2d20h ago)   2d20h
kube-system   weave-net-44k9t                                        2/2     Running            1 (2d20h ago)   2d20h
kube-system   weave-net-mdv4h                                        2/2     Running            0               2d20h
[root@master01 ~]# kubectl get pods --all-namespaces -n default
NAME                               READY   STATUS             RESTARTS        AGE
dev-redis                          1/1     Running            0               29m
ds-kusc00201-xgb9l                 1/1     Running            0               26h
ds-kusc00201-xkdtw                 1/1     Running            0               26h
hungry-bear                        0/1     CrashLoopBackOff   309 (10m ago)   26h
non-root-pod                       1/1     Running            0               19m
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h
[root@master01 ~]# kubectl get svc -- -n default
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d20h
[root@master01 ~]# kubectl -n default get svc kubec-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   2d20h
[root@master01 ~]# kubectl get pods -o=jsonpath="{}.}i}t}e}m}{}}}[}]}*]}[}]}']}m]}e]}t]}a]}t]}]}d]}a]}t]}a]}.]}n]}a]}m]}e]},]}']}]}]}']},]}']}m]}e]}t]}a]}d]}a]}t]}a]}.]}n]}a]}m]}e]}s]}p]}a]}c]}e]}s]}]}']}"
[root@master01 ~]# kubectl get pods -o=jsonpath="{.items[*]['metadata.name', 'metadata.namespace']}"
dev-redis ds-kusc00201-xgb9l ds-kusc00201-xkdtw hungry-bear non-root-pod web-project-268-84b655ccd9-jpwnz default default default default default default[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# kubectl run nginx-web --image=nginx 
pod/nginx-web created
[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS              RESTARTS          AGE
dev-redis                          1/1     Running             0                 34m
ds-kusc00201-xgb9l                 1/1     Running             0                 26h
ds-kusc00201-xkdtw                 1/1     Running             0                 26h
hungry-bear                        0/1     CrashLoopBackOff    310 (9m32s ago)   26h
nginx-web                          0/1     ContainerCreating   0                 5s
non-root-pod                       1/1     Running             0                 23m
web-project-268-84b655ccd9-jpwnz   1/1     Running             0                 28h
[root@master01 ~]# kubectl get pods -n default
NAME                               READY   STATUS             RESTARTS          AGE
dev-redis                          1/1     Running            0                 34m
ds-kusc00201-xgb9l                 1/1     Running            0                 26h
ds-kusc00201-xkdtw                 1/1     Running            0                 26h
hungry-bear                        0/1     CrashLoopBackOff   310 (9m39s ago)   26h
nginx-web                          1/1     Running            0                 12s
non-root-pod                       1/1     Running            0                 23m
web-project-268-84b655ccd9-jpwnz   1/1     Running            0                 28h
[root@master01 ~]# [root@master01 ~]# kubectl -n default get podsdelete web-nginx
error: the server doesn't have a resource type "web-nginx"
[root@master01 ~]# kubectl delete web-nginx web-nginxp web-nginxo web-nginxd web-nginx
Error from server (NotFound): pods "web-nginx" not found
[root@master01 ~]# kubectl delete pod web-nginxweb-nginx-n default get pods
NAME                               READY   STATUS             RESTARTS        AGE
dev-redis                          1/1     Running            0               34m
ds-kusc00201-xgb9l                 1/1     Running            0               26h
ds-kusc00201-xkdtw                 1/1     Running            0               26h
hungry-bear                        0/1     CrashLoopBackOff   310 (10m ago)   26h
nginx-web                          1/1     Running            0               61s
non-root-pod                       1/1     Running            0               24m
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h
[root@master01 ~]# kubectl get podsdelete pod web-nginxnginx-web
pod "nginx-web" deleted
[root@master01 ~]# kubectl delete pod nginx-webget podsdelete pod web-nginxweb-nginx-n default get podsrun nginx-web --image=nginx --restart=Never//List the pods
error: invalid restart policy: Never//List
See 'kubectl run -h' for help and examples
[root@master01 ~]# kubectl run nginx-web --image=nginx --restart=Never//List the pod
pod/nginx-web created
[root@master01 ~]# kubectl run nginx-web --image=nginx --restart=Never//List the poddelete pod nginx-webget pods
NAME                               READY   STATUS             RESTARTS        AGE
dev-redis                          1/1     Running            0               35m
ds-kusc00201-xgb9l                 1/1     Running            0               26h
ds-kusc00201-xkdtw                 1/1     Running            0               26h
hungry-bear                        0/1     CrashLoopBackOff   310 (11m ago)   26h
nginx-web                          1/1     Running            0               3s
non-root-pod                       1/1     Running            0               25m
web-project-268-84b655ccd9-jpwnz   1/1     Running            0               28h
[root@master01 ~]# kubectl get podsrun nginx-web --image=nginx --restart=Never > nginx-same.yaml -o yaml--dry-run=client 
[root@master01 ~]# vim nginx-same.yaml 
"nginx-same.yaml" 15L, 243C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-web
  name: nginx-web
spec:
  containers:
  - image: nginx
    name: nginx-web
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   3::q![root@master01 ~]# kubectl createvim nginx-same.yaml 
"nginx-same.yaml" 15L, 243C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-web
  name: nginx-web
spec:
  containers:
  - image: nginx
    name: nginx-web
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       ~                                                                                                                                                                       13,1All~@k   2~@k   1~@k   0~@k   9,1 ~@k   8~@k   7~@k   6~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --6,18All9-20a1m210s1a2m3e4-5p6o7d87,1-9s20a1m2e3-4p5o6d7^[  7,26All~@k   8,5 ~@k   9,13~@k   10,16~@k   1,19i -- INSERT --11,19All20-1s2a3m4e5-6p7o8d9^[  11,28All::wq!"nginx-same.yaml" 15L, 270C written
[root@master01 ~]# kubectl create -f nginx-same.yaml 
pod/nginx-web-same-pod created
[root@master01 ~]# kubectl get pdisods
NAME                               READY   STATUS             RESTARTS         AGE
dev-redis                          1/1     Running            0                37m
ds-kusc00201-xgb9l                 1/1     Running            0                26h
ds-kusc00201-xkdtw                 1/1     Running            0                26h
hungry-bear                        0/1     CrashLoopBackOff   311 (8m1s ago)   26h
nginx-web                          1/1     Running            0                2m
nginx-web-same-pod                 1/1     Running            0                5s
non-root-pod                       1/1     Running            0                27m
web-project-268-84b655ccd9-jpwnz   1/1     Running            0                29h
[root@master01 ~]# logout
=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.15 10:40:43 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Server refused our key
root@172.16.102.22's password: 
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Wed Sep 14 16:20:36 2022 from 192.168.200.34
[root@master01 ~]# vim .ssh/authorized_keys
".ssh/authorized_keys" 1L, 587C  ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDTcU2x1f5Wx3JM5UnqYe3hd2cplj+nppmAzcU2X2nLga9e2L3Okwka9yTiY+Od5uI7Ei50PaZ9B+cOYLYzmXXYoDnCtdd6zSdK/xAtC1ALSqkxdsCK4xwfILspvZLePA+MWDDnU7yj3BAk/MNsxH04qrXGBLD0PiqGxIzFzupcXCez/U2YrW5+VtSaNgCXhTNex94qLvUjJl73wiW/6ZtzuC4M+NGwoPre2FzIgAIWI4iTlJ7QpaMhWAtKxJlIMebLiaXm5Xhp+73Ou3luPsawXEydO8p0643E2oQDzON4uYIWww0DAvy3Jf0k2B01TAUQjL7HxqIcUTJvXjf7+hvgMHGFGK7d14qggnAHmtSoTh8NHq74jQ8gKvKToPeY7xDv8HChnn7nDgEzKfH4niD6l1EQYsa2FKBbA3EZggFTMlXDfEposnY1VyB/K8gNp49mwGLYNZHUFwbI8UWCu5pu5Cxll//rRRuDl5VjeS3+qHW9MNlIUhajFOKxxjJyTRET0s= root@master01.airtel.internal.lab
~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         1,1All~@k   o -- INSERT --2,1Allssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCEFYBDQ7t9M0MDvLhX/Eq3Xq+Yedk48tPeqs2m/r42niAVqSK2LBB3Au39pyjBiUZcwpg2kLX4RY+UToDjbTahVgwFGoAtClJILZbTcWJOxv6J9vx/rzEmTzLYCxbsKdqGDzzUIaFhb7YbovUmmZzmfjynubiSZP+HJcu1fE98OkZGBeGvIPy37TA+3Fua9/CIwuWOfUwXQTS8/+9DZLO5H+xSNNTopwELtuNlrMmpxRBrEOUtSKmy5bFZK0GwlRBR1vjr9mbgBjP+RT3m8rKjtxOdqp5c2EDvts9+bj7x+m4sSSqakUNzub2g5eB435hvMGTvuOz81+X9H9o0tBhLC1 rsa-key-20220112398^[  2,397All::wq!".ssh/authorized_keys" 2L, 985C written
[root@master01 ~]# [root@master01 ~]# kubectl get npods
NAME                               READY   STATUS             RESTARTS          AGE
dev-redis                          1/1     Running            0                 17h
ds-kusc00201-xgb9l                 1/1     Running            0                 43h
ds-kusc00201-xkdtw                 1/1     Running            0                 43h
hungry-bear                        0/1     CrashLoopBackOff   511 (9m20s ago)   43h
nginx-web                          1/1     Running            0                 17h
nginx-web-same-pod                 1/1     Running            0                 17h
non-root-pod                       1/1     Running            0                 17h
web-project-268-84b655ccd9-jpwnz   1/1     Running            0                 46h
[root@master01 ~]# kubectl get podsvim .ssh/authorized_keyskubectl get podscreate -f nginx-same.yaml get podsvim .ssh/authorized_keyskubectl get podskybecubegebe
kubeadm  kubectl  kubelet  
[root@master01 ~]# kubectl get pods nginx-web-same-pod > abc.yaml-.yamlnginx-web-same-pod.yaml
[root@master01 ~]# vim abc-nginx-web-same-pod.yaml 
"abc-nginx-web-same-pod.yaml" 2L, 108C  NAMEREADY   STATUS    RESTARTS   AGE
nginx-web-same-pod   1/1     Running   017h
~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         1,1All::QE492: Not an editor command: Q1,1All::q[root@master01 ~]# vim abc-nginx-web-same-pod.yaml kubectl get pods nginx-web-same-pod > abc-nginx-web-same-pod.yaml-o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-09-14T11:54:48Z"
  labels:
    run: nginx-web-same-pod
  name: nginx-web-same-pod
  namespace: default
  resourceVersion: "349343"
  uid: fe7b6cf9-b696-486a-9fe0-8326d46e71c9
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx-web-same-pod
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-jgrzl
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: worker02.airtel.internal.lab
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-jgrzl
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-09-14T11:46:46Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-09-14T11:46:49Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-09-14T11:46:49Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-09-14T11:54:48Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://e2eed2e58faf667ea28fac2789b8f392539c8ac376426a71c287b4581479cc1a
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    lastState: {}
    name: nginx-web-same-pod
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-09-14T11:46:49Z"
  hostIP: 172.16.102.24
  phase: Running
  podIP: 10.44.0.4
  podIPs:
  - ip: 10.44.0.4
  qosClass: BestEffort
  startTime: "2022-09-14T11:46:46Z"
[root@master01 ~]# rm -rf abc-nginx-web-same-pod.yaml 
[root@master01 ~]# rm -rf abc-nginx-web-same-pod.yaml kubectl get pods nginx-web-same-pod -o yaml --export
error: unknown flag: --export
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods nginx-web-same-pod -o yaml --export--help
Display one or many resources.

 Prints a table of the most important information about the specified resources. You can filter the list using a label
selector and the --selector flag. If the desired resource type is namespaced you will only see results in your current
namespace unless you pass --all-namespaces.

 By specifying the output as 'template' and providing a Go template as the value of the --template flag, you can filter
the attributes of the fetched resources.

Use "kubectl api-resources" for a complete list of supported resources.

Examples:
  # List all pods in ps output format
  kubectl get pods
  
  # List all pods in ps output format with more information (such as node name)
  kubectl get pods -o wide
  
  # List a single replication controller with specified NAME in ps output format
  kubectl get replicationcontroller web
  
  # List deployments in JSON output format, in the "v1" version of the "apps" API group
  kubectl get deployments.v1.apps -o json
  
  # List a single pod in JSON output format
  kubectl get -o json pod web-pod-13je7
  
  # List a pod identified by type and name specified in "pod.yaml" in JSON output format
  kubectl get -f pod.yaml -o json
  
  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  kubectl get -k dir/
  
  # Return only the phase value of the specified pod
  kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
  
  # List resource information in custom columns
  kubectl get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image
  
  # List all replication controllers and services together in ps output format
  kubectl get rc,services
  
  # List one or more resources by their type and names
  kubectl get rc/web service/frontend pods/web-pod-13je7
  
  # List status subresource for a single pod.
  kubectl get pod web-pod-13je7 --subresource status

Options:
    -A, --all-namespaces=false:
If present, list the requested object(s) across all namespaces. Namespace in current context is ignored even
if specified with --namespace.

    --allow-missing-template-keys=true:
If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to
golang and jsonpath output formats.

    --chunk-size=500:
Return large lists in chunks rather than all at once. Pass 0 to disable. This flag is beta and may change in
the future.

    --field-selector='':
Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector
key1=value1,key2=value2). The server only supports a limited number of field queries per type.

    -f, --filename=[]:
Filename, directory, or URL to files identifying the resource to get from a server.

    --ignore-not-found=false:
If the requested object does not exist the command will return exit code 0.

    -k, --kustomize='':
Process the kustomization directory. This flag can't be used together with -f or -R.

    -L, --label-columns=[]:
Accepts a comma separated list of labels that are going to be presented as columns. Names are case-sensitive.
You can also use multiple flag options like -L label1 -L label2...

    --no-headers=false:
When using the default or custom-column output format, don't print headers (default print headers).

    -o, --output='':
Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath,
jsonpath-as-json, jsonpath-file, custom-columns, custom-columns-file, wide). See custom columns
[https://kubernetes.io/docs/reference/kubectl/#custom-columns], golang template
[http://golang.org/pkg/text/template/#pkg-overview] and jsonpath template
[https://kubernetes.io/docs/reference/kubectl/jsonpath/].

    --output-watch-events=false:
Output watch event objects when --watch or --watch-only is used. Existing objects are output as initial ADDED
events.

    --raw='':
Raw URI to request from the server.  Uses the transport specified by the kubeconfig file.

    -R, --recursive=false:
Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests
organized within the same directory.

    -l, --selector='':
Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
objects must satisfy all of the specified label constraints.

    --server-print=true:
If true, have the server return the appropriate table output. Supports extension APIs and CRDs.

    --show-kind=false:
If present, list the resource type for the requested object(s).

    --show-labels=false:
When printing, show all labels as the last column (default hide labels column)

    --show-managed-fields=false:
If true, keep the managedFields when printing objects in JSON or YAML format.

    --sort-by='':
If non-empty, sort list types using this field specification.  The field specification is expressed as a
JSONPath expression (e.g. '{.metadata.name}'). The field in the API resource specified by this JSONPath
expression must be an integer or a string.

    --subresource='':
If specified, gets the subresource of the requested object. Must be one of [status scale]. This flag is alpha
and may change in the future.

    --template='':
Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format
is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

    -w, --watch=false:
After listing/getting the requested object, watch for changes.

    --watch-only=false:
Watch for changes to the requested object(s), without listing/getting first.

Usage:
  kubectl get
[(-o|--output=)json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file|custom-columns|custom-columns-file|wide]
(TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl get pods nginx-web-same-pod -o yaml --helpnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podnginx-web-same-podkubectl get po nginx -o yaml --exportnginx-web-same-pod nginx-web-same-pod
error: unknown flag: --export
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get po nginx -o yaml --export--export--export--export--export--export--export--exportexport -o yaml
error: unknown flag: --export
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get po nginx --export -o yamlkubectl get --export
error: unknown flag: --export
See 'kubectl get --help' for usage.
[root@master01 ~]# [root@master01 ~]# kubectl get all --export=true -o yaml
error: unknown flag: --export
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get all --export=true -o yaml--exportpo nginx --export -o yamlo yaml --exportdescribe
Name:             nginx-web
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Wed, 14 Sep 2022 17:14:51 +0530
Labels:           run=nginx-web
Annotations:      <none>
Status:           Running
IP:               10.44.0.3
IPs:
  IP:  10.44.0.3
Containers:
  nginx-web:
    Container ID:   containerd://5f2a4cd97704a62348bdee22957a8758764fc545567ded9cb4acb6d17b0a6ded
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 14 Sep 2022 17:14:54 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ltzh9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-ltzh9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>

Name:             nginx-web-same-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Wed, 14 Sep 2022 17:16:46 +0530
Labels:           run=nginx-web-same-pod
Annotations:      <none>
Status:           Running
IP:               10.44.0.4
IPs:
  IP:  10.44.0.4
Containers:
  nginx-web-same-pod:
    Container ID:   containerd://e2eed2e58faf667ea28fac2789b8f392539c8ac376426a71c287b4581479cc1a
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 14 Sep 2022 17:16:49 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jgrzl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-jgrzl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>

[root@master01 ~]# kubectl get pods
NAME                               READY   STATUS             RESTARTS          AGE
dev-redis                          1/1     Running            0                 19h
ds-kusc00201-xgb9l                 1/1     Running            0                 45h
ds-kusc00201-xkdtw                 1/1     Running            0                 45h
hungry-bear                        0/1     CrashLoopBackOff   528 (8m55s ago)   44h
nginx-web                          1/1     Running            0                 18h
nginx-web-same-pod                 1/1     Running            0                 18h
non-root-pod                       1/1     Running            0                 19h
web-project-268-84b655ccd9-jpwnz   1/1     Running            0                 47h
[root@master01 ~]# kubectl delete pods all --all
error: name cannot be provided when a selector is specified
[root@master01 ~]# kubectl delete pods all --all
Error from server (NotFound): pods "all" not found
[root@master01 ~]# kubectl delete pods all-A
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl delete pods -Aall -A
error: a resource cannot be retrieved by name across all namespaces
[root@master01 ~]# kubectl delete pods all -A -A -A -A
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl delete pods  -Aall -A --all-namespaces
error: a resource cannot be retrieved by name across all namespaces
[root@master01 ~]# kubectl delete pods all -A --all-namespaces --all-namespaces --all-namespaces--all-namespaces --all-namespaces --all-namespaces --all-namespaces
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl delete pods  --all-namespacesall -A
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl delete all -A-Aall
pod "dev-redis" deleted
pod "ds-kusc00201-xgb9l" deleted
pod "ds-kusc00201-xkdtw" deleted
pod "hungry-bear" deleted
pod "nginx-web" deleted
pod "nginx-web-same-pod" deleted
pod "non-root-pod" deleted
pod "web-project-268-84b655ccd9-jpwnz" deleted
service "kubernetes" deleted
daemonset.apps "ds-kusc00201" deleted
deployment.apps "web-project-268" deleted
[root@master01 ~]# kubectl run nginnginx1.17.4 --image=nginx:1.17.4 --o yaml nginx1.17.4.yaml
The Pod "nginx1.17.4" is invalid: spec.containers[0].name: Invalid value: "nginx1.17.4": a lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?')
[root@master01 ~]# kubectl run nginx1.17.4 --image=nginx:1.17.4 -o yaml nginx1.17.4.yaml174 nginx1.17.4.yaml> nginx1.17.4.yaml
[root@master01 ~]# vim nginx1.17.4.yaml 
"nginx1.17.4.yaml" 63L, 1577C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-09-15T06:36:28Z"
  labels:
    run: nginx1174
  name: nginx1174
  namespace: default
  resourceVersion: "444483"
  uid: 8d39be35-3e97-404e-aad8-c032d7bac1bd
spec:
  containers:
  - image: nginx:1.17.4
    imagePullPolicy: IfNotPresent
    name: nginx1174
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccountname: kube-api-access-fmxjcreadOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default1,1Top~@k   2~@k   3~@k   4~@k   2~@k   3~@k   4~@k   5~@k   5~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,5~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   
terminationGracePeriodSeconds: 3027,53%~@k   
tolerations:28,56%~@k   
- effect: NoExecute29,59%~@k   
key: node.kubernetes.io/not-ready30,512%~@k   
operator: Exists31,515%~@k   
tolerationSeconds: 30032,518%~@k   
- effect: NoExecute33,521%~@k   
key: node.kubernetes.io/unreachable34,525%~@k   
operator: Exists35,528%~@k   
tolerationSeconds: 30036,531%~@k   
volumes:37,534%~@k   
- name: kube-api-access-fmxjc38,537%~@k   
projected:39,540%~@k   
defaultMode: 42040,543%~@k   
sources:41,546%~@k   
- serviceAccountToken:42,550%~@k   
expirationSeconds: 360743,553%~@k   
path: token44,556%~@k   
- configMap:45,559%~@k   
items:46,562%~@k   
- key: ca.crt47,565%~@k   
path: ca.crt48,568%~@k   
name: kube-root-ca.crt49,571%~@k   
- downwardAPI:50,575%~@k   
items:51,578%~@k   
- fieldRef:52,581%~@k   
apiVersion: v153,584%~@k   
fieldPath: metadata.namespace54,587%~@k   
path: namespace55,590%~@k   
status:56,593%~@k   
phase: Pending57,596%~@k   
qosClass: BestEffort58,5Bot~@k   9~@k   60~@k   1~@k   2~@k   3~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   2~@k   1~@k   0~@k   59~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   49~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   39~@k   8~@k   terminationGracePeriodSeconds: 3037,596%~@k   serviceAccountName: default36,593%~@k   serviceAccount: default35,590%~@k   securityContext: {}34,587%~@k   schedulerName: default-scheduler33,584%~@k   restartPolicy: Always32,581%~@k   priority: 031,578%~@k   preemptionPolicy: PreemptLowerPriority30,575%~@k   enableServiceLinks: true29,571%~@k   dnsPolicy: ClusterFirst28,568%~@k   readOnly: true27,565%~@k   name: kube-api-access-fmxjc26,562%~@k   - mountPath: /var/run/secrets/kubernetes.io/serviceaccount25,559%~@k   volumeMounts:24,556%~@k   terminationMessagePolicy: File23,553%~@k   terminationMessagePath: /dev/termination-log22,550%::q[root@master01 ~]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx1174   1/1     Running   0          37s
[root@master01 ~]# kubectl delete pods nginx1174
pod "nginx1174" deleted
[root@master01 ~]# kubectl delete pods nginx1174get podsvim nginx1.17.4.yaml kubectl run nginx1174 --image=nginx:1.17.4 -o yaml > nginx1.17.4.yaml
[root@master01 ~]# kubectl run nginx1174 --image=nginx:1.17.4 -o yaml > nginx1.17.4.yamldelete pods nginx1174get pods
NAME        READY   STATUS              RESTARTS   AGE
nginx1174   0/1     ContainerCreating   0          3s
[root@master01 ~]# kubectl get podsrun nginx1174 --image=nginx:1.17.4 -o yaml > nginx1.17.4.yamlget podskubectl get podsrun nginx1174 --image=nginx:1.17.4 -o yaml > nginx1.17.4.yamldelete pods nginx1174
pod "nginx1174" deleted
[root@master01 ~]# kubectl delete pods nginx1174get podsrun nginx1174 --image=nginx:1.17.4 -o yaml > nginx1.17.4.yaml --dry-run=client
[root@master01 ~]# kubectl run nginx1174 --image=nginx:1.17.4 --dry-run=client -o yaml > nginx1.17.4.yamldelete pods nginx1174get podsrun nginx1174 --image=nginx:1.17.4 -o yaml > nginx1.17.4.yamldelete pods nginx1174get podsvim nginx1.17.4.yaml 
"nginx1.17.4.yaml" 15L, 251C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx1174
  name: nginx1174
spec:
  containers:
  - image: nginx:1.17.4
    name: nginx1174
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   2~@k   3~@k   4~@k   4~@k   3o -- INSERT --15,3Top15,3Allp4o5r6t7s8"98  ports:91016,3All-4- 5c6o7n8t9a10n1i2n3e4r5P6o7r8t9contaninerPort:2018203^[  16,22All~@k   5,9 ~@k   4,22~@k   3::wq!"nginx1.17.4.yaml" 17L, 284C written
[root@master01 ~]# kubectl create -f nginx1.17.4.yaml 
Error from server (BadRequest): error when creating "nginx1.17.4.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.ports"
[root@master01 ~]# kubectl create -f nginx1.17.4.yaml vim
"nginx1.17.4.yaml" 17L, 284C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx1174
  name: nginx1174
spec:
  containers:
  - image: nginx:1.17.4
    name: nginx1174
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  ports:
  - contaninerPort: 80
status: {}
~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         13,22All~@k   4~@k   5,9 ~@k   6,22~@k   5,9 ~@k   4,22~@k   5,9 i -- INSERT --15,9All876543 ports:4 ports:564323 - contaninerPort: 804 - contaninerPort: 805^[  16,4All~@k   5~@k   4~@k   3dd  
~                                                                                                                                                                         13,3Alldd  
~                                                                                                                                                                         13,5All::wq! 13,5All~@k   2dd  
~                                                                                                                                                                         12,5All::wq!"nginx1.17.4.yaml" 14L, 220C written
[root@master01 ~]# vim nginx1.17.4.yaml kubectl create -f
Error from server (BadRequest): error when creating "nginx1.17.4.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.containers[0].ports[0].contaninerPort"
[root@master01 ~]# kubectl create -f nginx1.17.4.yaml vim
"nginx1.17.4.yaml" 14L, 220C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx1174
  name: nginx1174
spec:
  containers:
  - image: nginx:1.17.4
    name: nginx1174
    ports:
    - contaninerPort: 80
status: {}
~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         ~                                                                                                                                                                         12,5All~@k   3~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2i -- INSERT --13,22AllcontaninerPort 801 800 8019 808 807 806 805 804 803 802 801 800 809  808 807806 805- containerPort: 8021^[  13,20All::wq!"nginx1.17.4.yaml" 14L, 219C written
[root@master01 ~]# vim nginx1.17.4.yaml kubectl create -f
pod/nginx1174 created
[root@master01 ~]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx1174   1/1     Running   0          4s
[root@master01 ~]# kubectl get podssv
error: the server doesn't have a resource type "sv"
[root@master01 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6m21s
[root@master01 ~]# kubectl get svc decribe nginx1174
error: unknown command "decribe" for "kubectl"

Did you mean this?
describe
[root@master01 ~]# kubectl decribe nginx1174 nginx1174 nginx1174 nginx1174 nginx1174 nginx1174s nginx1174c nginx1174r nginx1174i nginx1174b nginx1174e nginx1174 nginx1174p nginx1174o nginx1174d nginx1174 nginx1174 nginx1174 nginx1174nginx1174
error: the server doesn't have a resource type "nginx1174"
[root@master01 ~]# kubectl describe nginx1174pnginx1174onginx1174dnginx1174 nginx1174
Name:             nginx1174
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Thu, 15 Sep 2022 12:02:46 +0530
Labels:           run=nginx1174
Annotations:      <none>
Status:           Running
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
Containers:
  nginx1174:
    Container ID:   containerd://1f0eaa33d9f96b12f1a8c96a0cafbdf5c74ac4194ec2e98e554e2ead44053152
    Image:          nginx:1.17.4
    Image ID:       docker.io/library/nginx@sha256:77ebc94e0cec30b20f9056bac1066b09fbdc049401b71850922c63fc0cc1762e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 12:02:47 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zx98 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-8zx98:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  37s    default-scheduler  Successfully assigned default/nginx1174 to worker02.airtel.internal.lab
  Normal  Pulled     8m40s  kubelet            Container image "nginx:1.17.4" already present on machine
  Normal  Created    8m40s  kubelet            Created container nginx1174
  Normal  Started    8m40s  kubelet            Started container nginx1174
[root@master01 ~]# urlcut url 10.44.0.1



^C
[root@master01 ~]# curl 10.44.0.1kubectl describe pod nginx1174nginx1174get svcpods -o wide
NAME        READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
nginx1174   1/1     Running   0          2m38s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widecurl 10.44.0.1h10.44.0.1t10.44.0.1t10.44.0.1p10.44.0.1:10.44.0.1/10.44.0.1/10.44.0.1
^C
[root@master01 ~]# iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-NODEPORTS  all  --  anywhere             anywhere             /* kubernetes health check service ports */
KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
KUBE-FIREWALL  all  --  anywhere             anywhere            
DROP       tcp  --  anywhere             localhost            tcp dpt:6784 ADDRTYPE match src-type !LOCAL ! ctstate RELATED,ESTABLISHED /* Block non-local access to Weave Net control port */
WEAVE-NPC-EGRESS  all  --  anywhere             anywhere            

Chain FORWARD (policy DROP)
target     prot opt source               destination         
WEAVE-NPC-EGRESS  all  --  anywhere             anywhere             /* NOTE: this must go before '-j KUBE-FORWARD' */
WEAVE-NPC  all  --  anywhere             anywhere             /* NOTE: this must go before '-j KUBE-FORWARD' */
NFLOG      all  --  anywhere             anywhere             state NEW nflog-group 86
DROP       all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
DOCKER-USER  all  --  anywhere             anywhere            
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
KUBE-FIREWALL  all  --  anywhere             anywhere            

Chain KUBE-FIREWALL (2 references)
target     prot opt source               destination         
DROP       all  -- !127.0.0.0/8          127.0.0.0/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT
DROP       all  --  anywhere             anywhere             /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000

Chain DOCKER (1 references)
target     prot opt source               destination         

Chain DOCKER-ISOLATION-STAGE-1 (1 references)
target     prot opt source               destination         
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere            
RETURN     all  --  anywhere             anywhere            

Chain DOCKER-ISOLATION-STAGE-2 (1 references)
target     prot opt source               destination         
DROP       all  --  anywhere             anywhere            
RETURN     all  --  anywhere             anywhere            

Chain KUBE-KUBELET-CANARY (0 references)
target     prot opt source               destination         

Chain DOCKER-USER (1 references)
target     prot opt source               destination         
RETURN     all  --  anywhere             anywhere            

Chain KUBE-PROXY-CANARY (0 references)
target     prot opt source               destination         

Chain KUBE-EXTERNAL-SERVICES (2 references)
target     prot opt source               destination         

Chain KUBE-NODEPORTS (1 references)
target     prot opt source               destination         

Chain KUBE-SERVICES (2 references)
target     prot opt source               destination         

Chain KUBE-FORWARD (1 references)
target     prot opt source               destination         
DROP       all  --  anywhere             anywhere             ctstate INVALID
ACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding rules */ mark match 0x4000/0x4000
ACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED

Chain KUBE-PROXY-FIREWALL (3 references)
target     prot opt source               destination         

Chain WEAVE-NPC-INGRESS (1 references)
target     prot opt source               destination         

Chain WEAVE-NPC-DEFAULT (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             match-set weave-;rGqyMIl1HN^cfDki~Z$3]6!N dst /* DefaultAllow ingress isolation for namespace: default */
ACCEPT     all  --  anywhere             anywhere             match-set weave-]B*(W?)t*z5O17G044[gUo#$l dst /* DefaultAllow ingress isolation for namespace: kube-node-lease */
ACCEPT     all  --  anywhere             anywhere             match-set weave-Rzff}h:=]JaaJl/G;(XJpGjZ[ dst /* DefaultAllow ingress isolation for namespace: kube-public */
ACCEPT     all  --  anywhere             anywhere             match-set weave-P.B|!ZhkAr5q=XZ?3}tMBA+0 dst /* DefaultAllow ingress isolation for namespace: kube-system */

Chain WEAVE-NPC (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED
ACCEPT     all  --  anywhere             base-address.mcast.net/4 
ACCEPT     all  --  anywhere             anywhere             PHYSDEV match --physdev-out vethwe-bridge --physdev-is-bridged
WEAVE-NPC-DEFAULT  all  --  anywhere             anywhere             state NEW
WEAVE-NPC-INGRESS  all  --  anywhere             anywhere             state NEW

Chain WEAVE-NPC-EGRESS-ACCEPT (4 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             MARK or 0x40000

Chain WEAVE-NPC-EGRESS-CUSTOM (1 references)
target     prot opt source               destination         

Chain WEAVE-NPC-EGRESS-DEFAULT (1 references)
target     prot opt source               destination         
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src /* DefaultAllow egress isolation for namespace: default */
RETURN     all  --  anywhere             anywhere             match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src /* DefaultAllow egress isolation for namespace: default */
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src /* DefaultAllow egress isolation for namespace: kube-node-lease */
RETURN     all  --  anywhere             anywhere             match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src /* DefaultAllow egress isolation for namespace: kube-node-lease */
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src /* DefaultAllow egress isolation for namespace: kube-public */
RETURN     all  --  anywhere             anywhere             match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src /* DefaultAllow egress isolation for namespace: kube-public */
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src /* DefaultAllow egress isolation for namespace: kube-system */
RETURN     all  --  anywhere             anywhere             match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src /* DefaultAllow egress isolation for namespace: kube-system */

Chain WEAVE-NPC-EGRESS (2 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED
RETURN     all  --  anywhere             anywhere             PHYSDEV match --physdev-in vethwe-bridge --physdev-is-bridged
RETURN     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL
RETURN     all  --  anywhere             base-address.mcast.net/4 
WEAVE-NPC-EGRESS-DEFAULT  all  --  anywhere             anywhere             state NEW
WEAVE-NPC-EGRESS-CUSTOM  all  --  anywhere             anywhere             state NEW mark match ! 0x40000/0x40000
NFLOG      all  --  anywhere             anywhere             state NEW mark match ! 0x40000/0x40000 nflog-group 86
DROP       all  --  anywhere             anywhere             mark match ! 0x40000/0x40000

Chain WEAVE-CANARY (0 references)
target     prot opt source               destination         
# Warning: iptables-legacy tables present, use iptables-legacy to see them
[root@master01 ~]# systemctl stausatus firewalld
 firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)
   Active: inactive (dead)
     Docs: man:firewalld(1)
[root@master01 ~]# [root@master01 ~]# systemctl status firewalldiptables -Lcurl http://10.44.0.1kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
nginx1174   1/1     Running   0          3m16s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl exec -it nginx1174 -- bash
root@nginx1174:/# ls
bin  boot  devetc  home  liblib64  media  mnt  optproc  root  run  sbin  srv  sys  tmp  usr  var
root@nginx1174:/# n cut   t     ls -l 
total 8
drwxr-xr-x   2 root root 4096 Oct 14  2019 bin
drwxr-xr-x   2 root root    6 Aug 30  2019 boot
drwxr-xr-x   5 root root  360 Sep 15 06:32 dev
drwxr-xr-x   1 root root   19 Sep 15 06:32 etc
drwxr-xr-x   2 root root    6 Aug 30  2019 home
drwxr-xr-x   1 root root   56 Oct 17  2019 lib
drwxr-xr-x   2 root root   34 Oct 14  2019 lib64
drwxr-xr-x   2 root root    6 Oct 14  2019 media
drwxr-xr-x   2 root root    6 Oct 14  2019 mnt
drwxr-xr-x   2 root root    6 Oct 14  2019 opt
dr-xr-xr-x 187 root root    0 Sep 15 06:32 proc
drwx------   2 root root   37 Oct 14  2019 root
drwxr-xr-x   1 root root   38 Sep 15 06:32 run
drwxr-xr-x   2 root root 4096 Oct 14  2019 sbin
drwxr-xr-x   2 root root    6 Oct 14  2019 srv
dr-xr-xr-x  13 root root    0 Sep 15 06:32 sys
drwxrwxrwt   1 root root    6 Oct 17  2019 tmp
drwxr-xr-x   1 root root   66 Oct 14  2019 usr
drwxr-xr-x   1 root root   19 Oct 14  2019 var
root@nginx1174:/# n
namei          newgrp         ngettext       nginx-debug    nisdomainname  nl             nologin        nsenter        
nawk           newusers       nginx          nice           njs            nohup          nproc          numfmt         
root@nginx1174:/# n
namei          newgrp         ngettext       nginx-debug    nisdomainname  nl             nologin        nsenter        
nawk           newusers       nginx          nice           njs            nohup          nproc          numfmt         
root@nginx1174:/# new
newgrp    newusers  
root@nginx1174:/# new e   cd ng     ls -l       exit
[root@master01 ~]# [root@master01 ~]# kubectl delete -g f nginx1.17.4.yaml 
pod "nginx1174" deleted
[root@master01 ~]# kunbectl create run nginx1.17.174 --image=nginx:1.17.4 --restart=Never --port=80
pod/nginx1174 created
[root@master01 ~]# kubectl get pods -o dwide
NAME        READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
nginx1174   1/1     Running   0          9s    10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# curl 10.36.0.1
^C
[root@master01 ~]# curl 10.36.0.1kubectl get pods -o widerun nginx1174 --image=nginx:1.17.4 --restart=Never --port=80delete -f nginx1.17.4.yaml exec -it nginx1174 -- bashget pods -o widesystemctl status firewalldiptables -Lcurl http://10.44.0.1kubectl get pods -o widecurl 10.44.0.1kubectl describe pod nginx1174nginx1174pod nginx1174
Name:             nginx1174
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Thu, 15 Sep 2022 12:09:16 +0530
Labels:           run=nginx1174
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx1174:
    Container ID:   containerd://0ed17baab6ac6d4e9c7e9bd9f1cdff959c1bcab07ec64edf8308ba32945f1005
    Image:          nginx:1.17.4
    Image ID:       docker.io/library/nginx@sha256:77ebc94e0cec30b20f9056bac1066b09fbdc049401b71850922c63fc0cc1762e
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 12:09:17 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7z429 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-7z429:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  81s    default-scheduler  Successfully assigned default/nginx1174 to worker01.airtel.internal.lab
  Normal  Pulled     7m52s  kubelet            Container image "nginx:1.17.4" already present on machine
  Normal  Created    7m52s  kubelet            Created container nginx1174
  Normal  Started    7m52s  kubelet            Started container nginx1174
[root@master01 ~]# kubectl set --image pod nginx1174 --image=nginx:1.15-alpine --
error: unknown flag: --image
See 'kubectl set --help' for usage.
[root@master01 ~]# kubectl set pod nginx1174 --image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine--image=nginx:1.15-alpine pod nginx1174
error: unknown flag: --image
See 'kubectl set --help' for usage.
[root@master01 ~]# kubectl set --image=nginx:1.15-alpine pod nginx1174  nginx
error: all resources must be specified before image changes: pod
[root@master01 ~]# kubectl set image nginx=nginx:1.15-alpine pod nginx1174pod/1174nginx e
error: unable to find container named "nginx"
[root@master01 ~]# kubectl set image pod/nginx1174 nginx=nginx:1.15-alpine=nginx:1.15-alpine=nginx:1.15-alpine=nginx:1.15-alpine=nginx:1.15-alpine=nginx:1.15-alpinenginx1174=nginx:1.15-alpine
pod/nginx1174 image updated
[root@master01 ~]# kubectl set image pod/nginx1174 nginx1174=nginx:1.15-alpine=nginx:1.15-alpinenginx=nginx:1.15-alpine pod nginx1174--imagepod nginx1174 --image=nginx:1.15-alpinedescribe pod nginx1174
Name:             nginx1174
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Thu, 15 Sep 2022 12:09:16 +0530
Labels:           run=nginx1174
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx1174:
    Container ID:   containerd://f8974423af4bf2079dabff87a53b4b723f061c85fdacd1b03bcd11262a70d0cb
    Image:          nginx:1.15-alpine
    Image ID:       docker.io/library/nginx@sha256:57a226fb6ab6823027c0704a9346a890ffb0cacde06bc19bbc234c8720673555
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 12:13:56 +0530
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 15 Sep 2022 12:09:17 +0530
      Finished:     Thu, 15 Sep 2022 12:13:50 +0530
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7z429 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-7z429:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age                  From               Message
  ----    ------     ----                 ----               -------
  Normal  Scheduled  4m52s                default-scheduler  Successfully assigned default/nginx1174 to worker01.airtel.internal.lab
  Normal  Pulled     11m                  kubelet            Container image "nginx:1.17.4" already present on machine
  Normal  Killing    6m49s                kubelet            Container nginx1174 definition changed, will be restarted
  Normal  Pulling    6m49s                kubelet            Pulling image "nginx:1.15-alpine"
  Normal  Created    6m43s (x2 over 11m)  kubelet            Created container nginx1174
  Normal  Started    6m43s (x2 over 11m)  kubelet            Started container nginx1174
  Normal  Pulled     6m43s                kubelet            Successfully pulled image "nginx:1.15-alpine" in 6.213239636s
[root@master01 ~]# kubectl describe pod nginx1174set image pod/nginx1174 nginx1174=nginx:1.15-alpine17.1
pod/nginx1174 image updated
[root@master01 ~]# kubectl set image pod/nginx1174 nginx1174=nginx:1.17.1describe pod nginx1174
Name:             nginx1174
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Thu, 15 Sep 2022 12:09:16 +0530
Labels:           run=nginx1174
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx1174:
    Container ID:   containerd://f8974423af4bf2079dabff87a53b4b723f061c85fdacd1b03bcd11262a70d0cb
    Image:          nginx:1.17.1
    Image ID:       docker.io/library/nginx@sha256:57a226fb6ab6823027c0704a9346a890ffb0cacde06bc19bbc234c8720673555
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 12:13:56 +0530
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 15 Sep 2022 12:09:17 +0530
      Finished:     Thu, 15 Sep 2022 12:13:50 +0530
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7z429 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-7z429:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age                  From               Message
  ----    ------     ----                 ----               -------
  Normal  Scheduled  60m                  default-scheduler  Successfully assigned default/nginx1174 to worker01.airtel.internal.lab
  Normal  Pulling    62m                  kubelet            Pulling image "nginx:1.15-alpine"
  Normal  Created    62m (x2 over 66m)    kubelet            Created container nginx1174
  Normal  Started    62m (x2 over 66m)    kubelet            Started container nginx1174
  Normal  Pulled     62m                  kubelet            Successfully pulled image "nginx:1.15-alpine" in 6.213239636s
  Normal  Killing    6m33s (x2 over 62m)  kubelet            Container nginx1174 definition changed, will be restarted
  Normal  Pulling    6m33s                kubelet            Pulling image "nginx:1.17.1"
[root@master01 ~]# kubectl describe pod nginx1174
Name:             nginx1174
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Thu, 15 Sep 2022 12:09:16 +0530
Labels:           run=nginx1174
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx1174:
    Container ID:   containerd://f8974423af4bf2079dabff87a53b4b723f061c85fdacd1b03bcd11262a70d0cb
    Image:          nginx:1.17.1
    Image ID:       docker.io/library/nginx@sha256:57a226fb6ab6823027c0704a9346a890ffb0cacde06bc19bbc234c8720673555
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 12:13:56 +0530
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 15 Sep 2022 12:09:17 +0530
      Finished:     Thu, 15 Sep 2022 12:13:50 +0530
    Ready:          True
    Restart Count:  1
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7z429 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-7z429:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age                  From               Message
  ----    ------     ----                 ----               -------
  Normal  Scheduled  60m                  default-scheduler  Successfully assigned default/nginx1174 to worker01.airtel.internal.lab
  Normal  Pulling    62m                  kubelet            Pulling image "nginx:1.15-alpine"
  Normal  Created    62m (x2 over 67m)    kubelet            Created container nginx1174
  Normal  Started    62m (x2 over 67m)    kubelet            Started container nginx1174
  Normal  Pulled     62m                  kubelet            Successfully pulled image "nginx:1.15-alpine" in 6.213239636s
  Normal  Killing    6m36s (x2 over 62m)  kubelet            Container nginx1174 definition changed, will be restarted
  Normal  Pulling    6m36s                kubelet            Pulling image "nginx:1.17.1"
[root@master01 ~]# kubectl describe pod nginx1174
Name:             nginx1174
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Thu, 15 Sep 2022 12:09:16 +0530
Labels:           run=nginx1174
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx1174:
    Container ID:   containerd://7a7685588a17e33b7988426268d90b555446f752fb159b07e50eedf7c92a684d
    Image:          nginx:1.17.1
    Image ID:       docker.io/library/nginx@sha256:b4b9b3eee194703fc2fa8afa5b7510c77ae70cfba567af1376a573a967c03dbb
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 13:09:53 +0530
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 15 Sep 2022 12:13:56 +0530
      Finished:     Thu, 15 Sep 2022 13:09:42 +0530
    Ready:          True
    Restart Count:  2
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7z429 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-7z429:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age                  From               Message
  ----    ------     ----                 ----               -------
  Normal  Scheduled  60m                  default-scheduler  Successfully assigned default/nginx1174 to worker01.airtel.internal.lab
  Normal  Pulling    62m                  kubelet            Pulling image "nginx:1.15-alpine"
  Normal  Pulled     62m                  kubelet            Successfully pulled image "nginx:1.15-alpine" in 6.213239636s
  Normal  Killing    6m42s (x2 over 62m)  kubelet            Container nginx1174 definition changed, will be restarted
  Normal  Pulling    6m42s                kubelet            Pulling image "nginx:1.17.1"
  Normal  Created    6m31s (x3 over 67m)  kubelet            Created container nginx1174
  Normal  Started    6m31s (x3 over 67m)  kubelet            Started container nginx1174
  Normal  Pulled     6m31s                kubelet            Successfully pulled image "nginx:1.17.1" in 10.555371537s
[root@master01 ~]# kubectl describe pod nginx1174set image pod/nginx1174 nginx1174=nginx:1.17.1get pod nginx1174 --version=true
error: unknown flag: --version
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pod nginx1174 --version=true-o jsonpath'{}.}s}p}e}c}.}c}o}n}t}a}i}n}e}r}[}]}.}i}m}a}g}e}[]{}"}"}\"}n"}='{.spec.container[].image}{"\n"}
> ^C
[root@master01 ~]# kubectl get pod nginx1174 -o jsonpath='{.spec.container[].image}{"\n"}'

[root@master01 ~]# kubectl get pod nginx1174 -o jsonpath='{.spec.container[].image}{"\n"}' jsonpath='{.spec.container[].image}{"\n"}'o jsonpath='{.spec.container[].image}{"\n"}' jsonpath='{.spec.container[].image}{"\n"}'O jsonpath='{.spec.container[].image}{"\n"}'
error: unknown shorthand flag: 'O' in -O
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get po nginx -o jsonpath='{.spec.containers[].image}{"\n"}'nginx:1.17.1
Error from server (NotFound): pods "nginx:1.17.1" not found
[root@master01 ~]# kubectl get po nginx:1.17.1 -o jsonpath='{.spec.containers[].image}{"\n"}'nginx1174
nginx:1.17.1
[root@master01 ~]# kubectl get pod nginx1174 -o jsonpath='{.spec.container[].image}{"\n"}'`'{.spec.container[].image}{"\n"}''{.spec.container[].image}{"\n"}'

[root@master01 ~]# kubectl get po nginx1174 -o jsonpath='{.spec.containers[].image}{"\n"}'
nginx:1.17.1
[root@master01 ~]# kukubectl get pods
NAME        READY   STATUS    RESTARTS      AGE
nginx1174   1/1     Running   2 (14m ago)   68m
[root@master01 ~]# kubectl delete pod nginx1174
pod "nginx1174" deleted
[root@master01 ~]# kubectl run nginx --image=nginx --resattrrstart=Never
pod/nginx created
[root@master01 ~]# kubectl exec -it nginx -/bin/sh
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
# ll
/bin/sh: 1: ll: not found
# ls
bin  boot  devdocker-entrypoint.d  docker-entrypoint.sh  etchome  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
# exit
[root@master01 ~]# kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          34s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl curl 10.44.0.1





^C
[root@master01 ~]# curl 10.44.0.1:80
^C
[root@master01 ~]# kubectl get pod nginx |grep -i IP
[root@master01 ~]# kubectl get pod nginx |grep -i IPipdaddress
[root@master01 ~]# kubectl get pod nginx |grep -i ipaddressdescribe
Name:             nginx
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Thu, 15 Sep 2022 13:17:09 +0530
Labels:           run=nginx
Annotations:      <none>
Status:           Running
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
Containers:
  nginx:
    Container ID:   containerd://4dd70c4f4b8f51f4f3a8daacdc881bad2298a39f484db9ff62068c580e9f06c3
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 15 Sep 2022 13:17:11 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4gv4l (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-4gv4l:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m13s  default-scheduler  Successfully assigned default/nginx to worker02.airtel.internal.lab
  Normal  Pulling    10m    kubelet            Pulling image "nginx"
  Normal  Pulled     10m    kubelet            Successfully pulled image "nginx" in 2.056657312s
  Normal  Created    10m    kubelet            Created container nginx
  Normal  Started    10m    kubelet            Started container nginx
[root@master01 ~]# kubectl describe pod nginxget pod nginx |grep -i ipaddressIPdescribe
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
    Type:                    Projected (a volume that contains injected data from multiple sources)
[root@master01 ~]# kubectl describe pod nginx |grep -i IP IP IPIP
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
[root@master01 ~]# [root@master01 ~]# kubectl delete pods nginx
pod "nginx" deleted
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never
pod/busybox created
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/1     ContainerCreating   0          5s
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          7s
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          8s
[root@master01 ~]# cdkubeectl exec -i t t busybox -- bash ls
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
error: cannot exec into a container in a completed pod; current phase is Succeeded
[root@master01 ~]# kubectl exec -it busybox lsget podsrun busybox --image=busybox --restart=Never busybox busybox busyboxbusybox busyboxd busyboxe busyboxl busyboxe busyboxt busyboxe busybox
error: the server doesn't have a resource type "busybox"
[root@master01 ~]# kubectl delete busyboxpbusyboxobusyboxdbusybox busybox
pod "busybox" deleted
[root@master01 ~]# [root@master01 ~]# kubectl delete pod busyboxbusyboxexec -it busybox lsget podsrun busybox --image=busybox --restart=Never --ls 
error: unknown flag: --ls
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never --ls  ls
pod/busybox created
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never -- lsls delete pod busybox busybox busybox busyboxbusybox busybox busybox busybox busybox busybox busyboxl busyboxo busyboxg busyboxs busybox
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox -p
Error from server (BadRequest): previous terminated container "busybox" in pod "busybox" not found
[root@master01 ~]# kubectl crrun busybox --image-busybox --restart=Never -- bin/bin/bash ""s"l"e"e"p" "3"6"0"0"
error: unknown flag: --image-busybox
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run busybox --image-busybox --restart=Never -- /bin/bash "sleep 3600"=
Error from server (AlreadyExists): pods "busybox" already exists
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          7m7s
[root@master01 ~]# kubectl get pods pods pods podsd podse podsl podse podst podse pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxget podsrun busybox --image=busybox --restart=Never -- /bin/bash "sleep 3600"
pod/busybox created
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS       RESTARTS   AGE
busybox   0/1     StartError   0          10s
[root@master01 ~]# kubectl get podsrun busybox --image=busybox --restart=Never -- /bin/bash "sleep 3600"delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxget podsrun busybox --image=busybox --restart=Never -- /bin/bash "sleep 3600" -c
pod/busybox created
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never -- /bin/bash -c "sleep 3600"delete pods busyboxget pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/1     ContainerCreating   0          3s
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS       RESTARTS   AGE
busybox   0/1     StartError   0          59s
[root@master01 ~]# kubectl get podsrun busybox --image=busybox --restart=Never -- /bin/bash -c "sleep 3600"delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxget podsrun busybox --image=busybox --restart=Never -- /bin/bash -c "sleep 3600"
pod/busybox created
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"delete pods busyboxget pods
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          3s
[root@master01 ~]# kubectl get podsrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxget podsrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"""""""""""c"u"r"l""""" --image=nginx get podsdelete pods busyboxkubectl delete pods busyboxkubectl delete pods busyboxget pods
No resources found in default namespace.
[root@master01 ~]# kubectl get podsdelete pods busyboxget podsrun busybox --image=busybox  --image=nginx --restart=Never delete pods busyboxget podsrun busybox --image=busybox --restart=Never -- /bin/bash -c "sleep 3600"
pod/busybox created
[root@master01 ~]# kubectl create run nginx --image-nginx =nginx --restart =Never 
pod/nginx created
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/1     StartError          0          34s
nginx     0/1     ContainerCreating   0          4s
[root@master01 ~]# kubectl get podsrun nginx --image=nginx --restart=Never busybox --image=busybox --restart=Never -- /bin/bash -c "sleep 3600"
Error from server (AlreadyExists): pods "busybox" already exists
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"get podsrun nginx --image=nginx --restart=Never busybox --image=busybox --restart=Never -- /bin/bash -c "sleep 3600"get podsdelete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"get podsrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"
pod/busybox created
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"delete pods busyboxrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/1     ContainerCreating   0          3s
nginx     1/1     Running             0          20s
[root@master01 ~]# kubectl get podsrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"get podskubectl exec -it busybox -- wget -o 
wget: option requires an argument -- o
BusyBox v1.34.1 (2022-09-13 15:42:10 UTC) multi-call binary.

Usage: wget [-cqS] [--spider] [-O FILE] [-o LOGFILE] [--header 'HEADER: VALUE'] [-Y on/off]
[--no-check-certificate] [-P DIR] [-U AGENT] [-T SEC] URL...

Retrieve files via HTTP or FTP

--spiderOnly check URL existence: $? is 0 if exists
--no-check-certificateDon't validate the server's certificate
-cContinue retrieval of aborted transfer
-qQuiet
-P DIRSave to DIR (default .)
-S    Show server response
-T SECNetwork read timeout is SEC seconds
-O FILESave to FILE ('-' for stdout)
-o LOGFILELog messages to FILE
-U STRUse STR for User-Agent header
-Y on/offUse proxy
command terminated with exit code 1
[root@master01 ~]# kubectl exec -it busybox -- wget -o kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          34s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
nginx     1/1     Running   0          51s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o wideexec -it busybox -- wget -o -  10.36.0.1 
Connecting to 10.36.0.1 (10.36.0.1:80)
wget: can't connect to remote host (10.36.0.1): Connection timed out
command terminated with exit code 1
[root@master01 ~]# kubectl get po -o wide -l app=nginx
No resources found in default namespace.
[root@master01 ~]# dcocker sps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
[root@master01 ~]#  docker ps -a
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
[root@master01 ~]# curl localhost
curl: (7) Failed to connect to localhost port 80: Connection refused
[root@master01 ~]# curl localhostdocker ps -akubectl get po -o wide -l app=nginxexec -it busybox -- wget -o- 10.36.0.1 get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          3m53s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
nginx     1/1     Running   0          4m10s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl describekubectl get pods nginx -o jsonpath-='{}.}i}t}e}m}.}c}o}n}t}a}i}n}e}r}s}[}]}.}p}o}}}p}o}r}t}[]{}"}"}\"}n"}'

[root@master01 ~]# kubectl get pods nginx -o jsonpath='{.item.containers[].port}{"\n"}'.containers[].port}{"\n"}'.containers[].port}{"\n"}'.containers[].port}{"\n"}'.containers[].port}{"\n"}'s.containers[].port}{"\n"}'p.containers[].port}{"\n"}'e.containers[].port}{"\n"}'c.containers[].port}{"\n"}'

[root@master01 ~]# kubectl get pods nginx -o jsonpath='{.spec.containers[].port}{"\n"}'}{"\n"}'}{"\n"}'}{"\n"}'}{"\n"}'i}{"\n"}'m}{"\n"}'a}{"\n"}'g}{"\n"}'e}{"\n"}'
nginx
[root@master01 ~]# kubectl get pods nginx -o jsonpath='{.spec.containers[].image}{"\n"}'.}{"\n"}'p}{"\n"}'o}{"\n"}'r}{"\n"}'t}{"\n"}'

[root@master01 ~]# kubectl get pods nginx -o jsonpath='{.spec.containers[].image.port}{"\n"}'s}{"\n"}'

[root@master01 ~]# kubectl describe npo nginx
Name:             nginx
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Thu, 15 Sep 2022 13:46:24 +0530
Labels:           run=nginx
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx:
    Container ID:   containerd://3712854130596c569a16b9882ed87d12d4a5c21206a71f4c5cbffa8bd061b4ec
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 15 Sep 2022 13:46:34 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbd6s (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-jbd6s:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  6m23s  default-scheduler  Successfully assigned default/nginx to worker01.airtel.internal.lab
  Normal  Pulling    12m    kubelet            Pulling image "nginx"
  Normal  Pulled     12m    kubelet            Successfully pulled image "nginx" in 10.108975495s
  Normal  Created    12m    kubelet            Created container nginx
  Normal  Started    12m    kubelet            Started container nginx
[root@master01 ~]# kubectl delete po -nginx
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl delete po -nginxnginx
pod "nginx" deleted
[root@master01 ~]# kubectl delete po nginx-nginxscribe po nginxget pods nginx -o jsonpath='{.spec.containers[].image.ports}{"\n"}'}{"\n"}'}{"\n"}'port}{"\n"}'item.containers[].port}{"\n"}'-o widecurl localhostdocker ps -akubectl get po -o wide -l app=nginxexec -it busybox -- wget -o- 10.36.0.1 get pods -o wideexec -it busybox -- wget -o get podsrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"delete pods busyboxrun busybox --image=busybox --restart=Never -- /bin/sh -c "sleep 3600"get podsrun nginx --image=nginx --restart=Never --port=80
pod/nginx created
[root@master01 ~]# kubectl get pods -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
busybox   1/1     Running   0          6m53s   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
nginx     1/1     Running   0          7s      10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widerun nginx --image=nginx --restart=Never --port=80delete po nginx-nginxscribe po nginxget pods nginx -o jsonpath='{.spec.containers[].image.ports}{"\n"}'}{"\n"}'}{"\n"}'port}{"\n"}'item.containers[].port}{"\n"}'-o widecurl localhostdocker ps -akubectl get po -o wide -l app=nginxexec -it busybox -- wget -o- 10.36.0.1 
Connecting to 10.36.0.1 (10.36.0.1:80)
^Ccommand terminated with exit code 130
[root@master01 ~]# kubectl get pods -n kube-system
NAME                                                   READY   STATUS    RESTARTS        AGE
coredns-565d847f94-48j9p                               1/1     Running   1 (3d17h ago)   3d17h
coredns-565d847f94-xcm8b                               1/1     Running   1 (3d17h ago)   3d17h
etcd-master01.airtel.internal.lab                      1/1     Running   1 (3d17h ago)   3d17h
kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (3d17h ago)   3d17h
kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (3d17h ago)   3d17h
kube-proxy-bk8v6                                       1/1     Running   0               3d17h
kube-proxy-rfb9j                                       1/1     Running   1 (3d17h ago)   3d17h
kube-proxy-vpx59                                       1/1     Running   0               3d17h
kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (3d17h ago)   3d17h
weave-net-42gjg                                        2/2     Running   3 (3d17h ago)   3d17h
weave-net-44k9t                                        2/2     Running   1 (3d17h ago)   3d17h
weave-net-mdv4h                                        2/2     Running   0               3d17h
[root@master01 ~]# kubectl exec -it busybox -- wget -o- 10.36.0.1curl
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "85f00a9542670449d9c4abe5dcef5512370d2e1bc7cb0e26679db55532150516": OCI runtime exec failed: exec failed: unable to start container process: exec: "curl": executable file not found in $PATH: unknown
[root@master01 ~]# kubectl exec -it busybox -- curl -o- 10.36.0.1 10.36.0.1 10.36.0.1 10.36.0.110.36.0.1
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "61e5af90a75a34480f3ae3003966665826863f4eaaea4b239013ede8b296145f": OCI runtime exec failed: exec failed: unable to start container process: exec: "curl": executable file not found in $PATH: unknown
[root@master01 ~]# kubectl exec -it busybox -- curl 10.36.0.1
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "6800be5d0edaac6531f68f2e1e52082b200ce806051903c9b4918fa2cfc7eaf6": OCI runtime exec failed: exec failed: unable to start container process: exec: "curl": executable file not found in $PATH: unknown
[root@master01 ~]# kubectl exec busybox -- curl 10.36.0.1-it -o- 10.36.0.1get pods -n kube-systemexec -it busybox -- curl -o- 10.36.0.1get pods -n kube-systemexec -it busybox -- wget -o- 10.36.0.1 
Connecting to 10.36.0.1 (10.36.0.1:80)
^C
[root@master01 ~]# sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 1
[root@master01 ~]# sysctl net.bridge.bridge-nf-call-iptables
net.bridge.bridge-nf-call-iptables = 1
[root@master01 ~]# =~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.15 14:08:24 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Authenticating with public key "rsa-key-20220112"
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Sep 15 10:35:27 2022 from 192.168.200.15
[root@master01 ~]# ssh 172.16.102.23
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Sep 13 15:03:41 2022 from 172.16.102.22
[root@worker01 ~]# sysctl net.bridge.bridge-nf-call-iptables
net.bridge.bridge-nf-call-iptables = 1
[root@worker01 ~]# logout
Connection to 172.16.102.23 closed.
[root@master01 ~]# ssh 172.16.102.234
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Sep 13 15:01:59 2022 from 172.16.102.22
[root@worker02 ~]# sysctl net.bridge.bridge-nf-call-iptables
net.bridge.bridge-nf-call-iptables = 1
[root@worker02 ~]# kubectl exec -it busybox ip a
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
42: eth0@if43: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1376 qdisc noqueue 
    link/ether 7a:83:e6:00:3c:f8 brd ff:ff:ff:ff:ff:ff
    inet 10.44.0.1/12 brd 10.47.255.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::7883:e6ff:fe00:3cf8/64 scope link 
       valid_lft forever preferred_lft forever
[root@master01 ~]# iptables -L 
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-NODEPORTS  all  --  anywhere             anywhere             /* kubernetes health check service ports */
KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
KUBE-FIREWALL  all  --  anywhere             anywhere            
DROP       tcp  --  anywhere             localhost            tcp dpt:6784 ADDRTYPE match src-type !LOCAL ! ctstate RELATED,ESTABLISHED /* Block non-local access to Weave Net control port */
WEAVE-NPC-EGRESS  all  --  anywhere             anywhere            

Chain FORWARD (policy DROP)
target     prot opt source               destination         
WEAVE-NPC-EGRESS  all  --  anywhere             anywhere             /* NOTE: this must go before '-j KUBE-FORWARD' */
WEAVE-NPC  all  --  anywhere             anywhere             /* NOTE: this must go before '-j KUBE-FORWARD' */
NFLOG      all  --  anywhere             anywhere             state NEW nflog-group 86
DROP       all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
KUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */
DOCKER-USER  all  --  anywhere             anywhere            
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
KUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
KUBE-FIREWALL  all  --  anywhere             anywhere            

Chain KUBE-FIREWALL (2 references)
target     prot opt source               destination         
DROP       all  -- !127.0.0.0/8          127.0.0.0/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT
DROP       all  --  anywhere             anywhere             /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000

Chain DOCKER (1 references)
target     prot opt source               destination         

Chain DOCKER-ISOLATION-STAGE-1 (1 references)
target     prot opt source               destination         
DOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere            
RETURN     all  --  anywhere             anywhere            

Chain DOCKER-ISOLATION-STAGE-2 (1 references)
target     prot opt source               destination         
DROP       all  --  anywhere             anywhere            
RETURN     all  --  anywhere             anywhere            

Chain KUBE-KUBELET-CANARY (0 references)
target     prot opt source               destination         

Chain DOCKER-USER (1 references)
target     prot opt source               destination         
RETURN     all  --  anywhere             anywhere            

Chain KUBE-PROXY-CANARY (0 references)
target     prot opt source               destination         

Chain KUBE-EXTERNAL-SERVICES (2 references)
target     prot opt source               destination         

Chain KUBE-NODEPORTS (1 references)
target     prot opt source               destination         

Chain KUBE-SERVICES (2 references)
target     prot opt source               destination         

Chain KUBE-FORWARD (1 references)
target     prot opt source               destination         
DROP       all  --  anywhere             anywhere             ctstate INVALID
ACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding rules */ mark match 0x4000/0x4000
ACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED

Chain KUBE-PROXY-FIREWALL (3 references)
target     prot opt source               destination         

Chain WEAVE-NPC-INGRESS (1 references)
target     prot opt source               destination         

Chain WEAVE-NPC-DEFAULT (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             match-set weave-;rGqyMIl1HN^cfDki~Z$3]6!N dst /* DefaultAllow ingress isolation for namespace: default */
ACCEPT     all  --  anywhere             anywhere             match-set weave-]B*(W?)t*z5O17G044[gUo#$l dst /* DefaultAllow ingress isolation for namespace: kube-node-lease */
ACCEPT     all  --  anywhere             anywhere             match-set weave-Rzff}h:=]JaaJl/G;(XJpGjZ[ dst /* DefaultAllow ingress isolation for namespace: kube-public */
ACCEPT     all  --  anywhere             anywhere             match-set weave-P.B|!ZhkAr5q=XZ?3}tMBA+0 dst /* DefaultAllow ingress isolation for namespace: kube-system */

Chain WEAVE-NPC (1 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED
ACCEPT     all  --  anywhere             base-address.mcast.net/4 
ACCEPT     all  --  anywhere             anywhere             PHYSDEV match --physdev-out vethwe-bridge --physdev-is-bridged
WEAVE-NPC-DEFAULT  all  --  anywhere             anywhere             state NEW
WEAVE-NPC-INGRESS  all  --  anywhere             anywhere             state NEW

Chain WEAVE-NPC-EGRESS-ACCEPT (4 references)
target     prot opt source               destination         
MARK       all  --  anywhere             anywhere             MARK or 0x40000

Chain WEAVE-NPC-EGRESS-CUSTOM (1 references)
target     prot opt source               destination         

Chain WEAVE-NPC-EGRESS-DEFAULT (1 references)
target     prot opt source               destination         
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src /* DefaultAllow egress isolation for namespace: default */
RETURN     all  --  anywhere             anywhere             match-set weave-s_+ChJId4Uy_$}G;WdH|~TK)I src /* DefaultAllow egress isolation for namespace: default */
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src /* DefaultAllow egress isolation for namespace: kube-node-lease */
RETURN     all  --  anywhere             anywhere             match-set weave-sui%__gZ}{kX~oZgI_Ttqp=Dp src /* DefaultAllow egress isolation for namespace: kube-node-lease */
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src /* DefaultAllow egress isolation for namespace: kube-public */
RETURN     all  --  anywhere             anywhere             match-set weave-41s)5vQ^o/xWGz6a20N:~?#|E src /* DefaultAllow egress isolation for namespace: kube-public */
WEAVE-NPC-EGRESS-ACCEPT  all  --  anywhere             anywhere             match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src /* DefaultAllow egress isolation for namespace: kube-system */
RETURN     all  --  anywhere             anywhere             match-set weave-E1ney4o[ojNrLk.6rOHi;7MPE src /* DefaultAllow egress isolation for namespace: kube-system */

Chain WEAVE-NPC-EGRESS (2 references)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED
RETURN     all  --  anywhere             anywhere             PHYSDEV match --physdev-in vethwe-bridge --physdev-is-bridged
RETURN     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL
RETURN     all  --  anywhere             base-address.mcast.net/4 
WEAVE-NPC-EGRESS-DEFAULT  all  --  anywhere             anywhere             state NEW
WEAVE-NPC-EGRESS-CUSTOM  all  --  anywhere             anywhere             state NEW mark match ! 0x40000/0x40000
NFLOG      all  --  anywhere             anywhere             state NEW mark match ! 0x40000/0x40000 nflog-group 86
DROP       all  --  anywhere             anywhere             mark match ! 0x40000/0x40000

Chain WEAVE-CANARY (0 references)
target     prot opt source               destination         
# Warning: iptables-legacy tables present, use iptables-legacy to see them
[root@master01 ~]# [root@master01 ~]# kubectl get securitypolicy
error: the server doesn't have a resource type "securitypolicy"
[root@master01 ~]# kubectl get securitypolicyls
admin-pod.yaml   ds-kusc00201.yaml         etcd-v3.3.13-linux-amd64.tar.gz  nginx1.17.4.yaml  nginx.yaml  non-root-pod.yaml  pod-spec-KUCC00108.yaml  pv-analytics.yaml
anaconda-ks.cfg  etcd-v3.3.13-linux-amd64  nfs-pv.yaml                      nginx-same.yaml   nodes.txt   pod-redis.yaml     policynetwork.yaml       web-project-268.yaml
[root@master01 ~]# kubectl delete -f policynetwork.yaml 
networkpolicy.networking.k8s.io "default-deny" deleted
[root@master01 ~]# kubectl delete -f policynetwork.yaml lskubectl get securitypolicyiptables -L kubectl exec -it busybox ip asysctl net.bridge.bridge-nf-call-iptablesipv4.ip_forwardkubectl exec busybox -- wget -o- 10.36.0.1 
Connecting to 10.36.0.1 (10.36.0.1:80)
saving to 'index.html'
index.html           100% |********************************|   615  0:00:00 ETA
'index.html' saved
[root@master01 ~]# logout
Connection to 172.16.102.24 closed.
[root@master01 ~]# logout
[root@master01 ~]# kubectl delete pod nginx busybox
pod "nginx" deleted
pod "busybox" deleted
[root@master01 ~]# kubectl create run busybox --image=bustybox --restart=Never -- echo -o- ""b"i"n"/"""""/"b"i"n"/"s"h" """How are you""  -it
How are you
[root@master01 ~]# kubectl deede;lete pod busybox
pod "busybox" deleted
[root@master01 ~]# [root@master01 ~]# kubectl delete pod busyboxrun busybox --image=busybox --restart=Never -it -- echo "How are you"  -rnm-
How are you
pod "busybox" deleted
[root@master01 ~]# kuebectl get pods
No resources found in default namespace.
[root@master01 ~]# [root@master01 ~]# kubectl  get pods
No resources found in default namespace.
[root@master01 ~]# kubectl get podsrun nginx --image=nginx --nrestart = NNever --port--port=980
pod/nginx created
[root@master01 ~]# kubectl get pod nginx --v=7
I0915 14:37:53.306688  110350 loader.go:374] Config loaded from file:  /root/.kube/config
I0915 14:37:53.315098  110350 round_trippers.go:463] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/nginx
I0915 14:37:53.315120  110350 round_trippers.go:469] Request Headers:
I0915 14:37:53.315130  110350 round_trippers.go:473]     Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json
I0915 14:37:53.315142  110350 round_trippers.go:473]     User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe
I0915 14:37:53.328256  110350 round_trippers.go:574] Response Status: 200 OK in 13 milliseconds
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          12s
[root@master01 ~]# kubectl get pod nginx --v=78
I0915 14:38:07.669214  110358 loader.go:374] Config loaded from file:  /root/.kube/config
I0915 14:38:07.682597  110358 round_trippers.go:463] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/nginx
I0915 14:38:07.682631  110358 round_trippers.go:469] Request Headers:
I0915 14:38:07.682652  110358 round_trippers.go:473]     User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe
I0915 14:38:07.682669  110358 round_trippers.go:473]     Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json
I0915 14:38:07.695498  110358 round_trippers.go:574] Response Status: 200 OK in 12 milliseconds
I0915 14:38:07.695538  110358 round_trippers.go:577] Response Headers:
I0915 14:38:07.695559  110358 round_trippers.go:580]     X-Kubernetes-Pf-Flowschema-Uid: d43afb78-88f3-4116-97f6-ded19328e058
I0915 14:38:07.695582  110358 round_trippers.go:580]     X-Kubernetes-Pf-Prioritylevel-Uid: 74bbab13-ad92-41cb-beb6-909df531ce22
I0915 14:38:07.695602  110358 round_trippers.go:580]     Date: Thu, 15 Sep 2022 09:08:07 GMT
I0915 14:38:07.695621  110358 round_trippers.go:580]     Audit-Id: 03d5a8c6-8cff-4282-9931-0e8938bfb531
I0915 14:38:07.695643  110358 round_trippers.go:580]     Cache-Control: no-cache, private
I0915 14:38:07.695662  110358 round_trippers.go:580]     Content-Type: application/json
I0915 14:38:07.695908  110358 request.go:1073] Response Body: {"kind":"Table","apiVersion":"meta.k8s.io/v1","metadata":{"resourceVersion":"457466"},"columnDefinitions":[{"name":"Name","type":"string","format":"name","description":"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names","priority":0},{"name":"Ready","type":"string","format":"","description":"The aggregate readiness state of this pod for accepting traffic.","priority":0},{"name":"Status","type":"string","format":"","description":"The aggregate status of the containers in this pod.","priority":0},{"name":"Restarts","type":"string","format":"","description":"The number of times the containers in this pod have been restarted and when the last container in this pod has restarted.","priority":0},{"name":"Age","type":"st [truncated 3608 chars]
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          26s
[root@master01 ~]# kubectl get pod nginx --v=89
I0915 14:38:15.826777  110367 loader.go:374] Config loaded from file:  /root/.kube/config
I0915 14:38:15.836843  110367 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json" -H "User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe" 'https://172.16.102.22:6443/api/v1/namespaces/default/pods/nginx'
I0915 14:38:15.837582  110367 round_trippers.go:510] HTTP Trace: Dial to tcp:172.16.102.22:6443 succeed
I0915 14:38:15.851627  110367 round_trippers.go:553] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/nginx 200 OK in 14 milliseconds
I0915 14:38:15.851679  110367 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 11 ms ServerProcessing 2 ms Duration 14 ms
I0915 14:38:15.851723  110367 round_trippers.go:577] Response Headers:
I0915 14:38:15.851746  110367 round_trippers.go:580]     Cache-Control: no-cache, private
I0915 14:38:15.851765  110367 round_trippers.go:580]     Content-Type: application/json
I0915 14:38:15.851789  110367 round_trippers.go:580]     X-Kubernetes-Pf-Flowschema-Uid: d43afb78-88f3-4116-97f6-ded19328e058
I0915 14:38:15.851813  110367 round_trippers.go:580]     X-Kubernetes-Pf-Prioritylevel-Uid: 74bbab13-ad92-41cb-beb6-909df531ce22
I0915 14:38:15.851832  110367 round_trippers.go:580]     Date: Thu, 15 Sep 2022 09:08:15 GMT
I0915 14:38:15.851850  110367 round_trippers.go:580]     Audit-Id: d53f2c57-1b3f-485b-852d-6f8dae31bd1e
I0915 14:38:15.852009  110367 request.go:1073] Response Body: {"kind":"Table","apiVersion":"meta.k8s.io/v1","metadata":{"resourceVersion":"457466"},"columnDefinitions":[{"name":"Name","type":"string","format":"name","description":"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names","priority":0},{"name":"Ready","type":"string","format":"","description":"The aggregate readiness state of this pod for accepting traffic.","priority":0},{"name":"Status","type":"string","format":"","description":"The aggregate status of the containers in this pod.","priority":0},{"name":"Restarts","type":"string","format":"","description":"The number of times the containers in this pod have been restarted and when the last container in this pod has restarted.","priority":0},{"name":"Age","type":"string","format":"","description":"CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.\n\nPopulated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata","priority":0},{"name":"IP","type":"string","format":"","description":"IP address allocated to the pod. Routable at least within the cluster. Empty if not yet allocated.","priority":1},{"name":"Node","type":"string","format":"","description":"NodeName is a request to schedule this pod onto a specific node. If it is non-empty, the scheduler simply schedules this pod onto that node, assuming that it fits resource requirements.","priority":1},{"name":"Nominated Node","type":"string","format":"","description":"nominatedNodeName is set only when this pod preempts other pods on the node, but it cannot be scheduled right away as preemption victims receive their graceful termination periods. This field does not guarantee that the pod will be scheduled on this node. Scheduler may decide to place the pod elsewhere if other nodes become available sooner. Scheduler may also decide to give the resources on this node to a higher priority pod that is created after preemption. As a result, this field may be different than PodSpec.nodeName when the pod is scheduled.","priority":1},{"name":"Readiness Gates","type":"string","format":"","description":"If specified, all readiness gates will be evaluated for pod readiness. A pod is ready when all its containers are ready AND all conditions specified in the readiness gates have status equal to \"True\" More info: https://git.k8s.io/enhancements/keps/sig-network/580-pod-readiness-gates","priority":1}],"rows":[{"cells":["nginx","1/1","Running","0","34s","10.44.0.1","worker02.airtel.internal.lab","\u003cnone\u003e","\u003cnone\u003e"],"object":{"kind":"PartialObjectMetadata","apiVersion":"meta.k8s.io/v1","metadata":{"name":"nginx","namespace":"default","uid":"f50a3f9d-d4e7-430c-bb97-7dcfd88a32bc","resourceVersion":"457466","creationTimestamp":"2022-09-15T09:07:41Z","labels":{"run":"nginx"},"managedFields":[{"manager":"kubectl-run","operation":"Update","apiVersion":"v1","time":"2022-09-15T09:07:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:labels":{".":{},"f:run":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"nginx\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-09-15T09:07:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.44.0.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]}}}]}
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          34s
[root@master01 ~]# [root@master01 ~]# kubectl get pod -o=customerm=""N""n"g"i"n"x":.metadata.name,POD_STATUS:.status.containerStatuses[].state"
> ^C
[root@master01 ~]# kubectl get pod -o=custom="nginx":.metadata.name,POD_STATUS:.status.containerStatuses[].state":.metadata.name,POD_STATUS:.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state"n:.status.containerStatuses[].state"g:.status.containerStatuses[].state"i:.status.containerStatuses[].state"n:.status.containerStatuses[].state"x:.status.containerStatuses[].state"
error: unable to match a printer suitable for the output format "custom=nginx:.metadata.name,nginx:.status.containerStatuses[].state", allowed formats are: custom-columns,custom-columns-file,go-template,go-template-file,json,jsonpath,jsonpath-as-json,jsonpath-file,name,template,templatefile,wide,yaml
[root@master01 ~]# kubectl get pod -o=custom="nginx:.metadata.name,nginx:.status.containerStatuses[].state":.metadata.name,nginx:.status.containerStatuses[].state":.metadata.name,nginx:.status.containerStatuses[].state":.metadata.name,nginx:.status.containerStatuses[].state":.metadata.name,nginx:.status.containerStatuses[].state":.metadata.name,nginx:.status.containerStatuses[].state"P:.metadata.name,nginx:.status.containerStatuses[].state"O:.metadata.name,nginx:.status.containerStatuses[].state"D:.metadata.name,nginx:.status.containerStatuses[].state"_:.metadata.name,nginx:.status.containerStatuses[].state"N:.metadata.name,nginx:.status.containerStatuses[].state"A:.metadata.name,nginx:.status.containerStatuses[].state"M:.metadata.name,nginx:.status.containerStatuses[].state"E:.metadata.name,nginx:.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state":.status.containerStatuses[].state"P:.status.containerStatuses[].state"O:.status.containerStatuses[].state"D:.status.containerStatuses[].state"_:.status.containerStatuses[].state"S:.status.containerStatuses[].state"T:.status.containerStatuses[].state"A:.status.containerStatuses[].state"T:.status.containerStatuses[].state"U:.status.containerStatuses[].state"S:.status.containerStatuses[].state"
error: unable to match a printer suitable for the output format "custom=POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state", allowed formats are: custom-columns,custom-columns-file,go-template,go-template-file,json,jsonpath,jsonpath-as-json,jsonpath-file,name,template,templatefile,wide,yaml
[root@master01 ~]# kubectl get pod -o=custom="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"-="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"c="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"o="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"l="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"u="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"m="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"n="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"s="POD_NAME:.metadata.name,POD_STATUS:.status.containerStatuses[].state"
POD_NAME   POD_STATUS
nginx      map[running:map[startedAt:2022-09-15T08:59:42Z]]
[root@master01 ~]# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          4m21s
[root@master01 ~]# kubectl get pods --sort=name-=b=y=.meteadata.name
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          43m
[root@master01 ~]# kubectl get pods --sort-by=.metadata.nametime
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          43m
[root@master01 ~]# kubectl get pods --sort-by=.metadata.timecreationTimestamp
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          43m
[root@master01 ~]# kubectl get pods--sort-by=.metadata.creationTimestamp
error: the server doesn't have a resource type "pods--sort-by="
[root@master01 ~]# kubectl get pods--sort-by=.metadata.creationTimestamp --sort-by=.metadata.creationTimestamp
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          44m
[root@master01 ~]# kubectl describe pod nginx
Name:             nginx
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker02.airtel.internal.lab/172.16.102.24
Start Time:       Thu, 15 Sep 2022 14:29:39 +0530
Labels:           run=nginx
Annotations:      <none>
Status:           Running
IP:               10.44.0.1
IPs:
  IP:  10.44.0.1
Containers:
  nginx:
    Container ID:   containerd://686fd6151bd859c9cd8a1afbcdeaeb2a6f9ec81eb9aa1e89ec89464144757db1
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 15 Sep 2022 14:29:42 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w5gj6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-w5gj6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  45m   default-scheduler  Successfully assigned default/nginx to worker02.airtel.internal.lab
  Normal  Pulling    53m   kubelet            Pulling image "nginx"
  Normal  Pulled     53m   kubelet            Successfully pulled image "nginx" in 2.043662989s
  Normal  Created    53m   kubelet            Created container nginx
  Normal  Started    53m   kubelet            Started container nginx
[root@master01 ~]# kubectl run busybox --image-bustybox -- echo ""H"e"l"o" """l"o" "W"o"r"l"d"" --o-/bin/sh "ls; sleep 37600" > >- >- >d >r >y >- >r >u >n >= >c >l >i >n >e >t > >> > > >e >n >t > >- >o > >y >a >m >l > multe-container.yaml
error: unknown flag: --image-busybox
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run busybox --image-busybox -- echo "Hello World" /bin/sh "ls; sleep 3600" --dry-run=client -o yaml > multe-container.yaml=
[root@master01 ~]# vim multe-container.yaml 
"multe-container.yaml" 1L, 20C  pod/busybox created
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      1,1All::QE492: Not an editor command: Q1,1All::q[root@master01 ~]# vim multe-container.yaml kubectl run busybox --image=busybox -- echo "Hello World" /bin/sh "ls; sleep 3600" --dry-run=client -o yaml > multe-container.yamlvim multe-container.yaml kubectl get pods
NAME      READY   STATUS             RESTARTS        AGE
busybox   0/1     CrashLoopBackOff   1 (6m44s ago)   18s
nginx     1/1     Running            0               50m
[root@master01 ~]# kubectl get podsdeldelete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxget podsvim multe-container.yaml kubectl run busybox --image=busybox -- echo "Hello World" /bin/sh "ls; sleep 3600" --dry-run=client -o yaml > multe-container.yaml -o --dry-run=client
error: unable to match a printer suitable for the output format "--dry-run=client", allowed formats are: go-template,go-template-file,json,jsonpath,jsonpath-as-json,jsonpath-file,name,template,templatefile,yaml
[root@master01 ~]# kubectl run busybox --image=busybox -o --dry-run=client -- echo "Hello World" /bin/sh "ls; sleep 3600" > multe-container.yaml resta--restart=Never -o yaml
[root@master01 ~]# vim multe-container.yaml 
"multe-container.yaml" 20L, 313C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - echo
    - Hello World
    - /bin/sh
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3::q![root@master01 ~]# kubcectl create -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl get opo
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          5s
nginx     1/1     Running     0          52m
[root@master01 ~]# kubectl get po
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          8s
nginx     1/1     Running     0          52m
[root@master01 ~]# kubectl get po
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          9s
nginx     1/1     Running     0          52m
[root@master01 ~]# kubectl get po
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          18s
nginx     1/1     Running     0          53m
[root@master01 ~]# kubectl get pocreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- echo "Hello World" /bin/sh "ls; sleep 3600" > multe-container.yamlo --dry-run=clientdelete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxget pocreate -f multe-container.yaml vim
"multe-container.yaml" 20L, 313C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - echo
    - Hello World
    - /bin/sh
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      13,1All~@k   2~@k   1~@k   0~@k   9,1 ~@k   8::QE492: Not an editor command: Q8,1All::q![root@master01 ~]# vim multe-container.yaml kubectl delete pods busyboxvim multe-container.yaml kubectl create -f 
pod/busybox created
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          4s
nginx     1/1     Running     0          56m
[root@master01 ~]# kubectl get pods pods pods podsl podso podsg podss pods busybox
Error from server (NotFound): pods "pods" not found
[root@master01 ~]# kubectl logs pods busybox busybox busybox busybox busyboxbusybox busybox
Hello World /bin/sh ls; sleep 3600
[root@master01 ~]# kubectl logs  busyboxpods busyboxget podscreate -f  multe-container.yaml vimkubectl delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxlogs pods busyboxget podscreate -f  multe-container.yaml vim
"multe-container.yaml" 20L, 313C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - echo
    - Hello World
    - /bin/sh
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      8,1All~@k   9~@k   10,1dd  
~                                                                                                                                                                                      10,5Alldd  
~                                                                                                                                                                                      10,5Alldd  
~                                                                                                                                                                                      10,5Alldd  
~                                                                                                                                                                                      10,5Alldd  
~                                                                                                                                                                                      10,5All~@k   1~@k   2~@k   3~@k   2dd  
~                                                                                                                                                                                      12,3Alldd  
~                                                                                                                                                                                      12,3All~@k   1o -- INSERT --12,5Top12,5Allcommand: ['sh', '-c', 'echo "Hello, Kubernetes
  restartPolicy: Never
status: {}!" && sleep 3600']
  restartPolicy: Never
status: {}[]69^[  12,68All~@k   []7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   59~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   4~@k   3~@k   2~@k   1~@k   0~@k   49~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   39~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   29~@k   8i -- INSERT --12,28Alllecho "Hello, Kubernetes!" && sleep 3600']9secho "Hello, Kubernetes!" && sleep 3600']30 echo "Hello, Kubernetes!" && sleep 3600']1&echo "Hello, Kubernetes!" && sleep 3600']2&echo "Hello, Kubernetes!" && sleep 3600']3 echo "Hello, Kubernetes!" && sleep 3600']4^[  12,33All::wq!"multe-container.yaml" 14L, 270C written
[root@master01 ~]# vim multe-container.yaml kubectl delete pods busyboxlogs pods busyboxget podscreate -f  multe-container.yaml 
Error from server (BadRequest): error when creating "multe-container.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field PodSpec.spec.containers of type []v1.Container
[root@master01 ~]# kubectl create -f  multe-container.yaml vim
"multe-container.yaml" 14L, 270C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
    image: busybox
    name: busybox
    command: ['sh', '-c', 'ls && echo "Hello, Kubernetes!" && sleep 3600']
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      12,33All^[  ^[  ::Q!q![root@master01 ~]# vim multe-container.yaml kubectl create -f vimkubectl delete pods busybox
Error from server (NotFound): pods "busybox" not found
[root@master01 ~]# kubectl delete pods busyboxvim multe-container.yaml kubectl create -f vimkubectl delete pods busyboxlogs pods busyboxget podscreate -f  multe-container.yaml vimkubectl delete pods busyboxget pocreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- echo "Hello World" /bin/sh "ls; sleep 3600" > multe-container.yaml -c
[root@master01 ~]# vim multe-container.yaml 
"multe-container.yaml" 19L, 292C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      12,8All~@k   3~@k   2o -- INSERT --13,7Top13,7All65-6- 7e8c9h10o12"
    - ls; sleep 
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}3"
    - ls; sleep 
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}43H"4e"5l"6l"7o"8 "9P"20P"1^[  13,20All::wq!"multe-container.yaml" 20L, 314C written
[root@master01 ~]# kubectl create -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl create -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yamldelete pods busyboxvim multe-container.yaml kubectl create -f vimkubectl delete pods busyboxlogs 
Hello PP
[root@master01 ~]# kubectl logs  busybox
Hello PP
[root@master01 ~]# kubectl logs  busyboxcreate -f multe-container.yaml vim
"multe-container.yaml" 20L, 314C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      13,20All~@k   4~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0^[  ^[  i -- INSERT --14,10All sleep 3609 sleep 360015,6Allsleep 3605- sleep 36006 sleep 360078sleep 3607^[  15,6All::wq!"multe-container.yaml" 21L, 319C written
[root@master01 ~]# vim multe-container.yaml kubectl logs  busyboxcreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yamldelete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxvim multe-container.yaml kubectl logs  busyboxcreate -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubec`tl logs pods ubusybox
Error from server (NotFound): pods "pods" not found
[root@master01 ~]# kubectl logs pods busyboxcreate -f multe-container.yaml delete pods busyboxvim multe-container.yaml kubectl logs  busyboxget busyboxpods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          27s
[root@master01 ~]# kubectl get pods  busyboxlogs pods
Error from server (NotFound): pods "pods" not found
[root@master01 ~]# kubectl logs pods busyboxo busybox
Error from server (NotFound): pods "po" not found
[root@master01 ~]# kubectl logs po busyboxds busyboxget pods logs podscreate -f multe-container.yaml delete pods busyboxvim multe-container.yaml kubectl delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxlogs pods busyboxget pods logs podscreate -f multe-container.yaml delete pods busyboxvim multe-container.yaml 
"multe-container.yaml" 21L, 319C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"
    - ls
    - sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      15,6Alli -- INSERT --15,6All sleep 3605sleep 3604sleep 3603sleep 3602sleep 3601
   - ls sleep 3600~                                                                                                                                                                                      14,9All; sleep 360010^[  14,9All::wq!"multe-container.yaml" 20L, 314C written
[root@master01 ~]# vim multe-container.yaml kubectl delete pods busyboxlogs pods busyboxget pods logs podscreate -f multe-container.yaml delete pods busyboxvim multe-container.yaml kubectl logs  busyboxcreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yaml";ls
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c  "sleep 3600;ls" > multe-container.yamlvim multe-container.yaml 
"multe-container.yaml" 19L, 291C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600;ls
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      14,9All~@k   3~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8i -- INSERT --13,18All ls9^[  13,18All~@k   2,8 ~@k   3,18o -- INSERT --14,7Top14,7All65-6- 7bin/sh -c1765432109 87e8c9h10o12"
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}3"
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}4    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}3    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}2H3e4l5l6o765432"
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}3H4e5l6l7o8 9W20o1r2l3d4"
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}5^[  14,24All::wq!"multe-container.yaml" 20L, 317C written
[root@master01 ~]# vim multe-container.yaml kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c  "sleep 3600;ls" > multe-container.yamlvim multe-container.yaml kubectl delete pods busyboxlogs pods busyboxget pods logs podscreate -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl get po
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          4s
nginx     1/1     Running   0          66m
[root@master01 ~]# kubectl get pocreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c  "sleep 3600;ls" > multe-container.yamlvim multe-container.yaml kubectl delete pods busyboxlogs pods busybox busybox
Error from server (NotFound): pods "po" not found
[root@master01 ~]# kubectl logs po busyboxget po
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          18s
nginx     1/1     Running   0          67m
[root@master01 ~]# kubectl get pologs po busyboxbusybox
[root@master01 ~]# kubectl logs busybox
[root@master01 ~]# kubectl logs busybox
[root@master01 ~]# kubectl logs busyboxget pologs po busyboxget pocreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c  "sleep 3600;ls" > multe-container.yamlvim multe-container.yaml kubectl delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pods busyboxlogget pologs po busyboxget pocreate -f multe-container.yaml vim
"multe-container.yaml" 20L, 317C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    - echo "Hello World"
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      14,24Alldd  
~                                                                                                                                                                                      14,5All~@k   3~@k   2p - echo "Hello World"13,5All~@k   4~@k   6~@k   7i -- INSERT --14,7Alllsleep 3600; ls8ssleep 3600; ls9lsleep 3600; ls10sleep 3600; ls9 ;sleep 3600; ls10 sleep 3600; ls12543236001^[  14,20All::Wq!E492: Not an editor command: Wq!14,20All::wq!"multe-container.yaml" 20L, 317C written
[root@master01 ~]# kubectl delete po busybox 
Error from server (NotFound): pods "busybox" not found
[root@master01 ~]# kubectl get po
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          77m
[root@master01 ~]# kubectl get podelete po busybox vim multe-container.yamlkubectl delete pods busyboxlogget pologs po busyboxget pocreate -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl create -f multe-container.yaml get podelete po busybox vim multe-container.yamlkubectl delete pods busyboxlog
Hello World
[root@master01 ~]# kubect1 get podssort-by=.metadata.creationTimestampl sort-by=.metadata.creationTimestamp
Error from server (NotFound): pods "sort-by=.metadata.creationTimestamp" not found
[root@master01 ~]# kubectl get pods sort-by=.metadata.creationTimestamp-sort-by=.metadata.creationTimestamp
error: unknown shorthand flag: '' in -sort-by=.metadata.creationTimestamp
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods -sort-by=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp=.metadata.creationTimestamp-=.metadata.creationTimestamp-=.metadata.creationTimestamps=.metadata.creationTimestampo=.metadata.creationTimestampr=.metadata.creationTimestampt=.metadata.creationTimestamp-=.metadata.creationTimestampy=.metadata.creationTimestamp=.metadata.creationTimestampb=.metadata.creationTimestampy=.metadata.creationTimestamp
NAME      READY   STATUS      RESTARTS   AGE
nginx     1/1     Running     0          147m
busybox   0/1     Completed   0          69m
[root@master01 ~]# [root@master01 ~]# kubectl get pods --sort-by=.metadata.creationTimestampsort-by=.metadata.creationTimestamp
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          70m
nginx     1/1     Running     0          148m
[root@master01 ~]# kubectl get pods pods pods podslogs busybox
Hello World
[root@master01 ~]# kubectl delete busybox
error: the server doesn't have a resource type "busybox"
[root@master01 ~]# kubectl delete busybox busyboxp busyboxo busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete po busyboxbusyboxlogsget pods --sort-by=.metadata.creationTimestampsort-by=.metadata.creationTimestampsort-by=.metadata.creationTimestamplogs busyboxcreate -f multe-container.yaml get podelete po busybox vim multe-container.yamlkubectl delete pods busyboxvim multe-container.yaml 
"multe-container.yaml" 20L, 317C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      14,20All~@k   3~@k   2,8 ~@k   1,12~@k   0,9 ~@k   9,13~@k   8,5 ~@k   7,15~@k   6,16~@k   7,15~@k   8,5 ~@k   9,13~@k   10,9~@k   1,12~@k   2,8 ~@k   3,20~@k   4~@k   5,18~@k   6,17o -- INSERT --17,5Top17,5Alli6m7a8g9e10image:12b3u4s5y6b7o8x918,5Alln6a7m8e9name:101b2u3s4y5b6o7x819,5Alli6m7a8g9e10image:12b3u4s5y6b7o8x920,5Alln6a7m8e9name:101b2u3s4y5b6o7x8^[  20,17All~@k   19~@k   8~@k   7~@k   6~@k   ~@k   ~@k   7~@k   8i -- INSERT --18,17All8^[  18,17All::wq!"multe-container.yaml" 24L, 391C written
[root@master01 ~]# kubectl create -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          5s
nginx     1/1     Running     0          152m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          7s
nginx     1/1     Running     0          152m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml vim
"multe-container.yaml" 24L, 391C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 3600
    image: busybox
    name: busybox
    image: busybox
    name: busybox
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      18,17All~@k   7dd  
~                                                                                                                                                                                      17,5Alldd  
~                                                                                                                                                                                      17,5Alldd  
~                                                                                                                                                                                      17,5Alldd  
~                                                                                                                                                                                      17,5All~@k   6~@k   5~@k   6~@k   5~@k   6~@k   7~@k   6~@k   52yy   2~@k    3~@k   2~@k   1~@k   0p image: busybox
    name: busybox11,5All~@k   2~@k   3~@k   2~@k   1~@k   2dd  
~                                                                                                                                                                                      12,5All~@k   1~@k   0p name: busybox11,5All~@k   0dd  
~                                                                                                                                                                                      10,5All~@k   1p - args:12,3All~@k   1~@k   0~@k   4i -- INSERT --10,4Allname: busybox3- name: busybox4123456785^[  18,4All~@k   5dd  
~                                                                                                                                                                                      18,5All~@k   7~@k   6p name: busybox17,5Alli -- INSERT --17,5Allname: busybox4name: busybox3-name: busybox4- name: busybox58^[  18,4All~@k   72yy   2~@k    9~@k   8p - name: busybox
    image: busybox19,3All~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --17,17All81989,1829^[  19,18All::wq!"multe-container.yaml" 24L, 393C written
[root@master01 ~]# vim multe-container.yaml kubectl get podscreate -f multe-container.yaml 
The Pod "busybox" is invalid: 
* spec.containers[1].name: Required value
* spec.containers[1].image: Required value
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 24L, 393C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - name: busybox
    image: busybox
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 3600
  - name: busybox1
    image: busybox
  - name: busybox2
    image: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      19,18All~@k   8~@k   7dd  
~                                                                                                                                                                                      17,5Alldd  
~                                                                                                                                                                                      17,3Alldd  
~                                                                                                                                                                                      17,5Alldd  
~                                                                                                                                                                                      17,5Alldd  
~                                                                                                                                                                                      17,3Alldd  
~                                                                                                                                                                                      17,3All~@k   6o -- INSERT --17,7Top17,7All654321- name: key-value-storeimage: redis(paste) --ports:- containerPort: 6379
    - name: frontendimage: djangoports:- containerPcontainerPort: 800025,30All4,133,202,211,300,1319,1920,131,30^[  21,29All~@k   0,12dd  
~                                                                                                                                                                                      20,9Alldd  
~                                                                                                                                                                                      20,5All~@k   1~@k   2dd  
~                                                                                                                                                                                      22,9Alldd  
~                                                                                                                                                                                      22,3All~@k   1~@k   0~@k   19~@k   8~@k   7,0-1~@k   6,3  ~@k   5~@k   4~@k   3~@k   4~@k   5~@k   6~@k   7,0-1~@k   8,3  x dl   - name: key-value-storex dl  - name: key-value-store~@k   9~@k   20x dl   - name: frontendx dl  - name: frontend~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   19~@k   2~@k   1~@k   0~@k   9 ~@k   8~@k   7~@k   6~@k   5x dl   image: redisx dl  image: redis~@k   20~@k   1x dl   image: djangox dl  image: django~@k   0~@k   19~@k   8~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5i -- INSERT --18,25All65432101987654321busybox8199,1765432busybox92087654321busybox8291,18765432busybox9^[  21,18All::wq!"multe-container.yaml" 23L, 350C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
The Pod "busybox" is invalid: 
* spec.containers[1].name: Required value
* spec.containers[1].image: Required value
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 23L, 350C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - name: busybox
    image: busybox
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 3600

  - name: busybox1
    image: busybox
  - name: busybox2
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      21,18All~@k   0~@k   19~@k   8~@k   7,0-1~@k   6,18 ~@k   5~@k   4,8 ~@k   3,12~@k   2,9 ~@k   1,18~@k   0,17~@k   9,13 ~@k   8,5 ~@k   7,15~@k   6,16~@k   5,9 ~@k   6,16dd  
~                                                                                                                                                                                      6,3All~@k   5~@k   4dd  
~                                                                                                                                                                                      4,3Alldd  
~                                                                                                                                                                                      4,3All::wq!"multe-container.yaml" 20L, 297C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
The Pod "busybox" is invalid: 
* spec.containers[1].name: Required value
* spec.containers[1].image: Required value
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 20L, 297C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox
    image: busybox
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 3600

  - name: busybox1
    image: busybox
  - name: busybox2
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      4,3All~@k   5~@k   6~@k   7~@k   8~@k   4~@k   7i -- INSERT --7,4All name: busybox3  name: busybox4  name: busybox5name: busybox48910,41234,15,4 name: busybox13  name: busybox1467 name: busybox23  name: busybox2456^[  17,5All::wq!"multe-container.yaml" 20L, 297C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
error: error parsing multe-container.yaml: error converting YAML to JSON: yaml: line 8: did not find expected key
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 20L, 297C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    name: busybox
    image: busybox
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 3600name: busybox1
    image: busybox
    name: busybox2
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      17,5All::88,5All~@k   9~@k   10,5~@k   1~@k   2~@k   3~@k   2~@k   1~@k   0~@k   9,5 55y~@k9 lines yanked1,5All~@k   ~@k   2~@k   3~@k   4~@k   5~@k   6pmore linesapiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    name: busybox
    image: busybox
  - args:7,1All9 more lines7,1All^[  ^[  ufewer lines; before #1  4 seconds ago~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      6,5All~@k   7~@k   8~@k   9~@k   10,5~@k   1~@k   2~@k   355y5~@k13 lines yanked1,5All~@k   2~@k   3~@k   4~@k   5~@k   6pmore linesapiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    name: busybox
    image: busybox
  - args:
    - bin/sh
    - -c
    - echo "Hello World"
    - ls; sleep 36007,1Top13 more lines7,1Top^[  ^[  ufewer lines; before #2  3 seconds agorestartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      6,5All~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   dd  
~                                                                                                                                                                                      1,1Alldd  
~                                                                                                                                                                                      1,1Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,1Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,0-1Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,1Alldd  --No lines in buffer--0,0-1Alldd  dd  dd  dd  dd  dd  dd  dii   i -- INSERT --0,1AllapiVersion: v1kind: Podmetadata:  name: redis-django  labels:    app: webspec:  containers:    - name: key-value-store
      image: redis      ports:        - containerPort: 6379
    - name: frontend      image: django      ports:
        - containerPort: 800016,30^[  16,29All~@k   5,12dd  
~                                                                                                                                                                                      15,9All~@k   4~@k   5dd  
~                                                                                                                                                                                      4,7~@k   3~@k   2dd  
~                                                                                                                                                                                      12,5All~@k   1dd  
~                                                                                                                                                                                      11,5All~@k   0~@k   9,5 ~@k   8~@k   7~@k   6~@k   5dd  
~                                                                                                                                                                                      5,5Alldd  
~                                                                                                                                                                                      5,1All~@k   4~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --4,20All10198765432109 b10u1s2y3b4o5x65,6 6,147,16287654321019876543b4u5s6y7b8o9x208,1987654b5u6s7y8b9o20x19019876543b4u5s6y7b8o9x2010,201987654b5u6s7y8b9o20x1^[  10,20All::Wq!E492: Not an editor command: Wq!10,20All::Wq!wq!"multe-container.yaml" 10L, 153C written
[root@master01 ~]# vim multe-container.yaml kubectl create -fvimkubectl create -f
The Pod "busybox" is invalid: spec.containers[1].name: Duplicate value: "busybox"
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 10L, 153C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    - name: busyboximage: busybox
    - name: busyboximage: busybox
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      10,20All~@k   9,19 ~@k   8,20~@k   7,19i -- INSERT --7,19All201189,202110,21^[  10,20All2yy   2p      app: web    app: web1,5 dd  
~                                                                                                                                                                                      11,5Alldd  
~                                                                                                                                                                                      0,7~@k   ~@k   9,7 yy  ~@k   10,7p     - name: busybox21,5~@k   0~@k   6yy  ~@k   1p       image: busybox2,7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   ~@k   ~@k   ~@k   ~@k   1i -- INSERT --11,20All1031^[  11,20All::wq!"multe-container.yaml" 12L, 197C written
[root@master01 ~]# vim multe-container.yaml 
"multe-container.yaml" 12L, 197C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    - name: busybox1image: busybox
    - name: busybox2image: busybox
    - name: busybox3image: busybox
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      11,20All::q[root@master01 ~]# vim multe-container.yaml kubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl get podscreate -f multe-container.yaml vimkubectl delete po busyboxbusyboxlogsget pods --sort-by=.metadata.creationTimestampsort-by=.metadata.creationTimestampsort-by=.metadata.creationTimestamplogs busyboxcreate -f multe-container.yaml get podelete po busybox vim multe-container.yamlkubectl delete pods busyboxlogget pologs po busyboxget pocreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c  "sleep 3600;ls" > multe-container.yamlvim multe-container.yaml kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c  "sleep 3600;ls" > multe-container.yamlls;  echo ""Hello World
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yamlvim multe-container.yaml kubectl create -fvim
"multe-container.yaml" 19L, 292C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      11,12All~@k   2,8 ~@k   3,12~@k   4~@k   3~@k   2,8 o -- INSERT --13,7Top13,7All65-6- 7e8c9h10o12"
    - ls; sleep 
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}3"
    - ls; sleep 
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}43H"4e"5l"6l"7o"8 "9n"20e"1w"2 "3p"4o"5d"64,215,196,1817,5All- name: busybox1image: busybox
    - name: busybox(paste) --2image: busybox
    - name: busybox3image: busybox
23,1All210198765^[  15,1Alldd  
~                                                                                                                                                                                      15,5Alldd  
~                                                                                                                                                                                      15,9Alli -- INSERT --15,9All- name: busybox18- name: busybox17- name: busybox16- name: busybox15^[  15,4All~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1,0-1^[  ^[  ~@k   2,4  dd  
~                                                                                                                                                                                      22,3Alldd  
~                                                                                                                                                                                      22,3All~@k   1,0-1dd  
~                                                                                                                                                                                      21,3All~@k   2::wq!"multe-container.yaml" 22L, 364C written
[root@master01 ~]# vim multe-container.yaml kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yamlvim multe-container.yaml kubectl create -f
Error from server (BadRequest): error when creating "multe-container.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field Container.spec.containers.args of type string
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 22L, 364C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello new pod"
    - ls; sleep 3600
    - name: busybox1image: busybox
    - name: busybox2image: busybox
    - name: busybox3image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      22,3All~@K   1~@k   ~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,1 ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --1,1AllapiVersion: v12,1All^[  2,1All::Wq!wq!"multe-container.yaml" 23L, 365C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
Error from server (BadRequest): error when creating "multe-container.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field Container.spec.containers.args of type string
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 23L, 365C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello new pod"
    - ls; sleep 3600
    - name: busybox1image: busybox
    - name: busybox2image: busybox
    - name: busybox3image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      2,1All~@k   3~@k   4~@k   5~@k   6~@k   5dd  
~                                                                                                                                                                                      5,3Alldd  
~                                                                                                                                                                                      5,5Alldd  
~                                                                                                                                                                                      5,3All^[  ^[  ::wq!"multe-container.yaml" 20L, 312C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
Error from server (BadRequest): error when creating "multe-container.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field Container.spec.containers.args of type string
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 20L, 312C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello new pod"
    - ls; sleep 3600
    - name: busybox1image: busybox
    - name: busybox2image: busybox
    - name: busybox3image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      5,3All~@k   6~@k   7~@k   8~@k   9~@k   10,3~@k   1~@k   2~@k   1~@k   0~@k   9,3 ~@k   10,3~@k   1~@k   2~@k   3~@k   4~@k   3x dl   - name: busybox1x dl  - name: busybox1~@k   4x dl     image: busyboxx dl    image: busybox~@k   5x dl   - name: busybox2x dl  - name: busybox2~@k   6x dl     image: busyboxx dl    image: busybox~@k   7x dl   - name: busybox3x dl  - name: busybox3~@k   8x dl     image: busyboxx dl    image: busybox^[  ^[  ::wq!"multe-container.yaml" 20L, 300C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
The Pod "busybox" is invalid: 
* spec.containers[0].name: Required value
* spec.containers[0].image: Required value
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 20L, 300C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello new pod"
    - ls; sleep 3600
  - name: busybox1
    image: busybox
  - name: busybox2
    image: busybox
  - name: busybox3
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      18,3All~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,3 ~@k   8~@k   9~@k   10,3~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   ~@k   ~@k   ~@k   ~@k   dd  
~                                                                                                                                                                                      19dd  
~                                                                                                                                                                                      8,5::Wq!E492: Not an editor command: Wq!18,5All::wq!"multe-container.yaml" 18L, 266C written
[root@master01 ~]# vim multe-container.yaml vim multe-container.yaml kubectl create -f
The Pod "busybox" is invalid: 
* spec.containers[0].name: Required value
* spec.containers[0].image: Required value
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 18L, 266C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello new pod"
    - ls; sleep 3600
  - name: busybox1
    image: busybox
  - name: busybox2
    image: busybox
  - name: busybox3
    image: busybox
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      18,5All~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,5 ~@k   10,5~@k   9,5 ~@k   8dd  
~                                                                                                                                                                                      8,5Alldd  
~                                                                                                                                                                                      8,5Alldd  
~                                                                                                                                                                                      8,5Alldd  
~                                                                                                                                                                                      8,5Alldd  
~                                                                                                                                                                                      8,3All~@k   9o -- INSERT --10,5Top10,5All4321- args:
    - bin/sh
    - -c
    - echo "Hello new pod"(paste) --- ls; sleep 3600--a15,1All43210   - args:2   - args:31   - bin/sh4   - bin/sh52 - -c6 - -c7365 - echo "Hello new pod"6 - echo "Hello new pod"7465 - ls; sleep 36006 - ls; sleep 360075,1^[  15,0-1Alldd  
~                                                                                                                                                                                      15,3All::wq!"multe-container.yaml" 18L, 276C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
error: error parsing multe-container.yaml: error converting YAML to JSON: yaml: line 9: did not find expected key
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 18L, 276C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox1
    image: busybox
    - args:- bin/sh- -c- echo "Hello new pod"- ls; sleep 3600
  - name: busybox2
    image: busybox
  - name: busybox3
    image: busybox
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      15,3All::99,5Alli -- INSERT --9,5All image: busybox610,61234565 image: busybox675485 image: busybox6^[  18,5All::wq!"multe-container.yaml" 18L, 279C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
error: error parsing multe-container.yaml: error converting YAML to JSON: yaml: line 9: mapping values are not allowed in this context
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 18L, 279C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox1image: busybox
    - args:- bin/sh- -c- echo "Hello new pod"- ls; sleep 3600
  - name: busybox2image: busybox
  - name: busybox3image: busybox
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      18,5All~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,5 ~@k   8~@k   9x dl  image: busybox~@k   8i -- INSERT --8,5All-name: busybox14name: busybox13 name: busybox14 name: busybox15910,512345-name: busybox24name: busybox23 name: busybox24 name: busybox2566image: busybox57-name: busybox34name: busybox33 name: busybox34 name: busybox3586image: busybox5^[  18,4All::wq!"multe-container.yaml" 18L, 276C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
error: error parsing multe-container.yaml: error converting YAML to JSON: yaml: line 9: did not find expected key
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 18L, 276C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
    name: busybox1
    image: busybox
    - args:- bin/sh- -c- echo "Hello new pod"- ls; sleep 3600
    name: busybox2
    image: busybox
    name: busybox3
    image: busybox
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      18,4All::q![root@master01 ~]# vim multe-container.yaml kubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yamlvim multe-container.yaml kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "ls; sleep 3600" > multe-container.yamliecho ""Hello PP; 
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "echo "Hello PP"; ls; sleep 3600" > multe-container.yamlvim multe-container.yaml 
"multe-container.yaml" 20L, 313C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo Hello
    - PP; ls; sleep 3600
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      18,4All~@k   7~@k   6~@k   5~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1i -- INSERT --14,11Allls; sleep 3600ls; sleep 3609 ls; sleep 3608ls; sleep 36073891012iHello3Hello2"Hello
    - ls; sleep 
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}345678 9p20p1"
    - ls; sleep 
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}2^[  13,21All~@k   4,20~@k   5,18~@k   6,17o -- INSERT --17,5Top17,5All4321image: busybox
    name: busybox18,18All791206,187,20192206,185,19120432,9 1,130,109,14 8,6 7,16-7386,17-839^[  6,18All::wq!"multe-container.yaml" 22L, 357C written
[root@master01 ~]# vim multe-container.yaml kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "echo "Hello PP"; ls; sleep 3600" > multe-container.yamlvim multe-container.yaml kubectl create -f
pod/busybox-3 created
[root@master01 ~]# kibekubectl get pods
NAME        READY   STATUS         RESTARTS   AGE
busybox     0/1     Completed      0          18m
busybox-3   0/1     ErrImagePull   0          7s
nginx       1/1     Running        0          171m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml vim
"multe-container.yaml" 22L, 357C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox-3
  name: busybox-3
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello pp"
    - ls; sleep 3600
    image: busybox1
    name: busybox
    image: busybox2
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      6,18Alli -- INSERT --6,18All9^[  6,18All::q![root@master01 ~]# vim multe-container.yaml kubectl get podscreate -f multe-container.yaml delete
pod "busybox-3" deleted
[root@master01 ~]# kubectl delete -f multe-container.yaml vim
"multe-container.yaml" 22L, 357C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox-3
  name: busybox-3
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello pp"
    - ls; sleep 3600
    image: busybox1
    name: busybox
    image: busybox2
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      6,18All~@k   7,17~@k   8,5 ~@k   9,13~@k   10,9~@k   1,12~@k   2,8 ~@k   3,18~@k   4~@k   5i -- INSERT --15,18All920196,1819720198,1829^[  18,18All~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2,8 ~@k   1,12~@k   0,9 ~@k   9,13~@k   8,5 ~@k   7,17~@k   6,18~@k   i -- INSERT --6,18All^[  6,17All~@k   7~@k   8,5 ~@k   9,13~@k   10,9~@k   1,12~@k   2,8 ~@k   3,17~@k   4~@k   5~@k   6~@k   7yy  2p  image: busybox
    image: busybox18,5All^[  ^[  u2 fewer lines; before #5  3 seconds ago~                                                                                                                                                                                      ~                                                                                                                                                                                      17,17All~@k   8~@k   7yy  ~@k   8p image: busybox19,5All~@k   8yy  ~@k   9p name: busybox220,5All~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   ~@k   ~@k   ~@k   R -- REPLACE --20,18All39^[  20,18All::wq!"multe-container.yaml" 24L, 395C written
[root@master01 ~]# vim multe-container.yaml kubectl delete -fvimkubectl get podsvim multe-container.yaml kubectl delete -fvimkubectl get podscreate -f multe-container.yaml vimkubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "echo "Hello PP"; ls; sleep 3600" > multe-container.yamlvim multe-container.yaml kubectl create -f
pod/busybox-3 created
[root@master01 ~]# kubectl get pods
NAME        READY   STATUS      RESTARTS   AGE
busybox     0/1     Completed   0          20m
busybox-3   0/1     Completed   0          5s
nginx       1/1     Running     0          172m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml vim
"multe-container.yaml" 24L, 395C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox-3
  name: busybox-3
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello pp"
    - ls; sleep 3600
    image: busybox
    name: busybox1
    image: busybox
    name: busybox2
    image: busybox
    name: busybox3
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      20,18All~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2,8 ~@k   1,12~@k   0,9 ~@k   9,13~@k   8,5 ~@k   7,17~@k   6,18~@k   5,9 ~@k   4,18dd  
~                                                                                                                                                                                      4,3Alldd  
~                                                                                                                                                                                      4,5Alldd  
~                                                                                                                                                                                      4,3All~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --4,17All876^[  4,15All::wq!"multe-container.yaml" 21L, 338C written
[root@master01 ~]# kubectl delete po busybox busybox-3
pod "busybox" deleted
pod "busybox-3" deleted
[root@master01 ~]# kubectl delete po busybox busybox-3vim multe-container.yaml kubectl get podscreate -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kuebectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          7s
nginx     1/1     Running     0          173m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          12s
nginx     1/1     Running     0          173m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml delete po busybox busybox-3vim multe-container.yaml 
"multe-container.yaml" 21L, 338C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello pp"
    - ls; sleep 3600
    image: busybox
    name: busybox1
    image: busybox
    name: busybox2
    image: busybox
    name: busybox3
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      4,15All~@k   3,9 ~@k   2~@k   1,14~@k   dd  
~                                                                                                                                                                                      1,1Alldd  
~                                                                                                                                                                                      1,1Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,1Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,5Alldd  
~                                                                                                                                                                                      1,3Alldd  
~                                                                                                                                                                                      1,3Alli -- INSERT --1,3AllrestartPolicy: Never2,1All1^[  1,1Allu1 line less; before #20  3 seconds ago
  restartPolicy: Never~                                                                                                                                                                                      1,3Alli -- INSERT --1,3All21restartPolicy: Never2,1All^[  2,1Allu1 line less; before #21  2 seconds ago
  restartPolicy: Never~                                                                                                                                                                                      1,1Allu1 more line; before #19  14 seconds agodnsPolicy: ClusterFirst1,3Alli -- INSERT --1,3AlldnsPolicy: ClusterFirst2,1AlldnsPolicy: ClusterFirst3,1All2123dnsPolicy: ClusterFirst2dnsPolicy: ClusterFirst3^[  3,2Alldd  
~                                                                                                                                                                                      3,3All~@k   2,0-1~@k   1,2  ~@k   i -- INSERT --1,2All1apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  co(paste) --containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: logs-volmountPath: /var/log/nginx
  - name: sidecar
 image: busybox
    command: ["sh","-c","while true; do if [ \"  

  restartPolicy: Never
status: {}$(cat /var/log/nginx/error.log \
              | grep 'error')\"  


status: {}              | grep )\" != \"\" ]; then echo 'Error discovered!'; fi; \s

  restartPolicy: Neverleep 10; done"]16,31All^[  16,30All~@k   7,0-1dd  
~                                                                                                                                                                                      17,3All~@k   6~@k   5~@k   4~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   [
              | grep )\" != \"\" ]; then echo ; fi; \
              sleep 10; done"]4~@k   []5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   6~@k   5x dl  while true; do if [ \"$(cat /var/log/nginx/error.log \              | grep )\" != \"\" ]; then echo ; fi; \
              sleep 10; done"x dl  hile true; do if [ \"$(cat /var/log/nginx/error.log \x dl  ile true; do if [ \"$(cat /var/log/nginx/error.log \x dl  le true; do if [ \"$(cat /var/log/nginx/error.log \x dl  e true; do if [ \"$(cat /var/log/nginx/error.log \x dl   true; do if [ \"$(cat /var/log/nginx/error.log \x dl  true; do if [ \"$(cat /var/log/nginx/error.log \x dl  rue; do if [ \"$(cat /var/log/nginx/error.log \x dl  ue; do if [ \"$(cat /var/log/nginx/error.log \x dl  e; do if [ \"$(cat /var/log/nginx/error.log \x dl  ; do if [ \"$(cat /var/log/nginx/error.log \x dl   do if [ \"$(cat /var/log/nginx/error.log \x dl  do if [ \"$(cat /var/log/nginx/error.log \x dl  o if [ \"$(cat /var/log/nginx/error.log \x dl   if [ \"$(cat /var/log/nginx/error.log \x dl  if [ \"$(cat /var/log/nginx/error.log \x dl  f [ \"$(cat /var/log/nginx/error.log \x dl   [ \"$(cat /var/log/nginx/error.log \x dl  [ \"$(cat /var/log/nginx/error.log \]x dl   \"$(cat /var/log/nginx/error.log \]]x dl  \"$(cat /var/log/nginx/error.log \x dl  "$(cat /var/log/nginx/error.log \              | grep )\" != \"\" ]; then echo ; fi; \x dl  $(cat /var/log/nginx/error.log \              | grep )\" != \"\" ]; then echo ; fi; \x dl  (cat /var/log/nginx/error.log \)x dl  cat /var/log/nginx/error.log \)x dl  at /var/log/nginx/error.log \x dl  t /var/log/nginx/error.log \x dl   /var/log/nginx/error.log \x dl  /var/log/nginx/error.log \x dl  var/log/nginx/error.log \x dl  ar/log/nginx/error.log \x dl  r/log/nginx/error.log \x dl  /log/nginx/error.log \x dl  log/nginx/error.log \x dl  og/nginx/error.log \x dl  g/nginx/error.log \x dl  /nginx/error.log \x dl  nginx/error.log \x dl  ginx/error.log \x dl  inx/error.log \x dl  nx/error.log \x dl  x/error.log \x dl  /error.log \x dl  error.log \x dl  rror.log \x dl  ror.log \x dl  or.log \x dl  r.log \x dl  .log \x dl  log \x dl  og \x dl  g \x dl   \x dl  \x dl  4x dl  3x dl                | grep )\" != \"\" ]; then echo ; fi; \2x dl                sleep 10; done"]1x dl  0x dl                | grep )\" != \"\" ]; then echo ; fi; \
              sleep 10; done"]19x dl  8x dl                | grep )\" != \"\" ]; then echo ; fi; \7x dl                sleep 10; done"]6x dl  5x dl  [              | grep )\" != \"\" ]; then echo ; fi; \
              sleep 10; done"]4x dl  != \"\" ]3x dl  2x dl      command1x dl  0x dl  9 x dl  8x dl  7x dl  6x dl  5x dl  4x dl  3x dl  2x dl  1x dl  0-1x dl  x dl  x dl  x dl  x dl  x dl  x dl  ^[  ^[  u1 change; before #112  2 seconds ago14,0-1Allu114,0-1Allu014,0-1All^[  ^[  u09  414,0-1Allu814,0-1Allu714,0-1Allu6  514,0-1Allu514,1Allu414,2Allu314,3Allu2  614,4Allu1c14,5Allu0o14,6Allu99  6 seconds agom14,7Allu8  7m14,8Allu7a14,9Allu6n14,10Allu5d14,11Allu4    command:14,12Allu314,13Allu2  8[!= \"\" ]14,14Allu1  9["
              | grep )\" != \"\" ]; then echo ; fi; \
              sleep 10; done"]14,15All^[  ^[  ~@k   5~@k   6~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   30~@k   29~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9 ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9i -- INSERT --16,29All              sleep 10; don"] 8"] 7"] 6"]  
  restartPolicy: Never
status: {}5"]    restartPolicy: Never
status: {}4"] 3"] 2"]  
  restartPolicy: Never
status: {}1"]    restartPolicy: Never
status: {}0"] 19"] 8"] 7"] 6"]  
  restartPolicy: Never
status: {}5"] 4"] 3"] 2"] 1"] 0"] 9 "] 8"] 7"] 6"] 5"] 4"] 3"] 2"] 1
    | grep 'error')\" != \"\" ]; then echo 'Error discovered!'; fi; \"]
  restartPolicy: Never
status: {}~                                                                                                                                                                                      15,80All"]  
  restartPolicy: Never
status: {}79"]    restartPolicy: Never
status: {}8"] 7"] 6"]  
  restartPolicy: Never
status: {}5"]    restartPolicy: Never
status: {}4"]  
  restartPolicy: Never
status: {}3"] 2"] 1"] 0"] 69"] 8"] 7"] 6"] 5"] 4"] 3"] 2"] 1"] 0"] 59"] 8"] 7"] 6"] 5"] 4"]    restartPolicy: Never
status: {}3"] 2"] 1"] 0"]  
  restartPolicy: Never
status: {}49"]    restartPolicy: Never
status: {}8"] 7"] 6"] 5"]  
  restartPolicy: Never
status: {}4"]    restartPolicy: Never
status: {}3"]  
  restartPolicy: Never
status: {}[
              | grep )\" != \"\" ]]  
  restartPolicy: Never
status: {}2"] ["1"] 0]  
  restartPolicy: Never
status: {}39"]    restartPolicy: Never
status: {}8]  
  restartPolicy: Never
status: {}7"]    restartPolicy: Never
status: {}6"] 5"] 4"] 3"] 2]  
  restartPolicy: Never
status: {}1"]    restartPolicy: Never
status: {}0"] 29"] 8"] 7"] 6"] 5"] 4"] 3"] 2"] 1"] 0"] 19"] 8"] 7"]  
  restartPolicy: Never
status: {}87658/"] 
  restartPolicy: Never
status: {}9b"]20i"]1n"]2s"]3"] 2/"]3s"]4h"]5[]6,],]7 ] ]8-]-]9c]c]30[]298"-c]  
  restartPolicy: Never
status: {}9301"] 
  restartPolicy: Never
status: {}[]2,],]3 ] ]4"]  
  restartPolicy: Never
status: {}["5e]  6c]  7h]  8o]  9 ]  40']  1']  21H']  2e']  3l']  4l']  5o']  6 ']  7M']  8e']  96,23253456789301234567894012345678950;]  1 ]  2l]  3s]  4;]  5 ]  6s]  7l]  8] 7] 6s]  7l]  8e]  9e]  60p]  1 ]  23]  36]  40]  50]  67876"] 
  restartPolicy: Never
status: {}[]78^[  15,67All~@k   []4,15~@k   3,18~@k   2,17i -- INSERT --12,17All87654321b2u3s4y5b6o7x819109,18 10,199,18 8,179,18^[  9,17Alldd  
~                                                                                                                                                                                      9,5Alldd  
~                                                                                                                                                                                      9,7Alldd  
~                                                                                                                                                                                      9,3All~@k   8~@k   7~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --7,15All654321b2u3s4y5b6o7x8298,1765432b3u4s5y6b7o8x976,145,6 4,18765432109 busy box17654box3^[  4,12All~@k   5,5 ~@k   6,12~@k   7~@k   8~@k   9~@k   10,12~@k   1~@k   2o -- INSERT --13,15Top13,15All432109 87654321- name: busybox1
    image: busybox15,1All
image: busybox~                                                                                                                                                                                      14,19All3839^[  13,18All::wq!"multe-container.yaml" 16L, 307C written
[root@master01 ~]# vim multe-container.yaml kubectl get podscreate -f multe-container.yaml delete po busybox busybox-3create -f multe-container.yaml 
error: error parsing multe-container.yaml: error converting YAML to JSON: yaml: line 11: did not find expected ',' or ']'
[root@master01 ~]# kubectl create -f multe-container.yaml vim
"multe-container.yaml" 16L, 307C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox2
    image: busybox
  - name: busybox1
    image: busybox
    command: ["
              | "/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"]
  - name: busybox3
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      13,18All::1111,5All~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   []4~@k   []5~@k   ~@k   ~@k   2i -- INSERT --12,15All67"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 6"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 5"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 4"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 3"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 2"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 1"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 0"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 9 "/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 8"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 7"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 6"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 5"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 4"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 3"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 2"/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] 1
    command: ["/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"]~                                                                                                                                                                                      11,16All/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"] []5[]678920123456^[  11,25All::Wq!E492: Not an editor command: Wq!11,25All::wq!"multe-container.yaml" 15L, 289C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
Error from server (AlreadyExists): error when creating "multe-container.yaml": pods "busybox" already exists
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          6m28s
nginx     1/1     Running     0          179m
[root@master01 ~]# kubectl delete pods busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete pod busyboxget podscreate -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl create -f multe-container.yaml delete pod busyboxget pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          2s
nginx     1/1     Running             0          3h
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          8s
nginx     1/1     Running             0          3h
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          10s
nginx     1/1     Running    0          3h
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          12s
nginx     1/1     Running    0          3h
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          14s
nginx     1/1     Running    0          3h
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml delete pod busyboxget podscreate -f multe-container.yaml vim
"multe-container.yaml" 15L, 289C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox2
    image: busybox
  - name: busybox1
    image: busybox
    command: ["/bin/sh", "-c", "echo 'Hello Me'; ls; sleep 3600"]
  - name: busybox3
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      11,25All~@k   6~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9i -- INSERT --11,39AllHello Me'; ls; sleep 3600"] 8"Hello Me'; ls; sleep 3600"]94012345678; ls; sleep 3600"] 7"; ls; sleep 3600"]89501234567898765432104987654321039876543echo "Hello Me"; ls; sleep 3600"] 23456789401234567895012345678960123[]43600] ] 3[]2105987654321049876543210398765432iecho "Hello Me"; ls; sleep 3600]3"echo "Hello Me"; ls; sleep 3600]4echo "Hello Me"; ls; sleep 360] 3echo "Hello Me"; ls; sleep 360] 2"echo "Hello Me"; ls; sleep 3600]3676[]543600"]"]5^[  []11,64All::Wq!wq!"multe-container.yaml" 15L, 289C written
[root@master01 ~]# vim multe-container.yaml kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          98s
nginx     1/1     Running    0          3h1m
[root@master01 ~]# kubectl get podsvim multe-container.yaml kubectl get podscreate -f multe-container.yaml delete pod busybox
pod "busybox" deleted
^[[A    [root@master01 ~]# kubectl delete pod busyboxget podsvim multe-container.yaml kubectl get podscreate -f multe-container.yaml 
error: error parsing multe-container.yaml: error converting YAML to JSON: yaml: line 10: did not find expected ',' or ']'
[root@master01 ~]# kubectl create -f multe-container.yaml delete pod busyboxget podsvim multe-container.yaml 
"multe-container.yaml" 15L, 289C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox2
    image: busybox
  - name: busybox1
    image: busybox
    command: ["/bin/sh", "-c", "echo "Hello Me"; ls; sleep 3600"]
  - name: busybox3
    image: busybox
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      11,64All::1010,5All~@k   1~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   []4~@k   []5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   40~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   50i -- INSERT --11,50All498; ls; sleep 3600"] 7'; ls; sleep 3600"]87654321039Hello Me'; ls; sleep 3600"] 8'Hello Me'; ls; sleep 3600"]94012345678950123456^[  11,55All::wq!"multe-container.yaml" 15L, 289C written
[root@master01 ~]# vim multe-container.yaml kubectl create -f
pod/busybox created
[root@master01 ~]# kubectl create -f multe-container.yaml vimkubectl create -fdelete pod busyboxget pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          4s
nginx     1/1     Running             0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          6s
nginx     1/1     Running             0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          8s
nginx     1/1     Running             0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          9s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          11s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          12s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          13s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get podskubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          17s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          18s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS     RESTARTS   AGE
busybox   1/3     NotReady   0          20s
nginx     1/1     Running    0          3h3m
[root@master01 ~]# kubectl get podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Thu Sep 15 17:41:45 2022NAME  READY   STATUS     RESTARTS   AGEbusybox   1/3     NotReady   025snginx     1/1     Running    03h4m779951313466882:004022446688105022457799216133557799317133568840802254466885090224466893:01101s3355[root@master01 ~]# watch kubectl get podscreate -f multe-container.yaml vimkubectl create -fdelete pod busybox
pod "busybox" deleted
[root@master01 ~]# flagkubectl run busybox --image=busybox --restart=Never --dry-run -o yaml -- bin/sh -c "sleep 3600; ls" > multe-container.yaml 
W0915 17:43:59.181917  114404 helpers.go:639] --dry-run is deprecated and can be replaced with --dry-run=client.
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never --dry-run -o yaml -- bin/sh -c "sleep 3600; ls" > multe-container.yaml =client
[root@master01 ~]# kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "sleep 3600; ls" > multe-container.yaml delete pod busyboxwatch kubectl get podscreate -f multe-container.yaml vim
"multe-container.yaml" 19L, 292C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      11,12All~@k   2,8 o -- INSERT --13,7Top13,7All65-6- 7e8c9h10o12echo :3 ::4 :3echo 2H3e4l5l6o78w9o20r1l2d3;43"43210198765432"Hello world"3^[  13,12All^[  ^[  dd  
~                                                                                                                                                                                      13,5All~@k   2~@k   1~@k   0~@k   9,5 ~@k   8~@k   7~@k   6~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --6,16All7187,1627617^[  7,16All::wq!"multe-container.yaml" 19L, 294C written
[root@master01 ~]# kuvim multe-container.yaml 
"multe-container.yaml" 19L, 294C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox1
  name: busybox1
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      7,16All~@k   8,5 ~@k   9,13~@k   10,9~@k   1,12~@k   2,8 ~@k   3,16~@k   4~@k   5~@k   {}6~@k   {}7~@k   8~@k   {}9,10~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   o -- INSERT --20,1All{}
1apiVersion: v1
kind: Podmetadata:  creationTimestamp: null
  labels:    run: busybox1  name: busybox1spec:  conta  containers:
  - args:    - bin/sh- -c
    - sleep 3600; ls(paste) -- image: busyboxname: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}--a39,11Bot{}{}8765432,9 1,110,1029,118,6 7,1165,104,113,104,115,106,1123456787287,176278,6 9,1430,101,132,9 3,174895,18294387652,9 33,788%65-6- 7e8c9h10o12"
    - sleep 3600; ls
    image: busybox
    name: busybox2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never3"
    - sleep 3600; ls
    image: busybox
    name: busybox2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never43H"4e"5l"6l"7o"8 "9P"20P"14019^[  34,1888%~@k   
status: {}35,18Bot~@k   6~@k   {}7,17~@k   {}8,18~@k   9~@k   {}40,10~@k   ~@k   ~@k   ~@k   ~@k   ~@k   o 
-- INSERT --41,1Bot
41,1Bot{}

42,1BotapiVersion: v1
kind: Pod(paste) --metadata:
  creationTimestamp: null
  labels:run: busybox1
  name: busybox1
spec:
  containers:- args:
    - bin/sh
    - -c
    - sleep 3600; lsimage: busybox
    name: busybox
    resources: {}dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}--a60,11Bot{}{}59876543,9 2,111,100,1149,6 8,11723456787388,176379,6 50,141,102,133,9 4,175683954201019836007^[  54,16Bot~@k   5~@k   6~@k   {}7~@k   {}8~@k   9~@k   {}60,10~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   {}59,16~@k   8~@k   {}7~@k   {}6~@k   5~@k   4~@k   3,8 ~@k   2,12~@k   1,9 ~@k   0,13~@k   49,5 ~@k   8,16~@k   7~@k   6,9 ~@k   5,16~@k   4,9 ~@k   3~@k   2,14~@k   1,0-1~@k   {}0,10 ~@k   {}39,16~@k   8~@k   {}7~@k   {}6~@k   5~@k   containers:34,1696%~@k   spec:33,1693%~@k   7~@k   8~@k   4i -- INSERT --34,1893%^[  34,1793%dd  
restartPolicy: Never34,596%~@k   3~@k   name: busybox232,592%~@k   3~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --33,2192%23apiVersion: v1(paste) --kind: Pod
metadata:
creationTimestamp: null
labels:
    run: busybox1
  name: busybox1
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    image: busyboxname: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  rerestartPolicy: Never
status: {}--a51,1156%{}^[  51,1056%u18 fewer lines; before #14  2 seconds ago    image: busybox
    namebusybox2
    resources: {}dnsPolicy: ClusterFirst
  restartPolicy: Never
status{}apiVersion: v1
kind: Pod
metadata:creationTimestamp: null
  labels:run: busybox3name: busybox3spec:containers:- args:    - bin/sh
    - -c- sleep 3600image: busybox
    name: busybox3  resources: {}  dnsPolicy: ClusterFirst33,2192%^[  ^[  u more line; before #13  14 seconds ago- sleep 3600;34,589%~@k   4~@k   3~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --33,2189%23- sleep 3600;36543210298765432; - sleep 3600;3456sleep 3600;5sleep 3600;4567893012345360044,19^[  34,1889%dd  
dnsPolicy: ClusterFirst34,592%~@k   5~@k   6~@k   7~@k   8~@k   9~@k   40,0-1~@k   1,5  ~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   50~@k   1~@k   2~@k   
restartPolicy: Never53,596%~@k   
status: {}54,5Bot~@k   5~@k   6~@k   7~@k   8~@k   9~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   49~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0,0-1~@k   39,5  ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   spec:33,596%~@k   name: busybox232,592%~@k   run: busybox231,589%~@k   labels:30,585%~@k   creationTimestamp: null29,582%~@k   metadata:28,578%~@k   kind: Pod27,575%~@k   apiVersion: v126,571%~@k   25,567%~@k   status: {}24,564%~@k   restartPolicy: Never23,560%~@k   dnsPolicy: ClusterFirst22,557%~@k   resources: {}21,553%~@k   name: busybox20,0-150%~@k   image: busybox19,546%~@k   - sleep 3600; ls18,542%~@k   - -c17,539%~@k   - bin/sh16,535%~@k   - args:15,532%~@k   containers:14,528%~@k   spec:13,525%~@k   name: busybox112,521%~@k   run: busybox111,517%~@k   labels:10,514%~@k   creationTimestamp: null9,510%~@k   metadata:8,57%~@k   kind: Pod7,53%~@k   apiVersion: v16,5Top~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ::wq!"multe-container.yaml" 59L, 895C written
[root@master01 ~]# vim multe-container.yaml kubectl run busybox --image=busybox --restart=Never --dry-run=client -o yaml -- bin/sh -c "sleep 3600; ls" >delete pod busyboxwatch kubectl get podscreate -f multe-container.yaml 
pod/busybox3 created
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          4s
nginx      1/1     Running   0          3h11m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          13s
nginx      1/1     Running   0          3h11m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml vim
"multe-container.yaml" 59L, 895C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox1
  name: busybox1
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox2
  name: busybox2
spec:
  containers:
  - args:
    - bin/sh1,5Top~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,5~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20,0-1~@k   1,5  ~@k   2~@k   3~@k   4~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0,0-1~@k   19,5  ~@k   8~@k   7~@k   6~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20,0-1~@k   1,5  ~@k   2~@k   3~@k   4~@k   5~@k   6~@k   
- -c27,53%~@k   
- echo "Hello PP"; sleep 360028,57%~@k   
image: busybox29,510%~@k   
name: busybox230,514%~@k   
resources: {}31,517%~@k   
dnsPolicy: ClusterFirst32,521%~@k   1~@k   0~@k   29~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0,0-1~@k   19,5  ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --15,1721%819^[  15,1821%~@k   {}6,17~@k   {}7,18~@k   8~@k   {}9,10~@k   {}20,0-1~@k   1,14 ~@k   2,9 ~@k   3~@k   4,18~@k   5,9 ~@k   6,17~@k   7,16~@k   8,5 ~@k   9,13~@k   30,9 ~@k   1,12~@k   2,8 ~@k   
restartPolicy: Never33,1825%~@k   
status: {}34,1828%~@k   
35,1832%~@k   
{}apiVersion: v136,1735%~@k   
{}kind: Pod37,1839%~@k   
metadata:38,1842%~@k   
{}creationTimestamp: null39,1046%~@k   
{}labels:40,0-150%~@k   
run: busybox341,1453%~@k   
name: busybox342,957%~@k   
spec:43,960%~@k   
containers:44,1864%~@k   
- args:45,967%~@k   
- bin/sh46,1771%~@k   
- -c47,1675%~@k   
- sleep 360048,578%~@k   
image: busybox49,1382%~@k   
name: busybox350,985%~@k   
resources: {}51,1289%~@k   
dnsPolicy: ClusterFirst52,892%~@k   
restartPolicy: Never53,1696%~@k   
status: {}54,18Bot~@k   5~@k   {}6,17~@k   {}7,18~@k   8~@k   {}9,10~@k   ~@k   ~@k   ~@k   ~@k   ::wq!"multe-container.yaml" 59L, 896C written
[root@master01 ~]# vim multe-container.yaml kubectl get podscreate -f multe-container.yaml edelete
pod "busybox3" deleted
[root@master01 ~]# kubectl delete -f multe-container.yaml vimkubectl get podscreate -f multe-container.yaml 
pod/busybox3 created
[root@master01 ~]# kubectl create -f multe-container.yaml delevimkubectl get pods
NAME       READY   STATUS              RESTARTS   AGE
busybox3   0/1     ContainerCreating   0          2s
nginx      1/1     Running             0          3h13m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          5s
nginx      1/1     Running   0          3h13m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          6s
nginx      1/1     Running   0          3h13m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml delevim
"multe-container.yaml" 59L, 896C  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"; sleep 3600
    image: busybox
    name: busybox2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox3
  name: busybox3
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox3
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}59,10Bot~@k   {}8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2,8 ~@k   1,10~@k   0,9 ~@k   49,10~@k   8,5 ~@k   7,10~@k   6~@k   5,9 ~@k   4,10~@k   3,9 ~@k   2~@k   1,10~@k   0,0-1~@k   {}39,10 ~@k   {}8~@k   7~@k   6~@k   5~@k   4~@k   spec:33,1096%~@k   name: busybox232,892%~@k   run: busybox231,1089%~@k   labels:30,985%~@k   creationTimestamp: null29,1082%~@k   metadata:28,578%~@k   kind: Pod27,1075%~@k   apiVersion: v126,1071%~@k   25,967%~@k   status: {}24,1064%~@k   restartPolicy: Never23,960%~@k   dnsPolicy: ClusterFirst22,957%~@k   resources: {}21,1053%~@k   name: busybox120,0-150%~@k   image: busybox{}19,1046%~@k   - sleep 3600; ls{}18,1042%~@k   - -c17,1039%~@k   - bin/sh16,1035%~@k   - args:15,1032%~@k   containers:14,1028%~@k   spec:13,1025%~@k   name: busybox112,821%~@k   run: busybox111,1017%~@k   labels:10,914%~@k   creationTimestamp: null9,1010%~@k   metadata:8,57%~@k   kind: Pod7,103%~@k   apiVersion: v16,10Top~@k   5,9 ~@k   4,10~@k   3,9 ~@k   2~@k   1,10~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   2,9 ~@k   3~@k   4,10~@k   5,9 ~@k   6,10~@k   7~@k   8,5 ~@k   9,10~@k   10,9~@k   1,10~@k   2,8 ~@k   3,10~@k   4~@k   5~@k   6~@k   7dd  
- -c17,3Topdd  
- echo "Hello PP"; sleep 360017,1Topdd  
image: busybox17,0-1Top~@k   8,1  ~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   
name: busybox227,14%~@k   
resources: {}28,18%~@k   
dnsPolicy: ClusterFirst29,112%~@k   
restartPolicy: Never30,116%~@k   
status: {}31,120%~@k   
32,124%~@k   
apiVersion: v133,128%~@k   
kind: Pod34,132%~@k   
metadata:35,136%~@k   
creationTimestamp: null36,140%~@k   
labels:37,0-144%~@k   
run: busybox338,148%~@k   
name: busybox339,152%~@k   
spec:40,156%~@k   
containers:41,160%~@k   0~@k   39~@k   8~@k   7,0-1~@k   6,1  ~@k   5~@k   4dd  
- args:34,362%dd  
- bin/sh34,165%dd  
- -c34,0-168%dd  
- sleep 360034,171%i -- INSERT --34,171%apiVersion: v135,568%^[  35,468%u1 line less; before #8  2 seconds ago
apiVersion: v1- sleep 360034,171%~@k   3o -- INSERT --34,568%34,568%56789401
- sleep 360042,572%
image: busybox43,577%
name: busybox344,581%
resources: {}45,586%
dnsPolicy: ClusterFirst46,590%
restartPolicy: Never47,595%
status: {}48,5Bot9501232^[  52,4Bot~@k   1~@k   0dd  
~                                                                                                                                                                                      50,3Botdd  
~                                                                                                                                                                                      50,3Botdd  
~                                                                                                                                                                                      50,1Botdd  
~                                                                                                                                                                                      49,5~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   39~@k   8~@k   7~@k   6~@k   5~@k   4,0-1~@k   3,5  ~@k   2~@k   1~@k   0~@k   29~@k   8~@k   labels:27,5Bot~@k   creationTimestamp: null26,5Bot~@k   metadata:25,5Bot~@k   kind: Pod24,5Bot~@k   apiVersion: v123,594%~@k   22,588%~@k   resources: {}21,583%~@k   name: busybox120,577%~@k   image: busybox19,572%~@k   - sleep 3600; ls18,566%~@k   - -c17,0-161%~@k   - bin/sh16,555%~@k   - args:15,550%~@k   containers:14,544%~@k   spec:13,538%~@k   name: busybox112,533%~@k   run: busybox111,527%~@k   labels:10,522%~@k   creationTimestamp: null9,516%~@k   metadata:8,511%~@k   kind: Pod7,55%~@k   apiVersion: v16,5Top~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,5~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7,0-1~@k   8,5  ~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   
name: busybox227,55%~@k   
resources: {}28,511%~@k   
29,516%~@k   
apiVersion: v130,522%~@k   
kind: Pod31,527%~@k   
metadata:32,533%~@k   
creationTimestamp: null33,538%~@k   
labels:34,0-144%~@k   
run: busybox335,550%~@k   
name: busybox336,555%~@k   
spec:37,561%~@k   
containers:38,566%~@k   
- args:39,572%~@k   
- bin/sh40,577%~@k   
- -c41,583%~@k   
- sleep 360042,588%~@k   
image: busybox43,594%~@k   
name: busybox344,5Bot~@k   5~@k   6~@k   7~@k   8~@k   9~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ::wq!"multe-container.yaml" 49L, 698C written
[root@master01 ~]# kubectl delete busybox3
error: the server doesn't have a resource type "busybox3"
[root@master01 ~]# kubectl delete busybox3 busybox3p busybox3o busybox3
pod "busybox3" deleted
[root@master01 ~]# kubectl delete po busybox3busybox3vim multe-container.yaml kubectl get podscreate -f multe-container.yaml delecrea
pod/busybox3 created
[root@master01 ~]# kubectl create -f multe-container.yaml delete po busybox3busybox3vim multe-container.yaml kubectl get pods
NAME       READY   STATUS              RESTARTS   AGE
busybox3   0/1     ContainerCreating   0          3s
nginx      1/1     Running             0          3h14m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          5s
nginx      1/1     Running   0          3h15m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          6s
nginx      1/1     Running   0          3h15m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          7s
nginx      1/1     Running   0          3h15m
[root@master01 ~]# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
busybox3   1/1     Running   0          9s
nginx      1/1     Running   0          3h15m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml delete po busybox3busybox3
error: the server doesn't have a resource type "busybox3"
[root@master01 ~]# kubectl delete busybox3get podscreate -f multe-container.yaml delete po busybox3
pod "busybox3" deleted
[root@master01 ~]# kubectl delete po busybox3busybox3get podscreate -f multe-container.yaml delete po busybox3busybox3vim multe-container.yaml 
"multe-container.yaml" 49L, 698C  kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox2
  name: busybox2
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"; sleep 3600
    image: busybox
    name: busybox2
    resources: {}

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox3
  name: busybox3
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox349,5Bot~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   39~@k   8~@k   7~@k   6~@k   5~@k   4,0-1~@k   3,5  ~@k   2~@k   1~@k   0~@k   29~@k   8~@k   7~@k   6~@k   5~@k   4~@k   apiVersion: v123,594%~@k   22,588%~@k   resources: {}21,583%~@k   name: busybox120,577%~@k   image: busybox19,572%~@k   - sleep 3600; ls18,566%~@k   - -c17,0-161%~@k   - bin/sh16,555%~@k   - args:15,550%~@k   containers:14,544%~@k   spec:13,538%~@k   name: busybox112,533%~@k   run: busybox111,527%~@k   labels:10,522%~@k   creationTimestamp: null9,516%~@k   metadata:8,511%~@k   kind: Pod7,55%~@k   apiVersion: v16,5Top~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   2~@k   3~@k   4dd  
name: busybox24,3Topdd  
resources: {}4,5Topdd  
4,3Top~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,3~@k   1~@k   2~@k   3~@k   4,0-1~@k   5,3  dd  
apiVersion: v115,1Topdd  
kind: Pod15,1Topdd  
metadata:15,3Topdd  
creationTimestamp: null15,3Topdd  
labels:15,5Topdd  
run: busybox315,3Topdd  
name: busybox315,1Topdd  
spec:15,3Top~@k   4,0-1dd  
containers:14,3Topdd  
- args:14,3Top~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1,0-1~@k   0,3  ~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3dd  
- bin/sh13,3Top~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9dd  
- -c19,0-1Topdd  
- sleep 360019,1Topdd  
image: busybox19,1Topdd  
name: busybox319,1All~@k   20~@k   19dd  
~                                                                                                                                                                                      19,3Alldd  
~                                                                                                                                                                                      19,3Alldd  
~                                                                                                                                                                                      19,5Alldd  
~                                                                                                                                                                                      19,3Alldd  
~                                                                                                                                                                                      19,1Alldd  
~                                                                                                                                                                                      19,3Alldd  
~                                                                                                                                                                                      19,3All~@k   20~@k   1~@k   2~@k   3~@k   4~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   3~@k   2,16~@k   1,8 ~@k   0,12~@k   19,9 ~@k   8,18~@k   7~@k   6~@k   5,8 ~@k   4,12~@k   3,9 ~@k   2,18~@k   1~@k   0~@k   9,8  ~@k   8,12~@k   7,9 ~@k   6,13~@k   5,5 ~@k   4,16~@k   3,9 ~@k   2~@k   1,14~@k   ~@k   ~@k   2,9 ~@k   3~@k   4,16i -- INSERT --4,16All76^[  4,15All::wq!"multe-container.yaml" 24L, 353C written
[root@master01 ~]# vim multe-container.yaml kubectl delete po busybox3busybox3get podscreate -f multe-container.yaml delete po busybox3create -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl create -f multe-container.yaml vimkubectl delete po busybox3busybox3get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          3s
nginx     1/1     Running             0          3h17m
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/3     ContainerCreating   0          5s
nginx     1/1     Running             0          3h17m
[root@master01 ~]# kubectl get podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Thu Sep 15 17:55:37 2022NAME  READY   STATUS    RESTARTS   AGEbusybox   3/3     Running   09snginx     1/1     Running   03h17m912s424184668820[root@master01 ~]# watch kubectl get podswatch  pods pods podsl podso podsg podss podsbusybox
Defaulted container "busybox1" out of: busybox1, busybox2, busybox3
[root@master01 ~]# kubectl logs busybox1
Error from server (NotFound): pods "busybox1" not found
[root@master01 ~]# kubectl logs busybox1 busybox1
[root@master01 ~]# kubectl logs busybox busybox12
Hello PP
[root@master01 ~]# kubectl logs busybox busybox23
[root@master01 ~]# kubectl logs busybox busybox3211watch kubectl get podscreate -f multe-container.yaml vim
"multe-container.yaml" 24L, 353C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    image: busybox
    name: busybox1
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"; sleep 3600
    image: busybox
    name: busybox2
  - args:
    - bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox3
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      4,15All::q[root@master01 ~]# vim multe-container.yaml kubectl logs busybox busybox321edit
error: the server doesn't have a resource type "busybox"
[root@master01 ~]# kubectl edit busybox busybox1vim multe-container.yaml kubectl logs busybox busybox3211watch kubectl get podscreate -f multe-container.yaml vimkubectl delete po busybox3busybox3get podsdelete busybox3
error: the server doesn't have a resource type "busybox"
[root@master01 ~]# kubectl delete busybox busyboxp busyboxo busybox
pod "busybox" deleted
[root@master01 ~]# kubectl delete po busyboxbusyboxedit busybox busybox1vim multe-container.yaml 
"multe-container.yaml" 24L, 353C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - sleep 3600; ls
    image: busybox
    name: busybox1
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"; sleep 3600
    image: busybox
    name: busybox2
  - args:
    - bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox3
~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      ~                                                                                                                                                                                      4,15All~@k   5,5 ~@k   6,13~@k   7,9 ~@k   8,12~@k   9,8 ~@k   10,15~@k   6~@k   7~@k   8~@k   9~@k   20~@k   i -- INSERT --10,20All101983600765432109 87lsleep 36008ssleep 36009;sleep 360010 sleep 36001^[  10,10All::wq!"multe-container.yaml" 24L, 353C written
[root@master01 ~]# vim multe-container.yaml kubectl delete po busyboxbusyboxedit busybox busybox1vim multe-container.yaml kubectl logs busybox busybox3211watch kubectl get podscreate -f multe-container.yaml 
pod/busybox created
[root@master01 ~]# kubectl create -f multe-container.yaml vimkubectl delete po busyboxbusyboxedit busybox busybox1vim multe-container.yaml kubectl logs busybox busybox3211watch kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
busybox   3/3     Running   0          23s
nginx     1/1     Running   0          3h29m
[root@master01 ~]# kubectl get podscreate -f multe-container.yaml vimkubectl delete po busyboxbusyboxedit busybox busybox1vim multe-container.yaml kubectl edit busybox busybox1vim multe-container.yaml kubectl logs busybox busybox31
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox busybox12
Hello PP
[root@master01 ~]# kubectl logs busybox busybox23
[root@master01 ~]# logout
=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.16 10:15:08 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Authenticating with public key "rsa-key-20220112"
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Thu Sep 15 14:02:58 2022 from 192.168.200.15
[root@master01 ~]# [root@master01 ~]# kubectl get pods
NAME      READY   STATUS    RESTARTS         AGE
busybox   3/3     Running   48 (9m12s ago)   16h
nginx     1/1     Running   0                19h
[root@master01 ~]# kubectl get podslogs busybox busybox3
[root@master01 ~]# kubectl logs busybox busybox32
Hello PP
[root@master01 ~]# kubectl logs busybox busybox21
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# [root@master01 ~]# vim multe-container.yaml 
"multe-container.yaml" 24L, 353C  apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - ls; sleep 3600
    image: busybox
    name: busybox1
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"; sleep 3600
    image: busybox
    name: busybox2
  - args:
    - bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox3
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        10,10All~@k   1~@k   2~@k   3,9 ~@k   4,10~@k   5,8 ~@k   6,10~@k   7~@k   8~@k   9,9 ~@k   20,10~@k   1,8 ~@k   2,10~@k   3~@k   4~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ::q[root@master01 ~]# [root@master01 ~]# vim multe-container.yaml kubectl logs busybox busybox1vim multe-container.yaml vim multe-container.yaml kubectl logs busybox busybox1 -p
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox busybox1 -p--v=7
I0916 10:11:02.869497  130385 loader.go:374] Config loaded from file:  /root/.kube/config
I0916 10:11:02.875125  130385 round_trippers.go:463] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/busybox
I0916 10:11:02.875166  130385 round_trippers.go:469] Request Headers:
I0916 10:11:02.875194  130385 round_trippers.go:473]     Accept: application/json, */*
I0916 10:11:02.875220  130385 round_trippers.go:473]     User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe
I0916 10:11:02.888846  130385 round_trippers.go:574] Response Status: 200 OK in 13 milliseconds
I0916 10:11:02.894032  130385 round_trippers.go:463] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/busybox/log?container=busybox1
I0916 10:11:02.894071  130385 round_trippers.go:469] Request Headers:
I0916 10:11:02.894093  130385 round_trippers.go:473]     Accept: application/json, */*
I0916 10:11:02.894120  130385 round_trippers.go:473]     User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe
I0916 10:11:02.897894  130385 round_trippers.go:574] Response Status: 200 OK in 3 milliseconds
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox busybox1 --v=78 --v=82 --v=8
I0916 10:11:09.213470  130392 loader.go:374] Config loaded from file:  /root/.kube/config
I0916 10:11:09.218209  130392 round_trippers.go:463] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/busybox
I0916 10:11:09.218231  130392 round_trippers.go:469] Request Headers:
I0916 10:11:09.218242  130392 round_trippers.go:473]     User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe
I0916 10:11:09.218255  130392 round_trippers.go:473]     Accept: application/json, */*
I0916 10:11:09.232903  130392 round_trippers.go:574] Response Status: 200 OK in 14 milliseconds
I0916 10:11:09.232955  130392 round_trippers.go:577] Response Headers:
I0916 10:11:09.232977  130392 round_trippers.go:580]     X-Kubernetes-Pf-Flowschema-Uid: d43afb78-88f3-4116-97f6-ded19328e058
I0916 10:11:09.232997  130392 round_trippers.go:580]     X-Kubernetes-Pf-Prioritylevel-Uid: 74bbab13-ad92-41cb-beb6-909df531ce22
I0916 10:11:09.233019  130392 round_trippers.go:580]     Date: Fri, 16 Sep 2022 04:41:09 GMT
I0916 10:11:09.233038  130392 round_trippers.go:580]     Audit-Id: ef86dbfb-36d4-4910-888e-9260a9d65a19
I0916 10:11:09.233056  130392 round_trippers.go:580]     Cache-Control: no-cache, private
I0916 10:11:09.233076  130392 round_trippers.go:580]     Content-Type: application/json
I0916 10:11:09.233264  130392 request.go:1073] Response Body: {"kind":"Pod","apiVersion":"v1","metadata":{"name":"busybox","namespace":"default","uid":"da4d10f3-a849-44e4-b460-8b4ecd0cc99b","resourceVersion":"556513","creationTimestamp":"2022-09-15T12:36:19Z","managedFields":[{"manager":"kubectl-create","operation":"Update","apiVersion":"v1","time":"2022-09-15T12:36:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:containers":{"k:{\"name\":\"busybox1\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}},"k:{\"name\":\"busybox2\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}},"k:{\"name\":\"busybox3\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminatio [truncated 5028 chars]
I0916 10:11:09.237829  130392 round_trippers.go:463] GET https://172.16.102.22:6443/api/v1/namespaces/default/pods/busybox/log?container=busybox2
I0916 10:11:09.237850  130392 round_trippers.go:469] Request Headers:
I0916 10:11:09.237860  130392 round_trippers.go:473]     User-Agent: kubectl/v1.25.0 (linux/amd64) kubernetes/a866cbe
I0916 10:11:09.237874  130392 round_trippers.go:473]     Accept: application/json, */*
I0916 10:11:09.242484  130392 round_trippers.go:574] Response Status: 200 OK in 4 milliseconds
I0916 10:11:09.242523  130392 round_trippers.go:577] Response Headers:
I0916 10:11:09.242549  130392 round_trippers.go:580]     Audit-Id: 9224a019-eeae-4f59-a960-3e506efb365a
I0916 10:11:09.242573  130392 round_trippers.go:580]     Cache-Control: no-cache, private
I0916 10:11:09.242591  130392 round_trippers.go:580]     Content-Type: text/plain
I0916 10:11:09.242610  130392 round_trippers.go:580]     Date: Fri, 16 Sep 2022 04:41:09 GMT
Hello PP
[root@master01 ~]# kubectl logs busybox busybox2 --v=8-busybox2cbusybox2 busybox2
Hello PP
[root@master01 ~]# kubectl logs busybox -c busybox21
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox -c busybox13
[root@master01 ~]# kubectl logs busybox -c busybox31 -p
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox -c busybox1 -p--previous
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl logs busybox -c busybox1 --previousp3 ls
error: only one of -c or an inline [CONTAINER] arg is allowed
[root@master01 ~]# kubectl logs busybox -c busybox3 lskubectl logs busybox -c busybox3 ls exec -it  ls- ls- ls lsb lsa lss lsh ls ls ls ls lss lsh ls
Defaulted container "busybox1" out of: busybox1, busybox2, busybox3
sh: can't open 'ls': No such file or directory
command terminated with exit code 2
[root@master01 ~]# kubectl exec -it busybox busybox3 -- sh lslslslslslsls
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulted container "busybox1" out of: busybox1, busybox2, busybox3
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "d3c33ed39c516254188f124c5d5565b5904727f44398be71135f771ad2536909": OCI runtime exec failed: exec failed: unable to start container process: exec: "busybox3": executable file not found in $PATH: unknown
[root@master01 ~]# kubectl exec -it busybox busybox3 ls
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulted container "busybox1" out of: busybox1, busybox2, busybox3
error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "d4481e128aeaca2353e8c9dd95074da0cef984f552a413f2dee3fd5447fc252c": OCI runtime exec failed: exec failed: unable to start container process: exec: "busybox3": executable file not found in $PATH: unknown
[root@master01 ~]# kubectl exec busybox busybox3 ls-ls-ls lsslshls ls
Defaulted container "busybox1" out of: busybox1, busybox2, busybox3
sh: can't open 'ls': No such file or directory
command terminated with exit code 2
[root@master01 ~]# kubectl exec busybox busybox3 -- sh lslslslslslsls -c
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
bin
dev
etc
home
proc
root
sys
tmp
usr
var
[root@master01 ~]# kubectl exec busybox -c busybox3 lstop--containers >/ /tmp/multi.logs.logs.logs.logs.logs.logsf.logsi.logsl.logse.logs
error: unknown flag: --containers
See 'kubectl top --help' for usage.
[root@master01 ~]# kubectl top busybox --containers > /tmp/file.logs pod
error: Metrics API not available
[root@master01 ~]# kubectl create -f https://raw.githubusercontent.com/lerndevops/educka/master/9-monitoring/metrics-server/metrics-server-v0.5.yml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
[root@master01 ~]# kubectl create -f https://raw.githubusercontent.com/lerndevops/educka/master/9-monitoring/metrics-server/metrics-server-v0.5.ymltop pod busybox --containers > /tmp/file.logspod 
[root@master01 ~]# less /tmp/file.logs 
POD       NAME       CPU(cores)   MEMORY(bytes)   
busybox   busybox1   0m           0Mi             
busybox   busybox2   0m           0Mi             
busybox   busybox3   0m           0Mi             
/tmp/file.logs (END)[root@master01 ~]# less /tmp/file.logs kubectl top pod busybox --containers > /tmp/file.logs
POD       NAME       CPU(cores)   MEMORY(bytes)   
busybox   busybox1   0m           0Mi             
busybox   busybox2   0m           0Mi             
busybox   busybox3   0m           0Mi             
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS    RESTARTS       AGE
busybox   3/3     Running   51 (66m ago)   18h
nginx     1/1     Running   0              21h
[root@master01 ~]# kubectl delete po busybox nginx
pod "busybox" deleted
pod "nginx" deleted
[root@master01 ~]# kubectl create run busybox --image=busybox -o- /bin/sh -c """" ,"while true; do echo Hi I am from Main container >> /var/log/index.html; sleep 5; done"''  --d -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"  r -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"  y -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done" - -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done" t -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; doner -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; doneu -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; donn -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; do= -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; dc -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; l -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5;i -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5n -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep e -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleept -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; slee -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; slee -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep e -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep n -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleept -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; slee
> busy-side.yaml
error: unknown shorthand flag: 'c' in -c
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run busybox --image=busybox --dry-run=client -o- "/bin/sh", "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleeep 5; done" > busy-side.yaml "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; slee "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep  "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5"while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; 

error: unable to match a printer suitable for the output format "-", allowed formats are: go-template,go-template-file,json,jsonpath,jsonpath-as-json,jsonpath-file,name,template,templatefile,yaml
[root@master01 ~]# kubectl run busybox --image=busybox --dry-run=client -o- "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5;  done" > busy-side.yaml "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; d "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; do "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; don"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done

[root@master01 ~]# vim busy-side.yaml 
"busy-side.yaml" 1L, 30C  pod/busybox created (dry run)
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All::q![root@master01 ~]# kubectl get pods
No resources found in default namespace.
[root@master01 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                                   READY   STATUS    RESTARTS        AGE
kube-system   coredns-565d847f94-48j9p                               1/1     Running   1 (4d15h ago)   4d15h
kube-system   coredns-565d847f94-xcm8b                               1/1     Running   1 (4d15h ago)   4d15h
kube-system   etcd-master01.airtel.internal.lab                      1/1     Running   1 (4d15h ago)   4d15h
kube-system   kube-apiserver-master01.airtel.internal.lab            1/1     Running   1 (4d15h ago)   4d15h
kube-system   kube-controller-manager-master01.airtel.internal.lab   1/1     Running   1 (4d15h ago)   4d15h
kube-system   kube-proxy-bk8v6                                       1/1     Running   0               4d15h
kube-system   kube-proxy-rfb9j                                       1/1     Running   1 (4d15h ago)   4d15h
kube-system   kube-proxy-vpx59                                       1/1     Running   0               4d15h
kube-system   kube-scheduler-master01.airtel.internal.lab            1/1     Running   1 (4d15h ago)   4d15h
kube-system   metrics-server-55dd4998bd-qr2n9                        1/1     Running   0               122m
kube-system   weave-net-42gjg                                        2/2     Running   3 (4d15h ago)   4d15h
kube-system   weave-net-44k9t                                        2/2     Running   1 (4d15h ago)   4d15h
kube-system   weave-net-mdv4h                                        2/2     Running   0               4d15h
[root@master01 ~]# kubectl get pods --all-namespacesvim busy-side.yaml 
"busy-side.yaml" 1L, 30C  pod/busybox created (dry run)
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All::q[root@master01 ~]# vim busy-side.yaml kubectl get pods --all-namespacesvim busy-side.yaml kubectl run busybox --image=busybox --dry-run=client "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; donee" > busy-side.yaml-"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; doneo"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; don "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; doy"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; da"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; m"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5;l"/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5 "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 

[root@master01 ~]# vim busy-side.yaml 
"busy-side.yaml" 19L, 364C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - /bin/sh
    - while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep5; done
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7i -- INSERT --13,7All5; done65; done55; done45; done35; done25; done1
    - while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep5; done~                                                                                                                                                                        12,86All 5; done7^[  12,86All~@k   1,13o -- INSERT --12,7Top12,7All65-6- 7c83456787656^[  16,7Alldd  
~                                                                                                                                                                        16,3Alldd  
~                                                                                                                                                                        16,3Alldd  
~                                                                                                                                                                        16,1All^[  ^[  u1 more line; before #5  2 seconds agorestartPolicy: Always16,3All^[  ^[  ::wq!"busy-side.yaml" 17L, 322C written
[root@master01 ~]# kubectl craete -f busy-side.yaml 
error: unknown command "craete" for "kubectl"

Did you mean this?
create
[root@master01 ~]# kubectl craete -f busy-side.yaml eate
pod/busybox created
[root@master01 ~]# kubectl get pods
NAME      READY   STATUS   RESTARTS   AGE
busybox   0/1     Error    0          5s
[root@master01 ~]# kubectl get podscreate -f busy-side.yaml aevim
"busy-side.yaml" 17L, 322C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - /bin/sh
    - c
    - while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done
    image: busybox
    name: busybox
  restartPolicy: Always
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        16,3All~@k   5~@k   4~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   40~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   50~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   60~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   70~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   80~@k   1~@k   2,7 dd  
~                                                                                                                                                                        12,5All::wq!"busy-side.yaml" 16L, 314C written
[root@master01 ~]# vim busy-side.yaml kubectl get podscreate -f busy-side.yaml aevimkubectl run busybox --image=busybox --dry-run=client -o yaml "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleepp 5; done" > busy-side.yamlvim busy-side.yaml 
kubectl get pods --all-namespacesvim busy-side.yaml kubectl run busybox --image=busybox --dry-run=client "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; donee" > busy-side.yaml-o- "/bin/sh" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done, "-c" "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; delete po busybox nginx
get podsdelete po busybox nginx
pod "busybox" deleted
[root@master01 ~]# kubectl delete po busybox vim busy-side.yamlkubectl get podscreate -f busy-side.yaml 
pod/busybox created
[root@master01 ~]# kubectl get po
NAME      READY   STATUS   RESTARTS   AGE
busybox   0/1     Error    0          5s
[root@master01 ~]# kubectl get pocreate -f busy-side.yaml delete po busybox
pod "busybox" deleted
[root@master01 ~]# ls -l 
total 10240
-rw-r--r--  1 root root      284 Sep 13 12:33 admin-pod.yaml
-rw-------. 1 root root     1471 Jun  9 16:46 anaconda-ks.cfg
-rw-r--r--  1 root root      314 Sep 16 12:23 busy-side.yaml
-rw-r--r--  1 root root      281 Sep 13 14:36 ds-kusc00201.yaml
drwxr-xr-x  3 1000 1000      108 Sep 13 16:28 etcd-v3.3.13-linux-amd64
-rw-r--r--  1 root root 10423953 Dec  7  2021 etcd-v3.3.13-linux-amd64.tar.gz
-rw-r--r--  1 root root      353 Sep 15 18:06 multe-container.yaml
-rw-r--r--  1 root root      413 Sep 13 14:22 nfs-pv.yaml
-rw-r--r--  1 root root      219 Sep 15 12:10 nginx1.17.4.yaml
-rw-r--r--  1 root root      270 Sep 14 17:24 nginx-same.yaml
-rw-r--r--  1 root root      176 Sep 13 15:19 nginx.yaml
-rw-r--r--  1 root root        0 Sep 12 09:41 nodes.txt
-rw-r--r--  1 root root      249 Sep 14 16:55 non-root-pod.yaml
-rw-r--r--  1 root root      289 Sep 14 16:47 pod-redis.yaml
-rw-r--r--  1 root root      496 Sep 13 15:06 pod-spec-KUCC00108.yaml
-rw-r--r--  1 root root      143 Sep 14 17:00 policynetwork.yaml
-rw-r--r--  1 root root      184 Sep 13 17:46 pv-analytics.yaml
-rw-r--r--  1 root root      338 Sep 13 11:59 web-project-268.yaml
[root@master01 ~]# cat multe-container.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - args:
    - bin/sh
    - -c
    - ls; sleep 3600
    image: busybox
    name: busybox1
  - args:
    - bin/sh
    - -c
    - echo "Hello PP"; sleep 3600
    image: busybox
    name: busybox2
  - args:
    - bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox3
[root@master01 ~]# [root@master01 ~]# vim busy-side.yaml 
"busy-side.yaml" 16L, 314C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - args:
    - /bin/sh
    - while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done
    image: busybox
    name: busybox
  restartPolicy: Always
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        12,5All::q[root@master01 ~]# vim busy-side.yaml kubectl run multi-cont-pod --image=busbox --restart=Never --dry-run -o yaml > multi-container.yaml si busy-side.yaml
W0916 12:24:59.717425  132871 helpers.go:639] --dry-run is deprecated and can be replaced with --dry-run=client.
[root@master01 ~]# kubectl run multi-cont-pod --image=busbox --restart=Never --dry-run -o yaml >  busy-side.yaml=client
[root@master01 ~]# vim busy-side.yaml 
"busy-side.yaml" 15L, 259C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: multi-cont-pod
  name: multi-cont-pod
spec:
  containers:
  - image: busbox
    name: multi-cont-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        12,5All~@k   1~@k   0~@k   9,5 ~@k   8~@k   7~@k   6~@k   5~@k   4dd  
~                                                                                                                                                                        4,3Alldd  
~                                                                                                                                                                        4,5Alldd  
~                                                                                                                                                                        4,3All~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,3dd  
~                                                                                                                                                                        10,3All~@k   9,3 o -- INSERT --10,5Top10,5All4321^[  10,0-1All~@k   9,1   ~@k   8~@k   9dd  
~                                                                                                                                                                        9,0-1Alli -- INSERT --9,1All- name: nginx-container
    image: nginx
    volumeMo(paste) --volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html13,39All2,241,180,179,26 8,25--a9,5All10,5123432109,1 8,5765^[  5,4Allo -- INSERT --6,9Top6,9Alli109 i8i7i6i5i4i3i2321volumes:
  - name: shared-data
    emptyDir: {

  - - - (paste) --}

  - - - {}8,17All--a9,5All{}
    10,5123,14,53,14,556543,1volumeMounts:
    - name: shared-datamountPa(paste) --mountPath: /pod-data
    command: ["/bin/sh"]
    args: ["-c", "echo 
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  restartPolicy: Never
status: {}Hello from the debian container > /pod-data/index.html"]
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  restartPolicy: Never
status: {}[]17,80All--a18,5All[]
    92012321019^[  19,4All~@k   8,0-1~@k   7,4  ~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0o -- INSERT --11,11Top11,11All09 87654321- name: nginx-container
    image: nginx
    volumeM(paste) --volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html--a16,1Top789201234
restartPolicy: Never25,150%
status: {}26,1Bot789301029876545^[  25,1Botdd  
~                                                                                                                                                                        25,5Botdd  
~                                                                                                                                                                        25,5Botdd  
~                                                                                                                                                                        25,5Botdd  
~                                                                                                                                                                        25,7Botdd  
~                                                                                                                                                                        25,3Bot~@k   4,0-1dd  
~                                                                                                                                                                        24,3Bot~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6,0-1~@k   5,3  ~@k   4~@k   3~@k   2~@k   4~@k   5~@k   6dd  
~                                                                                                                                                                        12,5Bot~@k   1~@k   0p image: nginx11,5Boti -- INSERT --11,5Bot43        -  image: nginx12  image: nginx1 image: nginx0 image: nginx9  image: nginx8 image: nginx7 image: nginx6 image: nginx5 image: nginx4 image: nginx3-  image: nginx456image: nginx52-name: nginx-container4name: nginx-container3 name: nginx-container4 name: nginx-container5345^[  15,4Bot~@k   6,0-1~@k   7,4  ~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   []1~@k   []2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   30~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   40~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   50~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   60~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   70~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8i -- INSERT --23,78Bot"]7"]6"]5"]4"]3"]2"]1"]0"]69"]8"]7"]6"]5"]4"]3"]2"]1"]0"]59"]8"]7"]6"]5"]4"]3"]2"]1"]0"]49"]8"]7"]6"]5"]4"]3"]2"]1"]0"]39"]8"]7"]6"]5"]4"]3"]2"]1"]0"]29"]8"]7"]6"]5"]4"]3"]2"]1"]0"]19while true; do echo Hi I am from Main container >>"] /var/log/index.html; sleep 5; done"]10-10609-1058-1047-1036-1025-1014-1003-99 2-981-970-9699-95 8-947-936-925-914-903-892-881-870-8689-858-847-836-825-814-803-792-781-770-7679-758-747-736-725-714-703-692-6869-678-669-6772-68 >> /var/log/index.html; sleep 5; done"]69-67' >> /var/log/index.html; sleep 5; done"]70-6869-678-667-656-645-634-620-5859-578-567-556-545-534-523-512-501-490-4849-478-467-456-445-434-423-412-4039   42-40Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]39   'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]40^[  23,39Bot~@k   []2,24~@k   []1,26~@k   0,23~@k   19,17~@k   8,24~@k   7,17~@k   6,0-1~@k   5,38 o -- INSERT --16,766%16,7Bot7,18,79201289101234567892012345676543210198/var/log261,240,1819,258,187,1 65,267893012345678916,7Bot654321ports:- containerPort: 8017,28Bot6,155,284,245,286,15432109 ports:8ports:7ports:6ports:5545676789- containerPort: 808- containerPort: 807- containerPort: 806- containerPort: 805^[  17,4Bot~@k   8,0-1^[  ^[  dd  
~                                                                                                                                                                        18,0-1Botdd  
~                                                                                                                                                                        18,3Bot::wq!"busy-side.yaml" 26L, 566C written
[root@master01 ~]# kubectl create -f busy-side.yaml 
pod/multi-cont-pod created
[root@master01 ~]# kubectl get pod
NAME             READY   STATUS              RESTARTS   AGE
multi-cont-pod   0/2     ContainerCreating   0          5s
[root@master01 ~]# kubectl get podwatch 
Every 2.0s: kubectl get podmaster01.airtel.internal.lab: Fri Sep 16 12:38:57 2022NAMEREADY   STATUSRESTARTS   AGEmulti-cont-pod   0/2     ContainerCreating   011s91 ErrImagePull   0          13s9:01537[root@master01 ~]# watch kubectl get pod pod pod podd pode pods podc podr podi podb pode pod multi-cont-pod
Name:             multi-cont-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Fri, 16 Sep 2022 12:32:15 +0530
Labels:           <none>
Annotations:      <none>
Status:           Pending
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx-container:
    Container ID:   containerd://178124c7c8d07085645cce6a61db9a1060230e480bd65acc8f457d1ffbe78ba9
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 16 Sep 2022 12:32:18 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /usr/share/nginx/html from shared-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zsbhz (ro)
  multi-cont-pod:
    Container ID:  
    Image:         busbox
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      -c
      while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log from shared-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zsbhz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  shared-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-zsbhz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  32s                    default-scheduler  Successfully assigned default/multi-cont-pod to worker01.airtel.internal.lab
  Normal   Pulling    7m3s                   kubelet            Pulling image "nginx"
  Normal   Pulled     7m1s                   kubelet            Successfully pulled image "nginx" in 2.122637007s
  Normal   Created    7m1s                   kubelet            Created container nginx-container
  Normal   Started    7m1s                   kubelet            Started container nginx-container
  Normal   BackOff    6m52s                  kubelet            Back-off pulling image "busbox"
  Warning  Failed     6m52s                  kubelet            Error: ImagePullBackOff
  Normal   Pulling    6m41s (x2 over 7m1s)   kubelet            Pulling image "busbox"
  Warning  Failed     6m39s (x2 over 6m53s)  kubelet            Failed to pull image "busbox": rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/busbox:latest": failed to resolve reference "docker.io/library/busbox:latest": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     6m39s (x2 over 6m53s)  kubelet            Error: ErrImagePull
[root@master01 ~]# kubectl describe pod multi-cont-podwatch kubectl get podcreate -f busy-side.yaml vim
"busy-side.yaml" 26L, 566C  apiVersion: v1
kind: Pod
metadata:
  name: multi-cont-pod
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  containers:
  - image: nginx
    name: nginx-container
    volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html
    ports:
    - containerPort: 80
  - image: busbox
    name: multi-cont-pod
    volumeMounts:
    - name: shared-datamountPath: /var/log
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        18,3All~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5i -- INSERT --18,15Allybox6^[  18,15All::wq!"busy-side.yaml" 26L, 567C written
[root@master01 ~]# kubectl delete po multi-cont-pod
pod "multi-cont-pod" deleted
[root@master01 ~]# kubectl delete po multi-cont-podvim busy-side.yaml kubectl describe pod multi-cont-podwatch kubectl get podcreate -f busy-side.yaml 
pod/multi-cont-pod created
[root@master01 ~]# kubectl create -f busy-side.yaml delete po multi-cont-podvim busy-side.yaml kubectl describe pod multi-cont-podwatch kubectl get pod
Every 2.0s: kubectl get podmaster01.airtel.internal.lab: Fri Sep 16 12:39:53 2022NAMEREADY   STATUSRESTARTS   AGEmulti-cont-pod   0/2     ContainerCreating   02s5472 Running   0          6s9840:0110s[root@master01 ~]# kubectl get po multi-cont-pod
NAME             READY   STATUS    RESTARTS   AGE
multi-cont-pod   2/2     Running   0          38s
[root@master01 ~]# kubectl get po multi-cont-podwatch kubectl get podkubectl create -f busy-side.yaml delete po multi-cont-podvim busy-side.yaml kubectl describe pod multi-cont-pod
Name:             multi-cont-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Fri, 16 Sep 2022 12:33:20 +0530
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  nginx-container:
    Container ID:   containerd://52c6040b0c62d2b6996f411439b7b53970376bfd9333b63cb7eae9e0da0a48df
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 16 Sep 2022 12:33:22 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /usr/share/nginx/html from shared-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pf948 (ro)
  multi-cont-pod:
    Container ID:  containerd://3bc7e13fa967bfad53bca66fedc814e44265b0374e6d0dd73c2482fefc70e7e3
    Image:         busybox
    Image ID:      docker.io/library/busybox@sha256:ad9bd57a3a57cc95515c537b89aaa69d83a6df54c4050fcf2b41ad367bec0cd5
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/sh
    Args:
      -c
      while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done
    State:          Running
      Started:      Fri, 16 Sep 2022 12:33:25 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/log from shared-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pf948 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  shared-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-pf948:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  49s    default-scheduler  Successfully assigned default/multi-cont-pod to worker01.airtel.internal.lab
  Normal  Pulling    7m20s  kubelet            Pulling image "nginx"
  Normal  Pulled     7m18s  kubelet            Successfully pulled image "nginx" in 2.245600932s
  Normal  Created    7m18s  kubelet            Created container nginx-container
  Normal  Started    7m18s  kubelet            Started container nginx-container
  Normal  Pulling    7m18s  kubelet            Pulling image "busybox"
  Normal  Pulled     7m15s  kubelet            Successfully pulled image "busybox" in 2.370536694s
  Normal  Created    7m15s  kubelet            Created container multi-cont-pod
  Normal  Started    7m15s  kubelet            Started container multi-cont-pod
[root@master01 ~]# sudo kubectl describe pod multi-cont-podkubectl describe pod multi-cont-podget powatch kubectl get podkubectl get po multi-cont-pod
NAME             READY   STATUS    RESTARTS   AGE
multi-cont-pod   2/2     Running   0          77s
[root@master01 ~]# kubectl exec -it multi-cont-pod nginx -- bash 
Defaulted container "nginx-container" out of: nginx-container, multi-cont-pod
root@multi-cont-pod:/# root@multi-cont-pod:/# 
exit
[root@master01 ~]# kubectl exec -it multi-cont-pod nginx -- bash nginx-container
Defaulted container "nginx-container" out of: nginx-container, multi-cont-pod
root@multi-cont-pod:/# root@multi-cont-pod:/# ls -l /usr/share/nginx/html/index.html 
-rw-r--r-- 1 root root 728 Sep 16 07:05 /usr/share/nginx/html/index.html
root@multi-cont-pod:/# ls -l /usr/share/nginx/html/index.html root@multi-cont-pod:/# ls -lless
bash: less: command not found
root@multi-cont-pod:/# less /usr/share/nginx/html/index.html root@multi-cont-pod:/# m
mapfile           md5sum            mkdir             mkfs              mkfs.ext2         mkfs.minix        mknod             more              mv
mawk              md5sum.textutils  mke2fs            mkfs.bfs          mkfs.ext3         mkhomedir_helper  mkswap            mount             
mcookie           mesg              mkfifo            mkfs.cramfs       mkfs.ext4         mklost+found      mktemp            mountpoint        
root@multi-cont-pod:/# mo
more        mount       mountpoint  
root@multi-cont-pod:/# more /usr/share/nginx/html/index.htmlroot@multi-cont-pod:/# more /usr/share/nginx/html/index.html
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
root@multi-cont-pod:/# 
exit
[root@master01 ~]# kubectl exec -it multi-cont-pod nginx-container -- bash buskubectl exec -it multi-cont-pod  -- bash nginxget po multi-cont-poddescribe podget powatch kubectl get podkubectl create -f busy-side.yaml delete po multi-cont-podvim busy-side.yaml kubectl describe pod multi-cont-podvim busy-side.yaml 
"busy-side.yaml" 26L, 567C  apiVersion: v1
kind: Pod
metadata:
  name: multi-cont-pod
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  containers:
  - image: nginx
    name: nginx-container
    volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html
    ports:
    - containerPort: 80
  - image: busybox
    name: multi-cont-pod
    volumeMounts:
    - name: shared-datamountPath: /var/log
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        18,15All::q[root@master01 ~]# vim busy-side.yaml kubectl exec -it multi-cont-pod  -- bashnginxmulti-cont-pod
Defaulted container "nginx-container" out of: nginx-container, multi-cont-pod
# l df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  4.6G   43G  10% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
shm                   64M     0   64M   0% /dev/shm
/dev/mapper/rl-root   47G  4.6G   43G  10% /etc/hosts
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
# cd /var/log
# ls
apt  btmp  dpkg.log  faillog  lastlog  nginx  wtmp
# cat nginx
cat: nginx: Is a directory
# ^[[A    cd nginx
# ls
access.log  error.log
# cd
# df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  4.6G   43G  10% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
shm                   64M     0   64M   0% /dev/shm
/dev/mapper/rl-root   47G  4.6G   43G  10% /etc/hosts
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
# mount -o
mount: option requires an argument -- 'o'
Try 'mount --help' for more information.
# ^[[A    mount
overlay on / type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/45/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/44/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/43/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/42/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/41/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/40/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/724/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/724/work)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)
tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (ro,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (ro,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/rdma type cgroup (ro,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (ro,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/blkio type cgroup (ro,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (ro,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/devices type cgroup (ro,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/cpuset type cgroup (ro,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/pids type cgroup (ro,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/freezer type cgroup (ro,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/perf_event type cgroup (ro,nosuid,nodev,noexec,relatime,perf_event)
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k)
/dev/mapper/rl-root on /etc/hosts type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /dev/termination-log type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/hostname type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/resolv.conf type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /usr/share/nginx/html type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
tmpfs on /run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime,size=7847236k)
proc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/fs type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/irq type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)
tmpfs on /proc/acpi type tmpfs (ro,relatime)
tmpfs on /proc/kcore type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/keys type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/timer_list type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/sched_debug type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/scsi type tmpfs (ro,relatime)
tmpfs on /sys/firmware type tmpfs (ro,relatime)
# cd /var/log       usr/      /usr/share/nginx/html
# ls
index.html
# cat index.html
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
# 
[root@master01 ~]# curkubectl get pod -o wide
NAME             READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
multi-cont-pod   2/2     Running   0          6m18s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# curl 10.36.0.1
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
[root@master01 ~]# curl 10.36.0.1kubectl get pod -o wide
NAME             READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
multi-cont-pod   2/2     Running   0          6m42s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# ll
total 10240
-rw-r--r--  1 root root      284 Sep 13 12:33 admin-pod.yaml
-rw-------. 1 root root     1471 Jun  9 16:46 anaconda-ks.cfg
-rw-r--r--  1 root root      567 Sep 16 12:39 busy-side.yaml
-rw-r--r--  1 root root      281 Sep 13 14:36 ds-kusc00201.yaml
drwxr-xr-x  3 1000 1000      108 Sep 13 16:28 etcd-v3.3.13-linux-amd64
-rw-r--r--  1 root root 10423953 Dec  7  2021 etcd-v3.3.13-linux-amd64.tar.gz
-rw-r--r--  1 root root      353 Sep 15 18:06 multe-container.yaml
-rw-r--r--  1 root root      413 Sep 13 14:22 nfs-pv.yaml
-rw-r--r--  1 root root      219 Sep 15 12:10 nginx1.17.4.yaml
-rw-r--r--  1 root root      270 Sep 14 17:24 nginx-same.yaml
-rw-r--r--  1 root root      176 Sep 13 15:19 nginx.yaml
-rw-r--r--  1 root root        0 Sep 12 09:41 nodes.txt
-rw-r--r--  1 root root      249 Sep 14 16:55 non-root-pod.yaml
-rw-r--r--  1 root root      289 Sep 14 16:47 pod-redis.yaml
-rw-r--r--  1 root root      496 Sep 13 15:06 pod-spec-KUCC00108.yaml
-rw-r--r--  1 root root      143 Sep 14 17:00 policynetwork.yaml
-rw-r--r--  1 root root      184 Sep 13 17:46 pv-analytics.yaml
-rw-r--r--  1 root root      338 Sep 13 11:59 web-project-268.yaml
[root@master01 ~]# mv busy-side.yaml busy-side.yaml car.yaml
[root@master01 ~]# [root@master01 ~]# mv busy-side.yaml busy-sidecar.yamlllkubectl get pod -o widecurl 10.36.0.1kubectl get pod -o wideexec -it multi-cont-pod multi-cont-pod -- sh 
Defaulted container "nginx-container" out of: nginx-container, multi-cont-pod
# cd /vat r/log
# ls
apt  btmp  dpkg.log  faillog  lastlog  nginx  wtmp
# 
[root@master01 ~]# kubectl exec -it multi-cont-pod multi-cont-pod -- sh mv busy-side.yaml busy-sidecar.yamlllmv busy-side.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yamlbusy-sidecar.yaml busy-sidecar.yaml busy-sidecar.yamlv busy-sidecar.yamli busy-sidecar.yamli busy-sidecar.yamlm busy-sidecar.yaml
-bash: viim: command not found
[root@master01 ~]# viim busy-sidecar.yaml
"busy-sidecar.yaml" 26L, 567C  apiVersion: v1
kind: Pod
metadata:
  name: multi-cont-pod
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  containers:
  - image: nginx
    name: nginx-container
    volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html
    ports:
    - containerPort: 80
  - image: busybox
    name: multi-cont-pod
    volumeMounts:
    - name: shared-datamountPath: /var/log
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All::q![root@master01 ~]# vim busy-sidecar.yaml
"busy-sidecar.yaml" 26L, 567C  apiVersion: v1
kind: Pod
metadata:
  name: multi-cont-pod
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  containers:
  - image: nginx
    name: nginx-container
    volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html
    ports:
    - containerPort: 80
  - image: busybox
    name: multi-cont-pod
    volumeMounts:
    - name: shared-datamountPath: /var/log
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All::q[root@master01 ~]# vim busy-sidecar.yamlmulti-cont-podkubectl exec -it multi-cont-pod -cmulti-cont-pod multi-cont-pod -- sh
/ # / # cat /var/log/main.txt
cat: can't open '/var/log/main.txt': No such file or directory
/ # 
command terminated with exit code 1
[root@master01 ~]# kuvbectl delete -f multe-container.yaml busy-sidecar.yaml 
pod "multi-cont-pod" deleted
[root@master01 ~]# kubectl delete -f busy-sidecar.yaml exec -it multi-cont-pod -c multi-cont-pod -- shvim busy-sidecar.yaml
"busy-sidecar.yaml" 26L, 567C  apiVersion: v1
kind: Pod
metadata:
  name: multi-cont-pod
spec:
  volumes:
  - name: shared-data
    emptyDir: {}

  containers:
  - image: nginx
    name: nginx-container
    volumeMounts:
    - name: shared-datamountPath: /usr/share/nginx/html
    ports:
    - containerPort: 80
  - image: busybox
    name: multi-cont-pod
    volumeMounts:
    - name: shared-datamountPath: /var/log
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'Hi I am from Main container' >> /var/log/index.html; sleep 5; done"]
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   5~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9,0-1~@k   10,1 ~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   9~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --19,24All5432101987654321main-container258,197,246,115,254,243,182,254321019876-container5-container4-container3-container2-container1s-container2i-container3d-container4e-container5c-container6a-container7r-container8^[  12,17All~@k   1,16~@k   0,13~@k   9,0-1~@k   {}8,16 ~@k   {}7,17~@k   {}8,16~@k   {}9,0-1~@k   10,13~@k   1,16~@k   2,17~@k   3~@k   4~@k   5~@k   6,10~@k   7,17~@k   8~@k   9~@k   20~@k   1^[  ^[  ::wq!"busy-sidecar.yaml" 26L, 569C written
[root@master01 ~]# kubectl create -f busy-sidecar.yaml 
pod/multi-cont-pod created
[root@master01 ~]# kubectl get po
NAME             READY   STATUS              RESTARTS   AGE
multi-cont-pod   0/2     ContainerCreating   0          4s
[root@master01 ~]# kubectl get po -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
multi-cont-pod   2/2     Running   0          14s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# curl 10.36.0.1
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
[root@master01 ~]# kubectl exec -it multi-cont-pod -c main-container -- sh
/ # / # cat /var/log/main.txt
cat: can't open '/var/log/main.txt': No such file or directory
/ # df -h 
Filesystem                Size      Used Available Use% Mounted on
overlay                  46.9G      4.5G     42.4G  10% /
tmpfs                    64.0M         0     64.0M   0% /dev
tmpfs                     3.8G         0      3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /var/log
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /etc/hosts
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /dev/termination-log
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /etc/hostname
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /etc/resolv.conf
shm                      64.0M         0     64.0M   0% /dev/shm
tmpfs                     7.5G     12.0K      7.5G   0% /var/run/secrets/kubernetes.io/serviceaccount
tmpfs                     3.8G         0      3.8G   0% /proc/acpi
tmpfs                    64.0M         0     64.0M   0% /proc/kcore
tmpfs                    64.0M         0     64.0M   0% /proc/keys
tmpfs                    64.0M         0     64.0M   0% /proc/timer_list
tmpfs                    64.0M         0     64.0M   0% /proc/sched_debug
tmpfs                     3.8G         0      3.8G   0% /proc/scsi
tmpfs                     3.8G         0      3.8G   0% /sys/firmware
/ # cd /var/log
/var/log # ls
index.html
/var/log # cat index.html
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
/var/log # df -h 
Filesystem                Size      Used Available Use% Mounted on
overlay                  46.9G      4.5G     42.4G  10% /
tmpfs                    64.0M         0     64.0M   0% /dev
tmpfs                     3.8G         0      3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /var/log
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /etc/hosts
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /dev/termination-log
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /etc/hostname
/dev/mapper/rl-root      46.9G      4.5G     42.4G  10% /etc/resolv.conf
shm                      64.0M         0     64.0M   0% /dev/shm
tmpfs                     7.5G     12.0K      7.5G   0% /var/run/secrets/kubernetes.io/serviceaccount
tmpfs                     3.8G         0      3.8G   0% /proc/acpi
tmpfs                    64.0M         0     64.0M   0% /proc/kcore
tmpfs                    64.0M         0     64.0M   0% /proc/keys
tmpfs                    64.0M         0     64.0M   0% /proc/timer_list
tmpfs                    64.0M         0     64.0M   0% /proc/sched_debug
tmpfs                     3.8G         0      3.8G   0% /proc/scsi
tmpfs                     3.8G         0      3.8G   0% /sys/firmware
/var/log # mount
overlay on / type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/605/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/728/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/728/work)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)
tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (ro,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (ro,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/rdma type cgroup (ro,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (ro,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/blkio type cgroup (ro,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (ro,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/devices type cgroup (ro,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/cpuset type cgroup (ro,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/pids type cgroup (ro,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/freezer type cgroup (ro,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/perf_event type cgroup (ro,nosuid,nodev,noexec,relatime,perf_event)
/dev/mapper/rl-root on /var/log type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/hosts type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /dev/termination-log type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/hostname type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/resolv.conf type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k)
tmpfs on /var/run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime,size=7847236k)
proc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/fs type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/irq type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)
tmpfs on /proc/acpi type tmpfs (ro,relatime)
tmpfs on /proc/kcore type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/keys type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/timer_list type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/sched_debug type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/scsi type tmpfs (ro,relatime)
tmpfs on /sys/firmware type tmpfs (ro,relatime)
/var/log # 
[root@master01 ~]# kubectl exec -it multi-cont-pod -c main-container -- shsidecarkubectl exec -it multi-cont-pod -c sidecar-container -- sh/bin/sh
# cat /usr/share/nginx/html/index.html
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
# apt-get update && apt-get install -y curl
0% [Working]            Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]
0% [1 InRelease 26.1 kB/116 kB 23%]                                   0% [Working]            Get:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]
            Get:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]
0% [3 InRelease 44.1 kB/44.1 kB 100%]                                     0% [Working]0% [Working]            Get:4 http://deb.debian.org/debian bullseye/main amd64 Packages [8184 kB]
0% [4 Packages 44.2 kB/8184 kB 1%]                                  0% [Working]0% [4 Packages store 0 B]0% [4 Packages store 0 B]0% [4 Packages store 0 B]                         Get:5 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [2596 B]
0% [4 Packages store 0 B]0% [4 Packages store 0 B]0% [4 Packages store 0 B]                         0% [Working]0% [5 Packages store 0 B]                         0% [Working]            Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
E: Release file for http://deb.debian.org/debian-security/dists/bullseye-security/InRelease is not valid yet (invalid for another 9min 32s). Updates for this repository will not be applied.
# ^[[A    apt-get update
0% [Working]            Hit:1 http://deb.debian.org/debian bullseye InRelease
0% [Working]            Get:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]
0% [Connecting to deb.debian.org]                                 Hit:3 http://deb.debian.org/debian bullseye-updates InRelease
                                 0% [Working]0% [Working]0% [Working]0% [Working]            Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
E: Release file for http://deb.debian.org/debian-security/dists/bullseye-security/InRelease is not valid yet (invalid for another 9min 17s). Updates for this repository will not be applied.
# ping google.com
/bin/sh: 4: ping: not found
# 
command terminated with exit code 127
[root@master01 ~]# kubectl exec -it multi-cont-pod -c sidecar-container -- /bin/shbash
root@multi-cont-pod:/# root@multi-cont-pod:/# apt-get update
0% [Working]            Hit:1 http://deb.debian.org/debian bullseye InRelease
0% [Working]            Get:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]
0% [Connecting to deb.debian.org]                                 Hit:3 http://deb.debian.org/debian bullseye-updates InRelease
                                 0% [Working]0% [Working]0% [Working]0% [Working]            Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
E: Release file for http://deb.debian.org/debian-security/dists/bullseye-security/InRelease is not valid yet (invalid for another 9min 0s). Updates for this repository will not be applied.
root@multi-cont-pod:/# cu
curl  cut   
root@multi-cont-pod:/# curl localhost
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
Hi I am from Main container
root@multi-cont-pod:/# 
exit
[root@master01 ~]# [root@master01 ~]# kubectl exec -it multi-cont-pod -c sidecar-container -- /bin/bashshmain-container -- shcurl 10.36.0.1kubectl get po -o wide
NAME             READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES
multi-cont-pod   2/2     Running   0          4m31s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get po --show-lables
error: unknown flag: --show-lables
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get po --show-lablesels
NAME             READY   STATUS    RESTARTS   AGE    LABELS
multi-cont-pod   2/2     Running   0          5m8s   <none>
[root@master01 ~]# kubectl get po --show-labelsleselskubectl set po multi-cont-pod -l  sidecar-containers a p p = w e b 
error: unknown shorthand flag: 'l' in -l
See 'kubectl set --help' for usage.
[root@master01 ~]# kubectl set po multi-cont-pod sidecar-container -l app=web -
error: unknown flag: --l
See 'kubectl set --help' for usage.
[root@master01 ~]# kubectl set po multi-cont-pod sidecar-container --l app=web -l
error: unknown shorthand flag: 'l' in -l
See 'kubectl set --help' for usage.
[root@master01 ~]# kubectl set po multi-cont-pod sidecar-container -l app=web -help
Configure application resources.

 These commands help you make changes to existing application resources.

Available Commands:
  env              Update environment variables on a pod template
  image            Update the image of a pod template
  resources        Update resource requests/limits on objects with pod templates
  selector         Set the selector on a resource
  serviceaccount   Update the service account of a resource
  subject          Update the user, group, or service account in a role binding or cluster role binding

Usage:
  kubectl set SUBCOMMAND [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl set po multi-cont-pod sidecar-container --helpenv --heplp
Configure application resources.

 These commands help you make changes to existing application resources.

Available Commands:
  env              Update environment variables on a pod template
  image            Update the image of a pod template
  resources        Update resource requests/limits on objects with pod templates
  selector         Set the selector on a resource
  serviceaccount   Update the service account of a resource
  subject          Update the user, group, or service account in a role binding or cluster role binding

Usage:
  kubectl set SUBCOMMAND [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
multi-cont-pod   2/2     Running   0          10m
[root@master01 ~]# kubectl label pods multi-cont-pod sidecar-container app=web
pod/multi-cont-pod labeled
Error from server (NotFound): pods "sidecar-container" not found
[root@master01 ~]# kubectl label pods multi-cont-pod sidecar-container app=webget podslabel pods multi-cont-pod sidecar-container app=web-c 
error: unknown shorthand flag: 'c' in -c
See 'kubectl label --help' for usage.
[root@master01 ~]# kubectl label pods multi-cont-pod -c sidecar-container app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=web app=webapp=web app=web app=websidecar-nginx-busybox
error: 'app' already has a value (web), and --overwrite is false
[root@master01 ~]# kubectl label pods multi-cont-pod  app=sidecar-nginx-busyboxsidecar-nginx-busyboxsidecar-nginx-busyboxsidecar-nginx-busyboxsidecar-nginx-busybox=true
pod/multi-cont-pod labeled
[root@master01 ~]# kubectl label pods multi-cont-pod  sidecar-nginx-busybox=trueapp=sidecar-nginx-busybox-c sidecar-container app=web-c  app=sidecar-nginx-busyboxsidecar-nginx-busybox=trueapp=sidecar-nginx-busybox-c sidecar-container app=webget podsset po multi-cont-pod sidecar-container env --helpget pods
NAME             READY   STATUS    RESTARTS   AGE
multi-cont-pod   2/2     Running   0          11m
[root@master01 ~]# kubectl get pods -o wide
NAME             READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
multi-cont-pod   2/2     Running   0          11m   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widelabel pods multi-cont-pod  sidecar-nginx-busybox=trueapp=sidecar-nginx-busybox-c sidecar-container app=webget podsset po multi-cont-pod sidecar-container env --help--helpl app=web -get po --show-labels
NAME             READY   STATUS    RESTARTS   AGE   LABELS
multi-cont-pod   2/2     Running   0          12m   app=web,sidecar-nginx-busybox=true
[root@master01 ~]# kubectl get po --show-labelsds -o widelabel pods multi-cont-pod  sidecar-nginx-busybox=true -
error: all resources must be specified before label changes: -
[root@master01 ~]# kubectl label pods multi-cont-pod  sidecar-nginx-busybox=true --
error: invalid label value: "sidecar-nginx-busybox=true-": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')
[root@master01 ~]# kubectl label pods multi-cont-pod  sidecar-nginx-busybox=true------
pod/multi-cont-pod unlabeled
[root@master01 ~]# kubectl label pods multi-cont-pod  sidecar-nginx-busybox-app-
pod/multi-cont-pod unlabeled
[root@master01 ~]# kubectl label pods multi-cont-pod  app-sidecar-nginx-busybox-=true- -get po --show-labels
NAME             READY   STATUS    RESTARTS   AGE   LABELS
multi-cont-pod   2/2     Running   0          12m   <none>
[root@master01 ~]# kubectl delete pods -all -A
error: a resource cannot be retrieved by name across all namespaces
[root@master01 ~]# kubectl delete pods all -A-A
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl delete pods -Aall -Amulti-cont-pod
pod "multi-cont-pod" deleted
[root@master01 ~]# kubectl run gnginx-dev1 --image=nginx --restart =Never --lables=env=dev
error: unknown flag: --lables
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run nginx-dev1 --image=nginx --restart=Never --lables=env=deves=env=devls=env=dev
pod/nginx-dev1 created
[root@master01 ~]# kubectl run nginx-dev1 --image=nginx --restart=Never --labels=env=dev2
pod/nginx-dev2 created
[root@master01 ~]# kubectl run nginx-dev2 --image=nginx --restart=Never --labels=env=dev3
pod/nginx-dev3 created
[root@master01 ~]# kubectl run nginx-dev3 --image=nginx --restart=Never --labels=env=devprod4
pod/nginx-dev4 created
[root@master01 ~]# kubectl run nginx-dev4 --image=nginx --restart=Never --labels=env=prod45
pod/nginx-dev5 created
[root@master01 ~]# kubectl get pods --levels
error: unknown flag: --levels
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods --levelsvelsavels
error: unknown flag: --lavels
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods --lavelselsbels
error: unknown flag: --labels
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE   LABELS
nginx-dev1   1/1     Running   0          82s   env=dev
nginx-dev2   1/1     Running   0          74s   env=dev
nginx-dev3   1/1     Running   0          69s   env=dev
nginx-dev4   1/1     Running   0          57s   env=prod
nginx-dev5   1/1     Running   0          50s   env=prod
[root@master01 ~]# kubectl get pods --show-labels=env=dev
error: invalid argument "env=dev" for "--show-labels" flag: strconv.ParseBool: parsing "env=dev": invalid syntax
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods --show-labels=env=dev=dev=dev=devdev
error: invalid argument "dev" for "--show-labels" flag: strconv.ParseBool: parsing "dev": invalid syntax
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods --show-labels=devl env=dev
NAME         READY   STATUS    RESTARTS   AGE
nginx-dev1   1/1     Running   0          2m6s
nginx-dev2   1/1     Running   0          118s
nginx-dev3   1/1     Running   0          113s
[root@master01 ~]# kubectl get pods -l env=dev --showl-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx-dev1   1/1     Running   0          2m40s   env=dev
nginx-dev2   1/1     Running   0          2m32s   env=dev
nginx-dev3   1/1     Running   0          2m27s   env=dev
[root@master01 ~]# kubectl get pods -l env=dev --show-labels
NAME         READY   STATUS    RESTARTS   AGE
nginx-dev1   1/1     Running   0          3m10s
nginx-dev2   1/1     Running   0          3m2s
nginx-dev3   1/1     Running   0          2m57s
nginx-dev4   1/1     Running   0          2m45s
nginx-dev5   1/1     Running   0          2m38s
[root@master01 ~]# kubectl get pods -l env envL env
NAME         READY   STATUS    RESTARTS   AGE     ENV
nginx-dev1   1/1     Running   0          3m21s   dev
nginx-dev2   1/1     Running   0          3m13s   dev
nginx-dev3   1/1     Running   0          3m8s    dev
nginx-dev4   1/1     Running   0          2m56s   prod
nginx-dev5   1/1     Running   0          2m49s   prod
[root@master01 ~]# kubectl get pods -L envlenv=dev,prod'env=dev,proddev,prod dev,prodidev,prodndev,prod dev,prod(dev,prod)'
NAME         READY   STATUS    RESTARTS   AGE
nginx-dev1   1/1     Running   0          4m9s
nginx-dev2   1/1     Running   0          4m1s
nginx-dev3   1/1     Running   0          3m56s
nginx-dev4   1/1     Running   0          3m44s
nginx-dev5   1/1     Running   0          3m37s
[root@master01 ~]# kubectl get pods -l 'env in (dev,prod)'L
NAME         READY   STATUS    RESTARTS   AGE     ENV IN (DEV   PROD)
nginx-dev1   1/1     Running   0          4m36s                 
nginx-dev2   1/1     Running   0          4m28s                 
nginx-dev3   1/1     Running   0          4m23s                 
nginx-dev4   1/1     Running   0          4m11s                 
nginx-dev5   1/1     Running   0          4m4s                  
[root@master01 ~]# kubectl get pods -L 'env in (dev,prod)'l --show-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx-dev1   1/1     Running   0          5m      env=dev
nginx-dev2   1/1     Running   0          4m52s   env=dev
nginx-dev3   1/1     Running   0          4m47s   env=dev
nginx-dev4   1/1     Running   0          4m35s   env=prod
nginx-dev5   1/1     Running   0          4m28s   env=prod
[root@master01 ~]# kubectl get pods -l 'env in (dev,prod)' --show-labelsL 'env in (dev,prod)'lLl 'env in (dev,prod)' --show-labelskubectl labels pod nginx-dev1 env=uat --overidewideridete
error: unknown command "labels" for "kubectl"

Did you mean this?
label
[root@master01 ~]# kubectl labels pod nginx-dev1 env=uat --overwrite
pod/nginx-dev1 labeled
[root@master01 ~]# kubectl label pod nginx-dev1 env=uat --overwritesget pods -l 'env in (dev,prod)' --show-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx-dev2   1/1     Running   0          6m13s   env=dev
nginx-dev3   1/1     Running   0          6m8s    env=dev
nginx-dev4   1/1     Running   0          5m56s   env=prod
nginx-dev5   1/1     Running   0          5m49s   env=prod
[root@master01 ~]# kubectl get pods -l 'env in (dev,prod)' --show-labels,uat
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx-dev1   1/1     Running   0          6m30s   env=uat
nginx-dev2   1/1     Running   0          6m22s   env=dev
nginx-dev3   1/1     Running   0          6m17s   env=dev
nginx-dev4   1/1     Running   0          6m5s    env=prod
nginx-dev5   1/1     Running   0          5m58s   env=prod
[root@master01 ~]# kubectl get pods -l 'env in (dev,prod,uat)' --show-labelslabel pod nginx-dev1 env=uat --overwrite-
pod/nginx-dev1 unlabeled
[root@master01 ~]# kubectl label pod nginx-dev1 env- env- env- env- env- env- env- env- env- env- env-
error: resource(s) were provided, but no name was specified
[root@master01 ~]# kubectl get pods -l env-
Error from server (BadRequest): Unable to find "/v1, Resource=pods" that match label selector "env-", field selector "": unable to parse requirement: <nil>: Invalid value: "env-": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]')
[root@master01 ~]# kubectl get pods -l env- env-L env-
NAME         READY   STATUS    RESTARTS   AGE     ENV-
nginx-dev1   1/1     Running   0          7m31s   
nginx-dev2   1/1     Running   0          7m23s   
nginx-dev3   1/1     Running   0          7m18s   
nginx-dev4   1/1     Running   0          7m6s    
nginx-dev5   1/1     Running   0          6m59s   
[root@master01 ~]# kubectl get pods -L env-llabel pod get pods -lLkubectl get pods -L env- --show-labels
NAME         READY   STATUS    RESTARTS   AGE     ENV-   LABELS
nginx-dev1   1/1     Running   0          7m43s          <none>
nginx-dev2   1/1     Running   0          7m35s          env=dev
nginx-dev3   1/1     Running   0          7m30s          env=dev
nginx-dev4   1/1     Running   0          7m18s          env=prod
nginx-dev5   1/1     Running   0          7m11s          env=prod
[root@master01 ~]# kubectl get pods -L env- --show-labelsnginx-dev1{}1..1..3
NAME         READY   STATUS    RESTARTS   AGE
nginx-dev1   1/1     Running   0          8m28s
nginx-dev2   1/1     Running   0          8m20s
nginx-dev3   1/1     Running   0          8m15s
Error from server (NotFound): pods "env-" not found
[root@master01 ~]# kubectl get pods nginx-dev{1..3} env-25
NAME         READY   STATUS    RESTARTS   AGE
nginx-dev2   1/1     Running   0          8m36s
nginx-dev3   1/1     Running   0          8m31s
nginx-dev4   1/1     Running   0          8m19s
nginx-dev5   1/1     Running   0          8m12s
Error from server (NotFound): pods "env-" not found
[root@master01 ~]# kubectl get pods nginx-dev{2..5} env-12
Error from server (NotFound): pods "nginx-dev{1.2}" not found
Error from server (NotFound): pods "env-" not found
[root@master01 ~]# kubectl get pods nginx-dev{1.2} env-2..51..3-L env- --show-labels --show-labelsllabel pod nginx-dev1 env-get pods -l 'env in (dev,prod,uat)' --show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labelsshow-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx-dev1   1/1     Running   0          9m21s   <none>
nginx-dev2   1/1     Running   0          9m13s   env=dev
nginx-dev3   1/1     Running   0          9m8s    env=dev
nginx-dev4   1/1     Running   0          8m56s   env=prod
nginx-dev5   1/1     Running   0          8m49s   env=prod
[root@master01 ~]# kubectl labels pods nginx-dev{1.2} env-
error: unknown command "labels" for "kubectl"

Did you mean this?
label
[root@master01 ~]# kubectl labels pods nginx-dev{1.2} env-
Error from server (NotFound): pods "nginx-dev{1.2}" not found
[root@master01 ~]# kubectl label pods nginx-dev{1.2} env-.
label "env" not found.
pod/nginx-dev1 labeled
pod/nginx-dev2 unlabeled
[root@master01 ~]# kubectl label pods nginx-dev{1..2} env-3
label "env" not found.
pod/nginx-dev1 labeled
label "env" not found.
pod/nginx-dev2 labeled
pod/nginx-dev3 unlabeled
[root@master01 ~]# kubectl label pods nginx-dev{1..3} env-2
label "env" not found.
pod/nginx-dev2 labeled
label "env" not found.
pod/nginx-dev3 labeled
[root@master01 ~]# kubectl label pods nginx-dev{2..3} env-
Error from server (NotFound): pods "nginx-dev{2.3}" not found
[root@master01 ~]# kubectl label pods nginx-dev{2.3} env-1.2
label "env" not found.
pod/nginx-dev1 labeled
label "env" not found.
pod/nginx-dev2 labeled
[root@master01 ~]# kubectl label pods nginx-dev{1..2} env-2.3.12sget pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx-dev1   1/1     Running   0          10m     <none>
nginx-dev2   1/1     Running   0          10m     <none>
nginx-dev3   1/1     Running   0          10m     <none>
nginx-dev4   1/1     Running   0          10m     env=prod
nginx-dev5   1/1     Running   0          9m56s   env=prod
[root@master01 ~]# kubectl get pods --show-labelslabel pods nginx-dev{1..2} env-
label "env" not found.
pod/nginx-dev1 labeled
label "env" not found.
pod/nginx-dev2 labeled
[root@master01 ~]# kubectl label pods nginx-dev{1..2} env-get pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE   LABELS
nginx-dev1   1/1     Running   0          10m   <none>
nginx-dev2   1/1     Running   0          10m   <none>
nginx-dev3   1/1     Running   0          10m   <none>
nginx-dev4   1/1     Running   0          10m   env=prod
nginx-dev5   1/1     Running   0          10m   env=prod
[root@master01 ~]# kubectl get pods --show-labelslabel pods nginx-dev{1..2} env-45
pod/nginx-dev4 unlabeled
pod/nginx-dev5 unlabeled
[root@master01 ~]# kubectl label pods nginx-dev{4..5} env-get pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE   LABELS
nginx-dev1   1/1     Running   0          10m   <none>
nginx-dev2   1/1     Running   0          10m   <none>
nginx-dev3   1/1     Running   0          10m   <none>
nginx-dev4   1/1     Running   0          10m   <none>
nginx-dev5   1/1     Running   0          10m   <none>
[root@master01 ~]# kubectl label pods nginx-dev{1..2}}5} env=nginx
pod/nginx-dev1 labeled
pod/nginx-dev2 labeled
pod/nginx-dev3 labeled
pod/nginx-dev4 labeled
pod/nginx-dev5 labeled
[root@master01 ~]# kubectl label pods nginx-dev{1..5} env=nginxget pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE   LABELS
nginx-dev1   1/1     Running   0          11m   env=nginx
nginx-dev2   1/1     Running   0          11m   env=nginx
nginx-dev3   1/1     Running   0          11m   env=nginx
nginx-dev4   1/1     Running   0          11m   env=nginx
nginx-dev5   1/1     Running   0          11m   env=nginx
[root@master01 ~]# kubectl get pods --show-labelslabel pods nginx-dev{1..5} env=nginx-env
error: unknown shorthand flag: 'e' in -env
See 'kubectl label --help' for usage.
[root@master01 ~]# kubectl label pods nginx-dev{1..5} -envenv-
pod/nginx-dev1 unlabeled
pod/nginx-dev2 unlabeled
pod/nginx-dev3 unlabeled
pod/nginx-dev4 unlabeled
pod/nginx-dev5 unlabeled
[root@master01 ~]# kubectl label pods nginx-dev{1..5} env--envget pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE   LABELS
nginx-dev1   1/1     Running   0          12m   <none>
nginx-dev2   1/1     Running   0          12m   <none>
nginx-dev3   1/1     Running   0          12m   <none>
nginx-dev4   1/1     Running   0          12m   <none>
nginx-dev5   1/1     Running   0          11m   <none>
[root@master01 ~]# kubectl get pods --show-labelslabel pods nginx-dev{1..5} env--envenv--envget pods --show-labelslabel pods nginx-dev{1..5} env=nginxaenv=nginxpenv=nginxpenv=nginx=env=nginxnginxnginxginxnginx
pod/nginx-dev1 labeled
pod/nginx-dev2 labeled
pod/nginx-dev3 labeled
pod/nginx-dev4 labeled
pod/nginx-dev5 labeled
[root@master01 ~]# kubectl label pods nginx-dev{1..5} app=nginxget pods --show-labels
NAME         READY   STATUS    RESTARTS   AGE   LABELS
nginx-dev1   1/1     Running   0          12m   app=nginx
nginx-dev2   1/1     Running   0          12m   app=nginx
nginx-dev3   1/1     Running   0          12m   app=nginx
nginx-dev4   1/1     Running   0          12m   app=nginx
nginx-dev5   1/1     Running   0          12m   app=nginx
[root@master01 ~]# kuecbectl get modenodes
NAME                           STATUS   ROLES           AGE     VERSION
master01.airtel.internal.lab   Ready    control-plane   4d17h   v1.25.0
worker01.airtel.internal.lab   Ready    <none>          4d17h   v1.25.0
worker02.airtel.internal.lab   Ready    <none>          4d17h   v1.25.0
[root@master01 ~]# kubectl get nodes -L
error: flag needs an argument: 'L' in -L
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get nodes -Ll
error: flag needs an argument: 'l' in -l
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get nodes -l
NAME                           STATUS   ROLES           AGE     VERSION
master01.airtel.internal.lab   Ready    control-plane   4d17h   v1.25.0
worker01.airtel.internal.lab   Ready    <none>          4d17h   v1.25.0
worker02.airtel.internal.lab   Ready    <none>          4d17h   v1.25.0
[root@master01 ~]# kubectl label node worker02.airtel.internal.lab nodeName=nginxnode#
[root@master01 ~]# kubectl describe node worker02.airtel.internal.lab |grep -i nodeName
[root@master01 ~]# kubectl describe node worker02.airtel.internal.lab |grep -i nodeName
Name:               worker02.airtel.internal.lab
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=worker02.airtel.internal.lab
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 11 Sep 2022 20:55:54 +0530
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  worker02.airtel.internal.lab
  AcquireTime:     <unset>
  RenewTime:       Fri, 16 Sep 2022 14:18:29 +0530
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sun, 11 Sep 2022 20:48:28 +0530   Sun, 11 Sep 2022 20:48:28 +0530   WeaveIsUp                    Weave pod has set this
  MemoryPressure       False   Fri, 16 Sep 2022 14:16:13 +0530   Sun, 11 Sep 2022 20:47:53 +0530   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 16 Sep 2022 14:16:13 +0530   Sun, 11 Sep 2022 20:47:53 +0530   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 16 Sep 2022 14:16:13 +0530   Sun, 11 Sep 2022 20:47:53 +0530   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 16 Sep 2022 14:16:13 +0530   Sun, 11 Sep 2022 20:48:34 +0530   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.16.102.24
  Hostname:    worker02.airtel.internal.lab
Capacity:
  cpu:                4
  ephemeral-storage:  49197600Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7949644Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  45340508085
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7847244Ki
  pods:               110
System Info:
  Machine ID:                 eb4d5930d89b402d940dcef33ef62837
  System UUID:                3f000f42-8bec-b9b8-fe8f-1c28556c86d6
  Boot ID:                    79403d8e-8dcf-4e0f-bec0-be06000f7a1b
  Kernel Version:             4.18.0-372.9.1.el8.x86_64
  OS Image:                   Rocky Linux 8.6 (Green Obsidian)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.8
  Kubelet Version:            v1.25.0
  Kube-Proxy Version:         v1.25.0
Non-terminated Pods:          (5 in total)
  Namespace                   Name                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                               ------------  ----------  ---------------  -------------  ---
  default                     nginx-dev3                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
  default                     nginx-dev4                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-proxy-bk8v6                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h
  kube-system                 metrics-server-55dd4998bd-qr2n9    100m (2%)     0 (0%)      200Mi (2%)       0 (0%)         4h7m
  kube-system                 weave-net-mdv4h                    100m (2%)     0 (0%)      200Mi (2%)       0 (0%)         4d17h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                200m (5%)   0 (0%)
  memory             400Mi (5%)  0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>
[root@master01 ~]# kubectl describe node worker02.airtel.internal.lab |grep -i nodeName#kubectl label node worker02.airtel.internal.lab nodeName=nginxnode
node/worker02.airtel.internal.lab labeled
[root@master01 ~]# kubectl label node worker02.airtel.internal.lab nodeName=nginxnodedescribe node worker02.airtel.internal.lablabel node worker02.airtel.internal.lab nodeName=nginxnodedescribe node worker02.airtel.internal.lab |grep -i nodeName
                    nodeName=nginxnode
[root@master01 ~]# kubectl qrun run ngnnginx-node --image=nginx --restart=Never --label--dry-run=cleient -o yaml > nodnginxNode.yaml
[root@master01 ~]# vim nginxNode.yaml 
"nginxNode.yaml" 15L, 231C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   2dd  
~                                                                                                                                                                        12,3Alldd  
~                                                                                                                                                                        12,3All~@k   3~@k   ~@k   2~@k   1~@k   4~@k   5o -- INSERT --12,5Top12,5All43nodeSelector15  nodeSelector:67nginxnode26^[  12,25All::wq!"nginxNode.yaml" 14L, 213C written
[root@master01 ~]# kubectl cget pods
NAME         READY   STATUS    RESTARTS   AGE
nginx-dev1   1/1     Running   0          17m
nginx-dev2   1/1     Running   0          17m
nginx-dev3   1/1     Running   0          17m
nginx-dev4   1/1     Running   0          17m
nginx-dev5   1/1     Running   0          17m
[root@master01 ~]# kucecectubectl create -f nodes.txt ginxNode.yaml 
Error from server (BadRequest): error when creating "nginxNode.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal string into Go struct field PodSpec.spec.nodeSelector of type map[string]string
[root@master01 ~]# kubectl create -f nginxNode.yaml get podsvim nginxNode.yaml 
"nginxNode.yaml" 14L, 213C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
  nodeSelector: nginxnode
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        12,25All~@k   4~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9 ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   3i -- INSERT --12,3All nodeSelector: nginxnode4 nodeSelector: nginxnode5^[  12,4All::wq!"nginxNode.yaml" 14L, 215C written
[root@master01 ~]# vim nginxNode.yaml kubectl create -f
Error from server (BadRequest): error when creating "nginxNode.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.containers[0].nodeSelector"
[root@master01 ~]# kubectl create -f nginxNode.yaml vim
"nginxNode.yaml" 14L, 215C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    nodeSelector: nginxnode
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        12,4All~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   1,15o -- INSERT --12,5Top12,5All43nodeName:12109 876512109,2 12310,3122123109,3 2110,1123232^[  12,2All~@k   3dd  
~                                                                                                                                                                        12,5Alli -- INSERT --12,5AllnodeSelector: nginxnode4nodeSelector: nginxnode345678910123456787nginxnode13,3All nginxnode4 nginxnode5nodeNamenginxnode13:nginxnode4    nodeName: nginxnode5^[  13,14All::wq!"nginxNode.yaml" 15L, 228C written
[root@master01 ~]# vim nginxNode.yaml kubectl create -f
pod/nginx created
[root@master01 ~]# kubectl get pods -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
nginx        1/1     Running   0          11s   10.44.0.4   worker02.airtel.internal.lab   <none>           <none>
nginx-dev1   1/1     Running   0          22m   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
nginx-dev2   1/1     Running   0          22m   10.36.0.2   worker01.airtel.internal.lab   <none>           <none>
nginx-dev3   1/1     Running   0          22m   10.44.0.1   worker02.airtel.internal.lab   <none>           <none>
nginx-dev4   1/1     Running   0          21m   10.44.0.3   worker02.airtel.internal.lab   <none>           <none>
nginx-dev5   1/1     Running   0          21m   10.36.0.3   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widecreate -f nginxNode.yaml vimkubectl create -fvimkubectl create -fvimkubectl create -fvimkubectl create -fget pods -o widekubectl get pods -o widenginx |grep -i NodeSelectors
[root@master01 ~]# kubectl get pods nginx |grep -i NodeSelectors-Selectors
[root@master01 ~]# kubectl get pods nginx |grep -i Node-Selectors
[root@master01 ~]# kubectl get pods nginx |grep -i Node-SelectorsSelectorsdescribe
[root@master01 ~]# kubectl describe pods nginx |grep -i NodeSelectors
Node:             worker02.airtel.internal.lab/172.16.102.24
Node-Selectors:              nodeName=nginxnode
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
[root@master01 ~]# kubectl describe pods nginx |grep -i NodeSelectors-Selectors
Node-Selectors:              nodeName=nginxnode
[root@master01 ~]# kubectl describe pods nginx |grep -i Node-SelectorsSelectorsget pods nginx |grep -i Node--show-levels
error: unknown flag: --show-levels
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods nginx --show-levels
error: unknown flag: --show-level
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods nginx --show-levellevegetvelbvel
error: unknown flag: --show-lbvel
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get pods nginx --show-lbvellabels
NAME    READY   STATUS    RESTARTS   AGE     LABELS
nginx   1/1     Running   0          2m30s   run=nginx
[root@master01 ~]# kubectl get pods nginx --show-labelsdescribe|grep Labels
Labels:           run=nginx
[root@master01 ~]# kubectl annotate pod nginx-dev{}1}.}.}3} name=webapp
pod/nginx-dev1 annotated
pod/nginx-dev2 annotated
pod/nginx-dev3 annotated
[root@master01 ~]# kubectl annotate pod nginx-dev{1..3} name=webappdescribe pods nginx |grep Labelsget pods nginx --show-labels
NAME    READY   STATUS    RESTARTS   AGE     LABELS
nginx   1/1     Running   0          3m34s   run=nginx
[root@master01 ~]# kubectl get pods nginx --show-labels--show-labels--show-labels--show-labels--show-labels--show-labels--show-labels
NAME         READY   STATUS    RESTARTS   AGE     LABELS
nginx        1/1     Running   0          3m44s   run=nginx
nginx-dev1   1/1     Running   0          25m     app=nginx
nginx-dev2   1/1     Running   0          25m     app=nginx
nginx-dev3   1/1     Running   0          25m     app=nginx
nginx-dev4   1/1     Running   0          25m     app=nginx
nginx-dev5   1/1     Running   0          25m     app=nginx
[root@master01 ~]# kubectl get pods --show-labelskubectl get pods --show-labelsnginx --show-labelsannotate pod nginx-dev{1..3} name=webappdescribewebappwebappwebappwebappwebapp|webappgwebapprwebappewebapppwebapp webapp-webappiwebapp webappannotations
Annotations:      name: webapp
Annotations:      name: webapp
Annotations:      name: webapp
[root@master01 ~]# kubectl describe pod nginx-dev{1..3} |grep -i annotationsget pods --show-labelsnginx --show-labelsannotate pod nginx-dev{1..3} name=webapp-
pod/nginx-dev1 annotated
pod/nginx-dev2 annotated
pod/nginx-dev3 annotated
[root@master01 ~]# kubectl annotate pod nginx-dev{1..3} name-describe pod nginx-dev{1..3} |grep -i annotations
Annotations:      <none>
Annotations:      <none>
Annotations:      <none>
[root@master01 ~]# kubectl delete pods all -A--all
pod "nginx" deleted
pod "nginx-dev1" deleted
pod "nginx-dev2" deleted
pod "nginx-dev3" deleted
pod "nginx-dev4" deleted
pod "nginx-dev5" deleted
[root@master01 ~]# kubectl get pods
No resources found in default namespace.
[root@master01 ~]# kubectl run deplyoyment webapp --image=ngnx=nginx --restart=Never --replica=5 --dr-runty-run=client -o yaml > webapp-replica.yaml
error: unknown flag: --replica
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run deployment webapp --image=nginx --restart=Never --replica=5 --dry-run=client -o yaml > webapp-replica.yaml
[root@master01 ~]# vim web
webapp-replica.yaml   web-project-268.yaml  
[root@master01 ~]# vim webapp-replica.yaml 
"webapp-replica.yaml" 17L, 269C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: deployment
  name: deployment
spec:
  containers:
  - args:
    - webapp
    image: nginx
    name: deployment
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8o -- INSERT --9,9Top9,9All87654321  replicas: 34354^[  9,13All::wq!"webapp-replica.yaml" 18L, 283C written
[root@master01 ~]# kubectl create -f webapp-replica.yaml 
Error from server (BadRequest): error when creating "webapp-replica.yaml": Pod in version "v1" cannot be handled as a Pod: strict decoding error: unknown field "spec.replicas"
[root@master01 ~]# kubectl create -f webapp-replica.yaml vim
"webapp-replica.yaml" 18L, 283C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: deployment
  name: deployment
spec:
  replicas: 5
  containers:
  - args:
    - webapp
    image: nginx
    name: deployment
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        9,13All~@k   8,5 ~@k   7,13~@k   6~@k   5,9 ~@k   4,13~@k   3,9 ~@k   2i -- INSERT --2,9All109 87Deployment17^[  2,16All~@k   3,9 ~@k   4,16~@k   5,9 ~@k   6,16i -- INSERT --6,16All892019876543210webapp677898765432109 webapp158,6 9,1410,141,10^[  11,9Alldd  
~                                                                                                                                                                        11,5Alldd  
~                                                                                                                                                                        11,5All::wq!"webapp-replica.yaml" 16L, 259C written
[root@master01 ~]# vim webapp-replica.yaml kubectl create -fvimkubectl run deployment webapp --image=nginx --restart=Never --dry-run=client -o yaml > webapp-replica.yamlcreate
error: unknown flag: --restart
See 'kubectl create deployment --help' for usage.
[root@master01 ~]# kubectl create deployment webapp --image=nginx --restart=Never --dry-run=client -o yaml > webapp-replica.yaml
[root@master01 ~]# kubectl create deployment webapp --image=nginx --dry-run=client -o yaml > webapp-replica.yamlrestart=Never --vim webapp-replica.yaml 
"webapp-replica.yaml" 24L, 388C  apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: webapp
  name: webapp
spec:
  replicas: 1
  selector:
    matchLabels:app: webapp
  strategy: {}
  template:
    metadata:creationTimestamp: nulllabels:app: webapp
    spec:containers:- image: nginxname: nginxresources: {}
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        11,5All~@k   0~@k   9,5 ~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   R -- REPLACE --9,13All54^[  9,13All::wq!"webapp-replica.yaml" 24L, 388C written
[root@master01 ~]# kubectl create -f webapp-replica.yaml 
deployment.apps/webapp created
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS              RESTARTS   AGE
webapp-6684ccd7b8-4mfdd   0/1     ContainerCreating   0          3s
webapp-6684ccd7b8-4vh4m   1/1     Running             0          3s
webapp-6684ccd7b8-78pjg   1/1     Running             0          3s
webapp-6684ccd7b8-gjgd9   0/1     ContainerCreating   0          3s
webapp-6684ccd7b8-tgn6t   0/1     ContainerCreating   0          3s
[root@master01 ~]# kubectl get podsrs
NAME                DESIRED   CURRENT   READY   AGE
webapp-6684ccd7b8   5         5         5       9s
[root@master01 ~]# kubectl get rspods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-6684ccd7b8-4mfdd   1/1     Running   0          15s
webapp-6684ccd7b8-4vh4m   1/1     Running   0          15s
webapp-6684ccd7b8-78pjg   1/1     Running   0          15s
webapp-6684ccd7b8-gjgd9   1/1     Running   0          15s
webapp-6684ccd7b8-tgn6t   1/1     Running   0          15s
[root@master01 ~]# kubectl get podsrspodscreate -f webapp-replica.yaml vimkubectl create deployment webapp --image=nginx --dry-run=client -o yaml > webapp-replica.yamlrestart=Never --vim webapp-replica.yaml kubectl create -fvimkubectl create deployment webapp --image=nginx --restart=Never --dry-run=client -o yaml > webapp-replica.yamlvim webapp-replica.yaml kubectl create -fget podsrspodskubectl get deploy webapp --show-labels
NAME     READY   UP-TO-DATE   AVAILABLE   AGE   LABELS
webapp   5/5     5            5           52s   app=webapp
[root@master01 ~]# kubectl get deploy webapp --show-labels-o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2022-09-16T09:17:07Z"
  generation: 1
  labels:
    app: webapp
  name: webapp
  namespace: default
  resourceVersion: "580481"
  uid: 6950c2ed-fe5a-4fa7-a48f-462b3f609c75
spec:
  progressDeadlineSeconds: 600
  replicas: 5
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: webapp
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: webapp
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 5
  conditions:
  - lastTransitionTime: "2022-09-16T09:17:13Z"
    lastUpdateTime: "2022-09-16T09:17:13Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2022-09-16T09:17:07Z"
    lastUpdateTime: "2022-09-16T09:17:15Z"
    message: ReplicaSet "webapp-6684ccd7b8" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 5
  replicas: 5
  updatedReplicas: 5
[root@master01 ~]# kubectl get deploy webapp -o yamltop
error: unknown command "deploy webapp"
See 'kubectl top -h' for help and examples
[root@master01 ~]# kubectl top deploy webapp 
error: unknown command "webapp"
See 'kubectl top -h' for help and examples
[root@master01 ~]# kubectl top webapp 
Display Resource (CPU/Memory) usage.

 The top command allows you to see the resource consumption for nodes or pods.

 This command requires Metrics Server to be correctly configured and working on the server.

Available Commands:
  node          Display resource (CPU/memory) usage of nodes
  pod           Display resource (CPU/memory) usage of pods

Usage:
  kubectl top [flags] [options]

Use "kubectl <command> --help" for more information about a given command.
Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl topkubectl get pods -l app=webapp
NAME                      READY   STATUS    RESTARTS   AGE
webapp-6684ccd7b8-4mfdd   1/1     Running   0          2m37s
webapp-6684ccd7b8-4vh4m   1/1     Running   0          2m37s
webapp-6684ccd7b8-78pjg   1/1     Running   0          2m37s
webapp-6684ccd7b8-gjgd9   1/1     Running   0          2m37s
webapp-6684ccd7b8-tgn6t   1/1     Running   0          2m37s
[root@master01 ~]# kubectl get pods -l app=webapp app=webappL app=webapp
NAME                      READY   STATUS    RESTARTS   AGE     APP=WEBAPP
webapp-6684ccd7b8-4mfdd   1/1     Running   0          2m55s   
webapp-6684ccd7b8-4vh4m   1/1     Running   0          2m55s   
webapp-6684ccd7b8-78pjg   1/1     Running   0          2m55s   
webapp-6684ccd7b8-gjgd9   1/1     Running   0          2m55s   
webapp-6684ccd7b8-tgn6t   1/1     Running   0          2m55s   
[root@master01 ~]# kubectl sca;ele --help
Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is
guaranteed that the precondition holds true when the scale is sent to the server.

Examples:
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in "foo.yaml" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/foo rc/bar rc/baz
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web

Options:
    --all=false:
Select all resources in the namespace of the specified resource types

    --allow-missing-template-keys=true:
If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to
golang and jsonpath output formats.

    --current-replicas=-1:
Precondition for current size. Requires that the current size of the resource match this value in order to
scale. -1 (default) for no condition.

    --dry-run='none':
Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
sending it. If server strategy, submit server-side request without persisting the resource.

    -f, --filename=[]:
Filename, directory, or URL to files identifying the resource to set a new size

    -k, --kustomize='':
Process the kustomization directory. This flag can't be used together with -f or -R.

    -o, --output='':
Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath,
jsonpath-as-json, jsonpath-file).

    -R, --recursive=false:
Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests
organized within the same directory.

    --replicas=0:
The new desired number of replicas. Required.

    --resource-version='':
Precondition for resource version. Requires that the current resource version match this value in order to
scale.

    -l, --selector='':
Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
objects must satisfy all of the specified label constraints.

    --show-managed-fields=false:
If true, keep the managedFields when printing objects in JSON or YAML format.

    --template='':
Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format
is golang templates [http://golang.org/pkg/text/template/#pkg-overview].

    --timeout=0s:
The length of time to wait before giving up on a scale operation, zero means don't wait. Any other values
should contain a corresponding time unit (e.g. 1s, 2m, 3h).

Usage:
  kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
[options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
[root@master01 ~]# kubectl scale --helpkubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-6684ccd7b8-4mfdd   1/1     Running   0          4m21s
webapp-6684ccd7b8-4vh4m   1/1     Running   0          4m21s
webapp-6684ccd7b8-78pjg   1/1     Running   0          4m21s
webapp-6684ccd7b8-gjgd9   1/1     Running   0          4m21s
webapp-6684ccd7b8-tgn6t   1/1     Running   0          4m21s
[root@master01 ~]# kubectl scale --current-replicas=2 --replicas=3 deployment/mysql/webapp5 deployment/webapp2 deployment/webapp0 deployment/webapp
deployment.apps/webapp scaled
[root@master01 ~]# kubectl scale --current-replicas=5 --replicas=20 deployment/webappget podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Fri Sep 16 14:51:47 2022NAMEREADY   STATUSRESTARTS   AGEwebapp-6684ccd7b8-27ptp   0/1     ContainerCreating   04swebapp-6684ccd7b8-2tvfp   0/1     ContainerCreating   04swebapp-6684ccd7b8-4d64x   0/1     ContainerCreating   04swebapp-6684ccd7b8-4mfdd   1/1     Running04m40swebapp-6684ccd7b8-4vh4m   1/1     Running04m40swebapp-6684ccd7b8-78pjg   1/1     Running04m40swebapp-6684ccd7b8-d7qpb   0/1     ContainerCreating   04swebapp-6684ccd7b8-dtcmn   0/1     ContainerCreating   04swebapp-6684ccd7b8-gjgd9   1/1     Running04m40swebapp-6684ccd7b8-gsgnk   0/1     ContainerCreating   04swebapp-6684ccd7b8-hgdtv   0/1     ContainerCreating   04swebapp-6684ccd7b8-lhrsx   0/1     ContainerCreating   04swebapp-6684ccd7b8-n2kfc   0/1     ContainerCreating   04swebapp-6684ccd7b8-qphgc   0/1     ContainerCreating   04swebapp-6684ccd7b8-sq58h   0/1     ContainerCreating   04swebapp-6684ccd7b8-sxkjn   1/1     Running04swebapp-6684ccd7b8-tgn6t   1/1     Running04m40swebapp-6684ccd7b8-tsmd2   0/1     ContainerCreating   04swebapp-6684ccd7b8-vlb2t   0/1     ContainerCreating   04swebapp-6684ccd7b8-vsk6n   0/1     ContainerCreating   04s9666m422266m426666666m42661  Running          652999m455599m4591  Running          99991  Running          99m45999411s11s11s77711s1  Running          11s711s11s1  Running          11s11s11s11s11s711s11s11s63334999334933331  Running          333491  Running          333851  Running          55515151555155555555151  Running          552:00777533377531  Running          7771  Running          77775377721  Running          991  Running          95555995599999995599940 21s0 21s0 21s0 4m57s0 4m57s0 4m57s1  Running   0          21s0 21s0 4m57s0 21s0 21s0 21s0 21s0 21s0 21s0 21s0 4m57s0 21s0 21s0 21s[root@master01 ~]# watch kubectl get pods |wc -l

[root@master01 ~]# watch kubectl get pods |wc -l
21
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-6684ccd7b8-27ptp   1/1     Running   0          48s
webapp-6684ccd7b8-2tvfp   1/1     Running   0          48s
webapp-6684ccd7b8-4d64x   1/1     Running   0          48s
webapp-6684ccd7b8-4mfdd   1/1     Running   0          5m24s
webapp-6684ccd7b8-4vh4m   1/1     Running   0          5m24s
webapp-6684ccd7b8-78pjg   1/1     Running   0          5m24s
webapp-6684ccd7b8-d7qpb   1/1     Running   0          48s
webapp-6684ccd7b8-dtcmn   1/1     Running   0          48s
webapp-6684ccd7b8-gjgd9   1/1     Running   0          5m24s
webapp-6684ccd7b8-gsgnk   1/1     Running   0          48s
webapp-6684ccd7b8-hgdtv   1/1     Running   0          48s
webapp-6684ccd7b8-lhrsx   1/1     Running   0          48s
webapp-6684ccd7b8-n2kfc   1/1     Running   0          48s
webapp-6684ccd7b8-qphgc   1/1     Running   0          48s
webapp-6684ccd7b8-sq58h   1/1     Running   0          48s
webapp-6684ccd7b8-sxkjn   1/1     Running   0          48s
webapp-6684ccd7b8-tgn6t   1/1     Running   0          5m24s
webapp-6684ccd7b8-tsmd2   1/1     Running   0          48s
webapp-6684ccd7b8-vlb2t   1/1     Running   0          48s
webapp-6684ccd7b8-vsk6n   1/1     Running   0          48s
[root@master01 ~]# kubectl get podsrs
NAME                DESIRED   CURRENT   READY   AGE
webapp-6684ccd7b8   20        20        20      5m34s
[root@master01 ~]# kubectl get rspods |wc -l
21
[root@master01 ~]# kubectl get pods |wc -lrspods |wc -lwatch kubectl scale --current-replicas=5 --replicas=20 deployment/webappwatch kubectl get pods |wc -l-l app=webapp
NAME                      READY   STATUS    RESTARTS   AGE
webapp-6684ccd7b8-27ptp   1/1     Running   0          119s
webapp-6684ccd7b8-2tvfp   1/1     Running   0          119s
webapp-6684ccd7b8-4d64x   1/1     Running   0          119s
webapp-6684ccd7b8-4mfdd   1/1     Running   0          6m35s
webapp-6684ccd7b8-4vh4m   1/1     Running   0          6m35s
webapp-6684ccd7b8-78pjg   1/1     Running   0          6m35s
webapp-6684ccd7b8-d7qpb   1/1     Running   0          119s
webapp-6684ccd7b8-dtcmn   1/1     Running   0          119s
webapp-6684ccd7b8-gjgd9   1/1     Running   0          6m35s
webapp-6684ccd7b8-gsgnk   1/1     Running   0          119s
webapp-6684ccd7b8-hgdtv   1/1     Running   0          119s
webapp-6684ccd7b8-lhrsx   1/1     Running   0          119s
webapp-6684ccd7b8-n2kfc   1/1     Running   0          119s
webapp-6684ccd7b8-qphgc   1/1     Running   0          119s
webapp-6684ccd7b8-sq58h   1/1     Running   0          119s
webapp-6684ccd7b8-sxkjn   1/1     Running   0          119s
webapp-6684ccd7b8-tgn6t   1/1     Running   0          6m35s
webapp-6684ccd7b8-tsmd2   1/1     Running   0          119s
webapp-6684ccd7b8-vlb2t   1/1     Running   0          119s
webapp-6684ccd7b8-vsk6n   1/1     Running   0          119s
[root@master01 ~]# kubectl get pods -l app=webapp |wc -l
21
[root@master01 ~]# kubectl rollout status deploy webapp
deployment "webapp" successfully rolled out
[root@master01 ~]# kukubectl get rs -l app=webapp
NAME                DESIRED   CURRENT   READY   AGE
webapp-6684ccd7b8   20        20        20      7m27s
[root@master01 ~]# kubectl get rs -l app=webapp -o yaml
apiVersion: v1
items:
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "20"
      deployment.kubernetes.io/max-replicas: "25"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2022-09-16T09:17:07Z"
    generation: 2
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: webapp
      uid: 6950c2ed-fe5a-4fa7-a48f-462b3f609c75
    resourceVersion: "581070"
    uid: 1282649f-a972-4c80-a4b5-34629e8948d2
  spec:
    replicas: 20
    selector:
      matchLabels:
        app: webapp
        pod-template-hash: 6684ccd7b8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webapp
          pod-template-hash: 6684ccd7b8
      spec:
        containers:
        - image: nginx
          imagePullPolicy: Always
          name: nginx
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 20
    fullyLabeledReplicas: 20
    observedGeneration: 2
    readyReplicas: 20
    replicas: 20
kind: List
metadata:
  resourceVersion: ""
[root@master01 ~]# kubectl get rs -l app=webapp -o yamlpods
apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-27ptp
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581061"
    uid: 88e96cb4-8b0b-4368-a72c-1d00934edd76
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kftnp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-kftnp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e0265e224a65180b2a228ce2a71d4c1b7f7ca2d3d4aeddbd4ab7c24400fe57da
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:29Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.10
    podIPs:
    - ip: 10.36.0.10
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-2tvfp
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581028"
    uid: 6fffcf55-d4bd-4e39-aca4-01f8e1891692
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m8dn9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-m8dn9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://16bc714c13afeb7413e5306b2ff9de4d3acb9f1e30066426c8ec451a00d3d9d0
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:25Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.8
    podIPs:
    - ip: 10.36.0.8
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-4d64x
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581058"
    uid: 8c2fd345-b8f5-447d-96e9-aa1e3550ad0e
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j8w4r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-j8w4r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ed5bb3514f9964c619766e18e7cc2a7278242041cf16502ca3dc64a901559a0d
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:58Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.10
    podIPs:
    - ip: 10.44.0.10
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:17:07Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-4mfdd
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580470"
    uid: aae6023c-97cf-4778-a651-14e4c2b79505
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-snw6w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-snw6w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:17:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7fb4b471898cd5706be9d4028b7a4edd7a796826a6d12191e7e8576ba63a36d8
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:10:41Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.2
    podIPs:
    - ip: 10.36.0.2
    qosClass: BestEffort
    startTime: "2022-09-16T09:10:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:17:07Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-4vh4m
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580453"
    uid: c620aa44-e193-450f-b5fc-5f3014fbb9c2
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xpgd5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xpgd5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:09:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:09:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:09:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:17:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://15ae6ecea924249b8889e683e2a11c7e55316b1bd523a607335752b72a178d31
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:09:06Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.1
    podIPs:
    - ip: 10.44.0.1
    qosClass: BestEffort
    startTime: "2022-09-16T09:09:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:17:07Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-78pjg
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580450"
    uid: 90fe9f2a-a756-4c32-af5b-e212b946de60
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7ljw6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-7ljw6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:17:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e539a3b608ce69a9248a190f84071b939068f57afa0fc586b92a0079bf2c30f4
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:10:38Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.1
    podIPs:
    - ip: 10.36.0.1
    qosClass: BestEffort
    startTime: "2022-09-16T09:10:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-d7qpb
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581069"
    uid: cefe66d1-e574-44e5-b72d-ca248f368243
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mlnqx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mlnqx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:14:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:14:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b9c526cb384b91f09998524184610a85a51fbebf0bcf9d4ba3afdf19fe02cf3c
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:14:00Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.11
    podIPs:
    - ip: 10.44.0.11
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-dtcmn
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581001"
    uid: affc3fed-c98f-4dca-861e-b143430847e2
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t726x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-t726x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8bdc85d0b1980535e93f8d6f96ded2979c024905f01004b98e02eb148a96032b
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:20Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.6
    podIPs:
    - ip: 10.36.0.6
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:17:07Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-gjgd9
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580465"
    uid: bd369bcc-40a3-4479-a304-4300ecbc80fd
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9fzrh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-9fzrh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:09:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:09:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:09:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:17:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cdb10a1e1bef6eaf8c5de4167588f720d13643ecef3489dc051d99c9147c17b0
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:09:09Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.3
    podIPs:
    - ip: 10.44.0.3
    qosClass: BestEffort
    startTime: "2022-09-16T09:09:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-gsgnk
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581041"
    uid: dd6e276a-d649-4f65-9f8c-c0daee77cdb3
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-snq5z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-snq5z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f2de91ccc9e17166967912b396facffb0771237e7083e8368699d161d1b105eb
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:27Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.9
    podIPs:
    - ip: 10.36.0.9
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-hgdtv
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580987"
    uid: 7ca6bea0-f342-4557-a513-76ee761f887a
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ppxvk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-ppxvk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://47116aafe8f2d06ad136f90324ed5f71c0e6ab0ac5c575fff3a9987b3889631c
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:18Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.5
    podIPs:
    - ip: 10.36.0.5
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-lhrsx
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580998"
    uid: 11a2b78d-04fe-426d-b44f-7a170bd75fdc
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q25fw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-q25fw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://068d0908a40de6990911f7cd6ee43d1bbb3231bc559db5b60438ba2c183879e0
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:48Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.6
    podIPs:
    - ip: 10.44.0.6
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-n2kfc
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581047"
    uid: ca1abfae-b051-4ea6-9ea7-90686410353b
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-96zx5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-96zx5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e610da371b8df191af701e0fc384dd2c638e0405f2b08f16cbad3578e20f7455
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:55Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.9
    podIPs:
    - ip: 10.44.0.9
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-qphgc
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581013"
    uid: 20b7d7fd-7a1c-4cff-8be2-2f5e97731a80
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zs88w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zs88w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c71de6766904d7f30e9fb8a6af3961615e8234d335b094bb29edcdc3e1c98e81
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:23Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.7
    podIPs:
    - ip: 10.36.0.7
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-sq58h
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580984"
    uid: 0a4f2ab1-bb97-4eb4-8683-939780ce00c5
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pwxr9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-pwxr9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3ad79b7c2c069145ae4e53b25d240b4091b3913e405d086e3a2fac512946fb2e
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:46Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.5
    podIPs:
    - ip: 10.44.0.5
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-sxkjn
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580967"
    uid: 459cdf88-4b24-4348-96c8-7b84460a1301
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xbm9q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xbm9q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:15:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://61bc3756b12b18a07f7f979b4672675dd84b35ad5ce5cba11e7ee81099d24f36
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:15:16Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.4
    podIPs:
    - ip: 10.36.0.4
    qosClass: BestEffort
    startTime: "2022-09-16T09:15:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:17:07Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-tgn6t
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580479"
    uid: 44b50500-1e30-46b9-a79f-da75b273af3c
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ftrz9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker01.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-ftrz9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:10:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:17:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://547e1d14ee56bdb845944b54e62a753ebcfbc142199c92e6bee6a0a79ef538a3
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:10:43Z"
    hostIP: 172.16.102.23
    phase: Running
    podIP: 10.36.0.3
    podIPs:
    - ip: 10.36.0.3
    qosClass: BestEffort
    startTime: "2022-09-16T09:10:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-tsmd2
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581016"
    uid: ce865604-42d5-43d1-9eb8-86c12407a127
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pgkzq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-pgkzq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://21c42ef1fc7cc015c80ba138ba4d99c075c0554f98cc0badc082b75d88497e19
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:51Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.7
    podIPs:
    - ip: 10.44.0.7
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-vlb2t
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "581032"
    uid: 177f8cf9-04cb-4edb-97d9-8b2120d92c63
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kj574
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-kj574
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7516a910f4ac1d9f5f05059eca1792711dc3615bcdb36364c90b27067da194b4
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:53Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.8
    podIPs:
    - ip: 10.44.0.8
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2022-09-16T09:21:43Z"
    generateName: webapp-6684ccd7b8-
    labels:
      app: webapp
      pod-template-hash: 6684ccd7b8
    name: webapp-6684ccd7b8-vsk6n
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: webapp-6684ccd7b8
      uid: 1282649f-a972-4c80-a4b5-34629e8948d2
    resourceVersion: "580973"
    uid: cf69144b-b365-475a-91ed-4550379eb7b4
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: nginx
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6kbwf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: worker02.airtel.internal.lab
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-6kbwf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:13:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2022-09-16T09:21:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9b16e84723a2f5c34023ee3c63395f8c53a54412c7e304ef11bf5e86e56bd164
      image: docker.io/library/nginx:latest
      imageID: docker.io/library/nginx@sha256:0b970013351304af46f322da1263516b188318682b2ab1091862497591189ff1
      lastState: {}
      name: nginx
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2022-09-16T09:13:44Z"
    hostIP: 172.16.102.24
    phase: Running
    podIP: 10.44.0.4
    podIPs:
    - ip: 10.44.0.4
    qosClass: BestEffort
    startTime: "2022-09-16T09:13:40Z"
kind: List
metadata:
  resourceVersion: ""
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# kubectl delete pods --all --all --all --all --alld --alle --allp --alll --allo --ally --allm --alle --alln --allt --all webapp; kubectl get po -l app=webapp -w
deployment.apps "webapp" deleted
NAME                      READY   STATUS        RESTARTS   AGE
webapp-6684ccd7b8-27ptp   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-2tvfp   1/1     Running       0          4m35s
webapp-6684ccd7b8-4d64x   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-4mfdd   1/1     Running       0          9m11s
webapp-6684ccd7b8-4vh4m   1/1     Running       0          9m11s
webapp-6684ccd7b8-78pjg   1/1     Running       0          9m11s
webapp-6684ccd7b8-d7qpb   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-dtcmn   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-gjgd9   1/1     Terminating   0          9m11s
webapp-6684ccd7b8-gsgnk   1/1     Running       0          4m35s
webapp-6684ccd7b8-hgdtv   1/1     Running       0          4m35s
webapp-6684ccd7b8-lhrsx   1/1     Running       0          4m35s
webapp-6684ccd7b8-n2kfc   1/1     Running       0          4m35s
webapp-6684ccd7b8-qphgc   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-sq58h   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-sxkjn   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-tgn6t   1/1     Terminating   0          9m11s
webapp-6684ccd7b8-tsmd2   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-vlb2t   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-vsk6n   1/1     Running       0          4m35s
webapp-6684ccd7b8-lhrsx   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-gsgnk   1/1     Terminating   0          4m35s
webapp-6684ccd7b8-2tvfp   1/1     Terminating   0          4m36s
webapp-6684ccd7b8-n2kfc   1/1     Terminating   0          4m36s
webapp-6684ccd7b8-4vh4m   1/1     Terminating   0          9m12s
webapp-6684ccd7b8-4mfdd   1/1     Terminating   0          9m12s
webapp-6684ccd7b8-78pjg   1/1     Terminating   0          9m12s
webapp-6684ccd7b8-vsk6n   1/1     Terminating   0          4m36s
webapp-6684ccd7b8-hgdtv   1/1     Terminating   0          4m36s
webapp-6684ccd7b8-gjgd9   0/1     Terminating   0          9m12s
webapp-6684ccd7b8-4vh4m   0/1     Terminating   0          9m12s
webapp-6684ccd7b8-4vh4m   0/1     Terminating   0          9m12s
webapp-6684ccd7b8-4vh4m   0/1     Terminating   0          9m12s
webapp-6684ccd7b8-78pjg   0/1     Terminating   0          9m13s
webapp-6684ccd7b8-tsmd2   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-4mfdd   0/1     Terminating   0          9m13s
webapp-6684ccd7b8-4mfdd   0/1     Terminating   0          9m13s
webapp-6684ccd7b8-4mfdd   0/1     Terminating   0          9m13s
webapp-6684ccd7b8-hgdtv   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-hgdtv   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-hgdtv   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-tsmd2   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-tsmd2   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-gsgnk   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-n2kfc   0/1     Terminating   0          4m37s
webapp-6684ccd7b8-gsgnk   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-gsgnk   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-n2kfc   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-n2kfc   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-tgn6t   0/1     Terminating   0          9m14s
webapp-6684ccd7b8-vlb2t   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-tgn6t   0/1     Terminating   0          9m14s
webapp-6684ccd7b8-tgn6t   0/1     Terminating   0          9m14s
webapp-6684ccd7b8-vlb2t   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-vlb2t   0/1     Terminating   0          4m38s
webapp-6684ccd7b8-dtcmn   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-sq58h   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-dtcmn   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-dtcmn   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-sq58h   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-sq58h   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-78pjg   0/1     Terminating   0          9m15s
webapp-6684ccd7b8-78pjg   0/1     Terminating   0          9m15s
webapp-6684ccd7b8-4d64x   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-4d64x   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-4d64x   0/1     Terminating   0          4m39s
webapp-6684ccd7b8-sxkjn   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-sxkjn   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-sxkjn   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-d7qpb   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-d7qpb   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-d7qpb   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-2tvfp   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-2tvfp   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-2tvfp   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-vsk6n   0/1     Terminating   0          4m40s
webapp-6684ccd7b8-vsk6n   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-vsk6n   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-27ptp   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-27ptp   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-27ptp   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-lhrsx   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-lhrsx   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-lhrsx   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-qphgc   0/1     Terminating   0          4m41s
webapp-6684ccd7b8-qphgc   0/1     Terminating   0          4m42s
webapp-6684ccd7b8-qphgc   0/1     Terminating   0          4m42s
webapp-6684ccd7b8-gjgd9   0/1     Terminating   0          9m18s
webapp-6684ccd7b8-gjgd9   0/1     Terminating   0          9m18s
^C[root@master01 ~]# kubectl delete deployment webapp; kubectl get po -l app=webapp -wget pods -l app=webapp -o yamldelete deployment webapp; kubectl get po -l app=webapp -wkubectl get pods
No resources found in default namespace.
[root@master01 ~]# kubectl get podsrs
No resources found in default namespace.
[root@master01 ~]# kubectl crecreate deplouloyment webapp --image-=nginx --dry-run=client > webapp-deploy.yaml:1.17.1 --port 80
[root@master01 ~]# vim webapp-deploy.yaml 
"webapp-deploy.yaml" 1L, 41C  deployment.apps/webapp created (dry run)
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1AllL ::q[root@master01 ~]# kubectl delete deployment webapp
Error from server (NotFound): deployments.apps "webapp" not found
[root@master01 ~]# kubectl delete deployment webappvim webapp-deploy.yaml kubectl create deployment webapp --image=nginx:1.17.1 --port 80 --dry-run=client > webapp-deploy.yamlget rs
No resources found in default namespace.
[root@master01 ~]# kubectl get rspods
No resources found in default namespace.
[root@master01 ~]# kubectl get podsrsdelete deployment webappvim webapp-deploy.yaml kubectl create deployment webapp --image=nginx:1.17.1 --port 80 --dry-run=client > webapp-deploy.yaml -o yaml
[root@master01 ~]# vim webapp-deploy.yaml 
"webapp-deploy.yaml" 26L, 438C  apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: webapp
  name: webapp
spec:
  replicas: 1
  selector:
    matchLabels:app: webapp
  strategy: {}
  template:
    metadata:creationTimestamp: nulllabels:app: webapp
    spec:containers:- image: nginx:1.17.1name: nginxports:- containerPort: 80resources: {}
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3::wq!"webapp-deploy.yaml" 26L, 438C written
[root@master01 ~]# kubectl create -f webapp--deploy.yaml 
deployment.apps/webapp created
[root@master01 ~]# kubectl get deployment 
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   1/1     1            1           7s
[root@master01 ~]# kubectl get deployment ers
NAME                DESIRED   CURRENT   READY   AGE
webapp-684ff84dd7   1         1         1       13s
[root@master01 ~]# kubectl get rspods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-684ff84dd7-qj67s   1/1     Running   0          15s
[root@master01 ~]# kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE   IP          NODE                           NOMINATED NODE   READINESS GATES
webapp-684ff84dd7-qj67s   1/1     Running   0          23s   10.36.0.1   worker01.airtel.internal.lab   <none>           <none>
[root@master01 ~]# kubectl get pods -o widesvc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   26h
[root@master01 ~]# kubectl describe deployment webapp |grep -i image
    Image:        nginx:1.17.1
[root@master01 ~]# kubectl describe deployment webapp |grep -i imageport
    Port:         80/TCP
    Host Port:    0/TCP
[root@master01 ~]# curl kubectl describe deployment webapp |grep -i portIP
[root@master01 ~]# kubectl describe deployment webapp |grep -i IPip
[root@master01 ~]# kubectl describe deployment webapp |grep -i ipIPportimageget svcdescribe deployment webapp |grep -i imageportIPipcurl 10.36.0.1
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@master01 ~]# curl 10.36.0.1kubectl describe deployment webapp |grep -i ipIP
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.17.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-684ff84dd7 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  101s  deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
[root@master01 ~]# [root@master01 ~]# kubectl set deployment webapp im imagengnnginx=ngnginx:1.17.4
deployment.apps/webapp image updated
[root@master01 ~]# kubectl set image deployment webapp nginx=nginx:1.17.4describe deployment webappcurl 10.36.0.1kubectl describe deployment webapp |grep -i ipIPportimage
    Image:        nginx:1.17.4
[root@master01 ~]# kubectl describe deployment webapp |grep -i imageset image deployment webapp nginx=nginx:1.17.4describe deployment webapp
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.17.4
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-788f8c85b6 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  3m10s  deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
  Normal  ScalingReplicaSet  14s    deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1
  Normal  ScalingReplicaSet  13s    deployment-controller  Scaled down replica set webapp-684ff84dd7 to 0 from 1
[root@master01 ~]# kubectl rollout deployment webapp  history 
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

[root@master01 ~]# kubectl rollout history  deployment webapp describe deployment webapp |grep -i imageset image deployment webapp nginx=nginx:1.17.4 --roole-records --records3 --records
error: unknown flag: --records
See 'kubectl set image --help' for usage.
[root@master01 ~]# kubectl set image deployment webapp nginx=nginx:1.17.3 --records
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/webapp image updated
[root@master01 ~]# kubectl set image deployment webapp nginx=nginx:1.17.3 --recordsrollout history  deployment webapp describe deployment webapp
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 3
                        kubernetes.io/change-cause: kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 2 total | 1 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.17.3
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  webapp-788f8c85b6 (1/1 replicas created)
NewReplicaSet:   webapp-7649d7ff6f (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  4m29s  deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
  Normal  ScalingReplicaSet  93s    deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1
  Normal  ScalingReplicaSet  92s    deployment-controller  Scaled down replica set webapp-684ff84dd7 to 0 from 1
  Normal  ScalingReplicaSet  8s     deployment-controller  Scaled up replica set webapp-7649d7ff6f to 1
[root@master01 ~]# kubectl describe deployment webappset image deployment webapp nginx=nginx:1.17.3 --recordsrollout history  deployment webapp 
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
3         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true

[root@master01 ~]# kubectl rollout history  deployment webapp kubectl get deploy webapp s--show-labels
NAME     READY   UP-TO-DATE   AVAILABLE   AGE    LABELS
webapp   1/1     1            1           5m9s   app=webapp
[root@master01 ~]# kubectl get deploy webapp --show-labelsrs
Error from server (NotFound): replicasets.apps "webapp" not found
[root@master01 ~]# kubectl get rs webapp --show-labels --show-labels --show-labels --show-labels --show-labels --show-labels --show-labels
NAME                DESIRED   CURRENT   READY   AGE     LABELS
webapp-684ff84dd7   0         0         0       5m31s   app=webapp,pod-template-hash=684ff84dd7
webapp-7649d7ff6f   1         1         1       70s     app=webapp,pod-template-hash=7649d7ff6f
webapp-788f8c85b6   0         0         0       2m35s   app=webapp,pod-template-hash=788f8c85b6
[root@master01 ~]# kubectl get rs  --show-labels-p al app=webapp
NAME                DESIRED   CURRENT   READY   AGE
webapp-684ff84dd7   0         0         0       5m45s
webapp-7649d7ff6f   1         1         1       84s
webapp-788f8c85b6   0         0         0       2m49s
[root@master01 ~]# kubectl get rs -l app=webapp --show-labels-l app=webapp deploy
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   1/1     1            1           5m53s
[root@master01 ~]# kubectl get deploy -l app=webapppods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-7649d7ff6f-rlzm9   1/1     Running   0          101s
[root@master01 ~]# kubectl get pods -l app=webappdeployrs --show-labelswebapp --show-labelsdeployrollout history  deployment webapp get deploy webapp --show-labelsrs --show-labels-l app=webappdeploypodskubectl get image sdeployment webapp- nginx=nginx:1.17.1 --recordsrollout undo 
deployment.apps/webapp rolled back
[root@master01 ~]# kubectl rollout undo deployment webappget pods -l app=webappdeployrs --show-labelswebapp --show-labelsdeployrollout history  deployment webapp describe deployment webapp
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 4
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.17.4
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-788f8c85b6 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  8m1s   deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
  Normal  ScalingReplicaSet  5m5s   deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1
  Normal  ScalingReplicaSet  5m4s   deployment-controller  Scaled down replica set webapp-684ff84dd7 to 0 from 1
  Normal  ScalingReplicaSet  3m40s  deployment-controller  Scaled up replica set webapp-7649d7ff6f to 1
  Normal  ScalingReplicaSet  3m26s  deployment-controller  Scaled down replica set webapp-788f8c85b6 to 0 from 1
  Normal  ScalingReplicaSet  16s    deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1 from 0
  Normal  ScalingReplicaSet  15s    deployment-controller  Scaled down replica set webapp-7649d7ff6f to 0 from 1
[root@master01 ~]# kubectl describe deployment webapp
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 4
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.17.4
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-788f8c85b6 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  8m8s   deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
  Normal  ScalingReplicaSet  5m12s  deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1
  Normal  ScalingReplicaSet  5m11s  deployment-controller  Scaled down replica set webapp-684ff84dd7 to 0 from 1
  Normal  ScalingReplicaSet  3m47s  deployment-controller  Scaled up replica set webapp-7649d7ff6f to 1
  Normal  ScalingReplicaSet  3m33s  deployment-controller  Scaled down replica set webapp-788f8c85b6 to 0 from 1
  Normal  ScalingReplicaSet  23s    deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1 from 0
  Normal  ScalingReplicaSet  22s    deployment-controller  Scaled down replica set webapp-7649d7ff6f to 0 from 1
[root@master01 ~]# kubectl describe deployment webapp
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 4
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.17.4
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-788f8c85b6 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  8m10s  deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
  Normal  ScalingReplicaSet  5m14s  deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1
  Normal  ScalingReplicaSet  5m13s  deployment-controller  Scaled down replica set webapp-684ff84dd7 to 0 from 1
  Normal  ScalingReplicaSet  3m49s  deployment-controller  Scaled up replica set webapp-7649d7ff6f to 1
  Normal  ScalingReplicaSet  3m35s  deployment-controller  Scaled down replica set webapp-788f8c85b6 to 0 from 1
  Normal  ScalingReplicaSet  25s    deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1 from 0
  Normal  ScalingReplicaSet  24s    deployment-controller  Scaled down replica set webapp-7649d7ff6f to 0 from 1
[root@master01 ~]# kubectl describe deployment webappkubectl describe deployment webapprollout undoget pods -l app=webappdeployrs --show-labelswebapp --show-labelsdeployrollout history  deployment webapp describe deployment webapprollout history  deployment webapp 
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
3         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
4         <none>

[root@master01 ~]# kubectl set image deployment webapp nginx=nginx:1.16.1 --records
error: unknown flag: --records
See 'kubectl set image --help' for usage.
[root@master01 ~]# kubectl set image deployment webapp nginx=nginx:1.16.1 --records
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/webapp image updated
[root@master01 ~]# kubectl set image deployment webapp nginx=nginx:1.16.1 --recordkubectl rolout histroy webapp
error: unknown command "rolout" for "kubectl"

Did you mean this?
rollout
[root@master01 ~]# kubectl rolout histroy webappl
error: unknown command "histroy webapp"
See 'kubectl rollout -h' for help and examples
[root@master01 ~]# kubectl rollout histroy webappdeplouymnet webapp
error: unknown command "histroy deploymnet webapp"
See 'kubectl rollout -h' for help and examples
[root@master01 ~]# kubectl rollout histroy deploymnet webappr
error: the server doesn't have a resource type "deploymnet"
[root@master01 ~]# kubectl rollout history deploymnet webappploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
3         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true

[root@master01 ~]# kubectl rollout history deploy webappkubectl describe deploy webapp |grep image
                        kubernetes.io/change-cause: kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
[root@master01 ~]# 
[root@master01 ~]# kubectl describe deploy webapp |grep image image- imagei image
                        kubernetes.io/change-cause: kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
    Image:        nginx:1.16.1
[root@master01 ~]# kubectl qrollout rollout ubdoundo deploy webapp --to-revertrevsion=3#
[root@master01 ~]# kubectl set image deploy webapp -nginx=nginx:1.17.1#kubectl rollout undo deploy webapp --to-revsion=3
error: unknown flag: --to-revsion
See 'kubectl rollout undo --help' for usage.
[root@master01 ~]# kubectl rollout undo deploy webapp --to-revsion=3ision=3
deployment.apps/webapp rolled back
[root@master01 ~]# kubectl describe deploy webapp |grep -i image
                        kubernetes.io/change-cause: kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
    Image:        nginx:1.17.3
[root@master01 ~]# kubectl rollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
6         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true

[root@master01 ~]# kubectl rollout history deploy webappkubectl set image deploy webapp nginx=nginx:1.100 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/webapp image updated
[root@master01 ~]# kubectl set image deploy webapp nginx=nginx:1.100 --recordrollout history deploy webappdescribe deploy webapp |grep -i image
                        kubernetes.io/change-cause: kubectl set image deploy webapp nginx=nginx:1.100 --record=true
    Image:        nginx:1.100
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS         RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running        0          92s
webapp-77b685f94f-ntq4m   0/1     ErrImagePull   0          13s
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running            0          95s
webapp-77b685f94f-ntq4m   0/1     ImagePullBackOff   0          16s
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running            0          97s
webapp-77b685f94f-ntq4m   0/1     ImagePullBackOff   0          18s
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running            0          98s
webapp-77b685f94f-ntq4m   0/1     ImagePullBackOff   0          19s
[root@master01 ~]# kubectl get podsdescribe deploy webapp |grep -i imageset image deploy webapp nginx=nginx:1.100 --recorddescribe deploy webapp |grep -i image
Name:                   webapp
Namespace:              default
CreationTimestamp:      Fri, 16 Sep 2022 14:59:25 +0530
Labels:                 app=webapp
Annotations:            deployment.kubernetes.io/revision: 7
                        kubernetes.io/change-cause: kubectl set image deploy webapp nginx=nginx:1.100 --record=true
Selector:               app=webapp
Replicas:               1 desired | 1 updated | 2 total | 1 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=webapp
  Containers:
   nginx:
    Image:        nginx:1.100
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  webapp-7649d7ff6f (1/1 replicas created)
NewReplicaSet:   webapp-77b685f94f (1/1 replicas created)
Events:
  Type    Reason             Age                 From                   Message
  ----    ------             ----                ----                   -------
  Normal  ScalingReplicaSet  15m                 deployment-controller  Scaled up replica set webapp-684ff84dd7 to 1
  Normal  ScalingReplicaSet  12m                 deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1
  Normal  ScalingReplicaSet  12m                 deployment-controller  Scaled down replica set webapp-684ff84dd7 to 0 from 1
  Normal  ScalingReplicaSet  11m                 deployment-controller  Scaled up replica set webapp-7649d7ff6f to 1
  Normal  ScalingReplicaSet  7m41s               deployment-controller  Scaled up replica set webapp-788f8c85b6 to 1 from 0
  Normal  ScalingReplicaSet  7m40s               deployment-controller  Scaled down replica set webapp-7649d7ff6f to 0 from 1
  Normal  ScalingReplicaSet  6m6s                deployment-controller  Scaled up replica set webapp-7f46474c7d to 1
  Normal  ScalingReplicaSet  6m3s (x2 over 10m)  deployment-controller  Scaled down replica set webapp-788f8c85b6 to 0 from 1
  Normal  ScalingReplicaSet  105s                deployment-controller  Scaled up replica set webapp-7649d7ff6f to 1 from 0
  Normal  ScalingReplicaSet  26s (x2 over 104s)  deployment-controller  (combined from similar events): Scaled up replica set webapp-77b685f94f to 1
[root@master01 ~]# kubectl rollout status deploy webapp
Waiting for deployment "webapp" rollout to finish: 1 old replicas are pending termination...
^C[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS             RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running            0          2m37s
webapp-77b685f94f-ntq4m   0/1     ImagePullBackOff   0          78s
[root@master01 ~]# kubectl rollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
6         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true

[root@master01 ~]# kubectl rollout history deploy webapp undo deploy webapp --to-revers=5 
deployment.apps/webapp rolled back
[root@master01 ~]# kubectl rollout status deploy webapp
deployment "webapp" successfully rolled out
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running   0          4m41s
[root@master01 ~]# kubectl get podsrollout status deploy webappundohistoryget podsrollout history deploy webappundostatusget podskubectl describe deploy webapp |grep i -i Image
                        kubernetes.io/change-cause: kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
    Image:        nginx:1.17.3
[root@master01 ~]# kubectl get rolllout history deply webapp --recvision=7
error: the server doesn't have a resource type "deply"
[root@master01 ~]# kubectl rollout history deply webapp --revision=7o
deployment.apps/webapp with revision #7
Pod Template:
  Labels:app=webapp
pod-template-hash=77b685f94f
  Annotations:kubernetes.io/change-cause: kubectl set image deploy webapp nginx=nginx:1.100 --record=true
  Containers:
   nginx:
    Image:nginx:1.100
    Port:80/TCP
    Host Port:0/TCP
    Environment:<none>
    Mounts:<none>
  Volumes:<none>

[root@master01 ~]# kubectl rollout pause deploy webapp
deployment.apps/webapp paused
[root@master01 ~]# kubectl rollout pause deploy webapphistory deploy webapp --revision=7
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true
8         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true

[root@master01 ~]# kubectl set image deploy webapp nginx=nginx :latest
deployment.apps/webapp image updated
[root@master01 ~]# kubectl set image deploy webapp nginx=nginx:latestrollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true
8         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true

[root@master01 ~]# kubectl rollout history deploy webappset image deploy webapp nginx=nginx:latestrollout history deploy webappset image deploy webapp nginx=nginx:latestrollout history deploy webappkubectl rollout history deploy webappset image deploy webapp nginx=nginx:latest --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/webapp image updated
[root@master01 ~]# kubectl set image deploy webapp nginx=nginx:latest --recordrollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true
8         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true

[root@master01 ~]# kubectl rollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true
8         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true

[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-7649d7ff6f-6dv5t   1/1     Running   0          8m5s
[root@master01 ~]# kubectl describe deploy webapp |grep -i Image
                        kubernetes.io/change-cause: kubectl set image deploy webapp nginx=nginx:latest --record=true
    Image:        nginx:latest
[root@master01 ~]# kubectl roolllout resume deploy webapp
deployment.apps/webapp resumed
[root@master01 ~]# kubectl rollout resume deploy webappdescribe deploy webapp |grep -i Imageget podsrollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true
8         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
9         kubectl set image deploy webapp nginx=nginx:latest --record=true

[root@master01 ~]# kubectl rollout history deploy webapp
deployment.apps/webapp 
REVISION  CHANGE-CAUSE
1         <none>
4         <none>
5         kubectl set image deployment webapp nginx=nginx:1.16.1 --record=true
7         kubectl set image deploy webapp nginx=nginx:1.100 --record=true
8         kubectl set image deployment webapp nginx=nginx:1.17.3 --record=true
9         kubectl set image deploy webapp nginx=nginx:latest --record=true

[root@master01 ~]# kubectl rollout history deploy webapp --revision=9
deployment.apps/webapp with revision #9
Pod Template:
  Labels:app=webapp
pod-template-hash=84bc478888
  Annotations:kubernetes.io/change-cause: kubectl set image deploy webapp nginx=nginx:latest --record=true
  Containers:
   nginx:
    Image:nginx:latest
    Port:80/TCP
    Host Port:0/TCP
    Environment:<none>
    Mounts:<none>
  Volumes:<none>

[root@master01 ~]# kuvkubcectl scale autoscale deploy webapp --min=10 --max=20 --cpu-percent=85
horizontalpodautoscaler.autoscaling/webapp autoscaled
[root@master01 ~]# kubectl get hpa
NAME     REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
webapp   Deployment/webapp   <unknown>/85%   10        20        0          4s
[root@master01 ~]# kubectl get hpa
NAME     REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
webapp   Deployment/webapp   <unknown>/85%   10        20        0          8s
[root@master01 ~]# kubectl get hpapods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-84bc478888-sff6m   1/1     Running   0          29m
[root@master01 ~]# kubectl get pods -l app-we=webapp
NAME                      READY   STATUS    RESTARTS   AGE
webapp-84bc478888-5mfpx   1/1     Running   0          9s
webapp-84bc478888-5sdfk   1/1     Running   0          9s
webapp-84bc478888-9t6r2   1/1     Running   0          9s
webapp-84bc478888-c2wtv   1/1     Running   0          9s
webapp-84bc478888-jmvj6   1/1     Running   0          9s
webapp-84bc478888-nbk7q   1/1     Running   0          9s
webapp-84bc478888-sff6m   1/1     Running   0          29m
webapp-84bc478888-wskpj   1/1     Running   0          9s
webapp-84bc478888-x2qf7   1/1     Running   0          9s
webapp-84bc478888-xsnr5   1/1     Running   0          9s
[root@master01 ~]# kubectl get pods -l app=webappdeployment
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webapp   10/10   10           10          52m
[root@master01 ~]# kubectl get deployment -l app=webapppods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-84bc478888-5mfpx   1/1     Running   0          41s
webapp-84bc478888-5sdfk   1/1     Running   0          41s
webapp-84bc478888-9t6r2   1/1     Running   0          41s
webapp-84bc478888-c2wtv   1/1     Running   0          41s
webapp-84bc478888-jmvj6   1/1     Running   0          41s
webapp-84bc478888-nbk7q   1/1     Running   0          41s
webapp-84bc478888-sff6m   1/1     Running   0          30m
webapp-84bc478888-wskpj   1/1     Running   0          41s
webapp-84bc478888-x2qf7   1/1     Running   0          41s
webapp-84bc478888-xsnr5   1/1     Running   0          41s
[root@master01 ~]# kubectl get podsdeployment -l app=webapppods
NAME                      READY   STATUS    RESTARTS   AGE
webapp-84bc478888-5mfpx   1/1     Running   0          44s
webapp-84bc478888-5sdfk   1/1     Running   0          44s
webapp-84bc478888-9t6r2   1/1     Running   0          44s
webapp-84bc478888-c2wtv   1/1     Running   0          44s
webapp-84bc478888-jmvj6   1/1     Running   0          44s
webapp-84bc478888-nbk7q   1/1     Running   0          44s
webapp-84bc478888-sff6m   1/1     Running   0          30m
webapp-84bc478888-wskpj   1/1     Running   0          44s
webapp-84bc478888-x2qf7   1/1     Running   0          44s
webapp-84bc478888-xsnr5   1/1     Running   0          44s
[root@master01 ~]# kubectl get pods -l app=webappwebapptop
Error from server (NotFound): pods "webapp" not found
[root@master01 ~]# kubectl top pods webapp*
error: pod [NAME | -l label]
See 'kubectl top pod -h' for help and examples
[root@master01 ~]# kubectl top pods webapp*get pods -l app=webapptop
NAME                      CPU(cores)   MEMORY(bytes)   
webapp-84bc478888-5mfpx   0m           3Mi             
webapp-84bc478888-5sdfk   0m           3Mi             
webapp-84bc478888-9t6r2   0m           3Mi             
webapp-84bc478888-c2wtv   0m           3Mi             
webapp-84bc478888-jmvj6   0m           3Mi             
webapp-84bc478888-nbk7q   0m           3Mi             
webapp-84bc478888-sff6m   0m           3Mi             
webapp-84bc478888-wskpj   0m           3Mi             
webapp-84bc478888-x2qf7   0m           3Mi             
webapp-84bc478888-xsnr5   0m           3Mi             
[root@master01 ~]# kubectl delete deployment webapp; kubectl delete hap webapp
deployment.apps "webapp" deleted
error: the server doesn't have a resource type "hap"
[root@master01 ~]# kubectl delete deployment webapp; kubectl delete hap webappa webapp
Error from server (NotFound): deployments.apps "webapp" not found
horizontalpodautoscaler.autoscaling "webapp" deleted
[root@master01 ~]# kubectl create job nodeversion --image=node -- node -v
job.batch/nodeversion created
[root@master01 ~]# kubectl get job -w
NAME          COMPLETIONS   DURATION   AGE
nodeversion   0/1           10s        10s
nodeversion   0/1           20s        20s
nodeversion   0/1           22s        22s
nodeversion   0/1           24s        24s
nodeversion   1/1           24s        24s
^C[root@master01 ~]# kubectl get pods
NAME                READY   STATUS      RESTARTS   AGE
nodeversion-9gnpp   0/1     Completed   0          65s
[root@master01 ~]# kubectl get podslogs jobs nodeversion
Error from server (NotFound): pods "jobs" not found
[root@master01 ~]# kubectl logs jobs nodeversion\nodeversion\nodeversion\nodeversionnodeversionnodeversionnodeversionnodeversionnodeversionnodeversionnodeversionnodeversion
Error from server (NotFound): pods "nodeversion" not found
[root@master01 ~]# kubectl logs nodeversion get jobs
NAME          COMPLETIONS   DURATION   AGE
nodeversion   1/1           24s        23m
[root@master01 ~]# kubectl get jobslogs nodeversionget jobs jobs jobs jobsl jobso jobsg jobss jobs nodeversion
Error from server (NotFound): pods "jobs" not found
[root@master01 ~]# kubectl get pods
NAME                READY   STATUS      RESTARTS   AGE
nodeversion-9gnpp   0/1     Completed   0          23m
[root@master01 ~]# kubectl get podslogs jobs nodeversionnodeversion-9gnpp
v18.9.0
[root@master01 ~]# kubectl logs nodeversion-9gnppget pods -o yamlcreate jobs hello-job --image=busybox --dry-run=client -- echo ""Hello I am from job"-o yaml 
error: unknown flag: --image
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create jobs hello-job --image=busybox --dry-run=client -o yaml -- echo "Hello I am from job"--image
apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - command:
        - echo
        - Hello I am from job
        image: busybox
        name: hello-job
        resources: {}
      restartPolicy: Never
status: {}
[root@master01 ~]# vim hello.jonb-yaml.yaml
"hello.job.yaml" [New File]  ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        0,0-1Alli -- INSERT --0,1AllapiVersion: batch/v1
kind: Jobmetadata:  creationTimestamp: null
  name: hello-jobspec:  template:    metadata:      ccreationTimestamp: null
    spec:      containers:      - comcommand:
        - echo        - Hello I am from job        imagimage: busybox
        name: hello-job        resources: {}      restartPolicy: Never
status: {}{}19,11^[  19,10All^[  ^[  ::wq!"hello.job.yaml" [New] 19L, 344C written
[root@master01 ~]# kubectl create -f hello.job.yaml 
job.batch/hello-job created
[root@master01 ~]# kube
kubeadm  kubectl  kubelet  
[root@master01 ~]# kubectl get pods
NAME                READY   STATUS      RESTARTS   AGE
hello-job-vptjv     0/1     Completed   0          6s
nodeversion-9gnpp   0/1     Completed   0          26m
[root@master01 ~]# kubectl get podsjob
NAME          COMPLETIONS   DURATION   AGE
hello-job     1/1           7s         10s
nodeversion   1/1           24s        26m
[root@master01 ~]# kubectl get jobpods pods pods podsl podso podsg podss podshello-job-vptjv
Hello I am from job
[root@master01 ~]# kubectl delete job hello-job
job.batch "hello-job" deleted
[root@master01 ~]# kubectl get jobs
NAME          COMPLETIONS   DURATION   AGE
nodeversion   1/1           24s        27m
[root@master01 ~]# kubectl delete job nodeversion
job.batch "nodeversion" deleted
[root@master01 ~]# kubectl delete job nodeversionget jobsdelete job hello-joblogs hello-job-vptjvget jobpodscreate -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 19L, 344C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: null
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobresources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        19,10All~@k   {}8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6o -- INSERT --17,9Top17,9Allcompletions: 1024^[  17,23All::wq!"hello.job.yaml" 20L, 368C written
[root@master01 ~]# kubectl create -f hello.job.yaml 
Error from server (BadRequest): error when creating "hello.job.yaml": Job in version "v1" cannot be handled as a Job: strict decoding error: unknown field "spec.template.spec.containers[0].completions"
[root@master01 ~]# kubectl create -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 20L, 368C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: null
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobcompletions: 10resources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        17,23All~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9 i -- INSERT --17,9Allcompletions: 108completions: 107^[  17,6All::wq!"hello.job.yaml" 20L, 366C written
[root@master01 ~]# vim hello.job.yamlkubectl create -f hello.job.yaml 
error: error parsing hello.job.yaml: error converting YAML to JSON: yaml: line 18: mapping values are not allowed in this context
[root@master01 ~]# kubectl create -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 20L, 366C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: null
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobcompletions: 10resources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        17,6All~@k   7dd  
~                                                                                                                                                                        17,9All~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,9 ~@k   8~@k   7p completions: 1010,7All^[  ^[  ::wq!"hello.job.yaml" 20L, 366C written
[root@master01 ~]# vim hello.job.yamlkubectl create -f hello.job.yaml 
Error from server (BadRequest): error when creating "hello.job.yaml": Job in version "v1" cannot be handled as a Job: strict decoding error: unknown field "spec.template.metadata.completions"
[root@master01 ~]# kubectl create -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 20L, 366C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: nullcompletions: 10
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobresources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        10,7Alldd  
~                                                                                                                                                                        10,5All~@k   9,5 o -- INSERT --10,7Top10,7All654321  completions: 5765432109 87654^[  10,3All::wq!"hello.job.yaml" 20L, 367C written
[root@master01 ~]# vim hello.job.yamlkubectl create -f hello.job.yaml vim hello.job.yamlkubectl create -f hello.job.yaml 
error: error parsing hello.job.yaml: error converting YAML to JSON: yaml: line 11: mapping values are not allowed in this context
[root@master01 ~]# kubectl create -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 20L, 367C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: null
  completions: 5
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobresources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        10,3All::1111,5All::q![root@master01 ~]# vim hello.job.yamlkubectl create -f hello.job.yaml vim hello.job.yamlkubectl create -f hello.job.yaml vim hello.job.yamlkubectl create -f hello.job.yaml vim hello.job.yamlkubectl create -f hello.job.yaml vim hello.job.yamlkubectl delete job nodeversionget jobsdelete job hello-joblogs hello-job-vptjvget jobpodscreate -f hello.job.yaml vim hello.job.yamlkubectl create job hello-job --image=busybox --dry-run=client -o yaml -- echo "Hello I am from job"vim hello.job.yamlkubectl create job hello-job --image=busybox --dry-run=client -o yaml -- echo "Hello I am from job" > hello.job.yaml 
[root@master01 ~]# kubectl create job hello-job --image=busybox --dry-run=client -o yaml -- echo "Hello I am from job" > hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 19L, 344C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: null
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobresources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        11,5All~@k   6~@k   0~@k   7o -- INSERT --11,13Top11,13All2109 87  completions: 523210198765432109 completions: 58completions: 57^[  11,6All::wq!"hello.job.yaml" 20L, 365C written
[root@master01 ~]# vim hello.job.yamlkubectl create job hello-job --image=busybox --dry-run=client -o yaml -- echo "Hello I am from job" > hello.job.yaml vim hello.job.yamlkubectl create -f hello.job.yaml 
Error from server (BadRequest): error when creating "hello.job.yaml": Job in version "v1" cannot be handled as a Job: strict decoding error: unknown field "spec.template.spec.completions"
[root@master01 ~]# kubectl create -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 20L, 365C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  template:
    metadata:creationTimestamp: null
    spec:completions: 5containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobresources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        11,6All~@k   7dd  
~                                                                                                                                                                        11,7All~@k   0~@k   9,7 ~@k   8~@k   7~@k   6,5p completions: 57,7All~@k   6~@k   5~@k   4~@k   5~@k   6~@k   7i -- INSERT --7,7Allcompletions: 56completions: 55completions: 54completions: 53completions: 52 completions: 53^[  7,2All::wq!"hello.job.yaml" 20L, 361C written
[root@master01 ~]# vim hello.job.yamlkubectl create -f hello.job.yaml 
job.batch/hello-job created
[root@master01 ~]# kubectl get pods
NAME              READY   STATUS              RESTARTS   AGE
hello-job-kfz55   0/1     ContainerCreating   0          2s
hello-job-zclws   0/1     Completed           0          9s
[root@master01 ~]# kubectl get pods
NAME              READY   STATUS              RESTARTS   AGE
hello-job-6nxwm   0/1     ContainerCreating   0          3s
hello-job-kfz55   0/1     Completed           0          9s
hello-job-zclws   0/1     Completed           0          16s
[root@master01 ~]# kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
hello-job-6nxwm   0/1     Completed   0          5s
hello-job-kfz55   0/1     Completed   0          11s
hello-job-zclws   0/1     Completed   0          18s
[root@master01 ~]# kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
hello-job-6nxwm   0/1     Completed   0          7s
hello-job-kfz55   0/1     Completed   0          13s
hello-job-zclws   0/1     Completed   0          20s
[root@master01 ~]# kubectl get podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Fri Sep 16 16:27:56 2022NAMEREADY   STATUSRESTARTS   AGEhello-job-25nfc   0/1     ContainerCreating   02shello-job-6nxwm   0/1     Completed09shello-job-kfz55   0/1     Completed015shello-job-zclws   0/1     Completed022s8mpleted   0          5s0   12s0   18s0   25s8:01        RESTARTS   AGE 0     7s2hmnbntainerCreating   0          1s6nxwm 0          14skfz55 0          20shello-job-zclws   0/1     Completed027s39362950   11smpleted   0          5s0   18s0   24s0   31s737206395928511711s430739362952158441737306395928521822541843047350626952[root@master01 ~]# delete kubcectl delete jobs hello*
Error from server (NotFound): jobs.batch "hello.job.yaml" not found
[root@master01 ~]# kubectl delete jobs hello*-*
Error from server (NotFound): jobs.batch "hello-*" not found
[root@master01 ~]# kubectl delete jobs hello-*j*o*b*s*
Error from server (NotFound): jobs.batch "hello-jobs*" not found
[root@master01 ~]# kubectl delete jobs hello-jobs*
Error from server (NotFound): jobs.batch "hello-jobs" not found
[root@master01 ~]# kubectl delete jobs hello-jobsb hello-job
job.batch "hello-job" deleted
[root@master01 ~]# kubectl get jobs -w



^C[root@master01 ~]# kubectl get jobs -w
No resources found in default namespace.
[root@master01 ~]# kubectl get jobspods
No resources found in default namespace.
[root@master01 ~]# kubectl get podsjobs -wdelete job hello-jobs hello-jobs***watch kubectl get podscreate -f hello.job.yaml vim hello.job.yaml
"hello.job.yaml" 20L, 361C  apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: hello-job
spec:
  completions: 5
  template:
    metadata:creationTimestamp: null
    spec:containers:- command:- echo- Hello I am from jobimage: busyboxname: hello-jobresources: {}restartPolicy: Never
status: {}
~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        ~                                                                                                                                                                        7,2All~@k   3i -- INSERT --7,3All^[  7,2Allo -- INSERT --8,3Top8,3Allparallelism: 10187,17^[  7,16Alldd  
~                                                                                                                                                                        7,3All::wq!"hello.job.yaml" 20L, 362C written
[root@master01 ~]# kubectl create jobs hello.job.yaml 
Error: must specify one of -f and -k

error: unknown command "jobs hello.job.yaml"
See 'kubectl create -h' for help and examples
[root@master01 ~]# kubectl create jobs hello.job.yaml -f
job.batch/hello-job created
[root@master01 ~]# kubectl create -f hello.job.yaml jobsvim hello.job.yamlkubectl get podsjobs -w
NAME        COMPLETIONS   DURATION   AGE
hello-job   0/1 of 10     4s         4s
hello-job   0/1 of 10     7s         7s
hello-job   2/1 of 10     7s         7s
hello-job   2/1 of 10     9s         9s
hello-job   4/1 of 10     9s         9s
hello-job   4/1 of 10     11s        11s
hello-job   6/1 of 10     11s        11s
hello-job   6/1 of 10     12s        12s
hello-job   7/1 of 10     12s        12s
hello-job   7/1 of 10     13s        13s
hello-job   8/1 of 10     13s        13s
hello-job   8/1 of 10     15s        15s
hello-job   9/1 of 10     15s        15s
hello-job   9/1 of 10     15s        15s
hello-job   9/1 of 10     18s        18s
hello-job   10/1 of 10    18s        18s
^C[root@master01 ~]# kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
hello-job-4zlbj   0/1     Completed   0          30s
hello-job-b28xk   0/1     Completed   0          30s
hello-job-ckfvm   0/1     Completed   0          30s
hello-job-f5qvp   0/1     Completed   0          30s
hello-job-fwdbm   0/1     Completed   0          30s
hello-job-gdj59   0/1     Completed   0          30s
hello-job-lg2jn   0/1     Completed   0          30s
hello-job-ll5k7   0/1     Completed   0          30s
hello-job-lmg79   0/1     Completed   0          30s
hello-job-rmkj2   0/1     Completed   0          30s
[root@master01 ~]# kubectl get podsjobs -wdelete  h e l l o - p o d s     j o b s 
Error from server (NotFound): jobs.batch "hello-jobs" not found
[root@master01 ~]# kubectl delete jobs hello-jobs 
Error from server (NotFound): jobs.batch "hello-jobs" not found
[root@master01 ~]# kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
hello-job   10/1 of 10    18s        54s
[root@master01 ~]# kubectl get jobsdelete job hello-jobs get jobskubectl get jobs jobs jobs jobsd jobse jobsl jobst jobse jobs jobs jobs jobsl jobse jobst jobse jobs hello-job
job.batch "hello-job" deleted
[root@master01 ~]# kubectl delete jobs hello-jobget jobs
No resources found in default namespace.
[root@master01 ~]# kubectl create create run cronjob cronjob-pod --image-=busybox --dry-run=cleient -o yaml createdate-job--shchedule=*/"*/ * * * *" --bin/sh -c "date; echo ""hello from kubernetes clusterr" > crondate-job.yaml[root@master01 ~]# kubectl create cronjob date-job --image=busybox --dry-run=client -o yaml --schedule="*/ * * * *" --bin/sh -c "date; echo "hello from kubernetes cluster" > date-job.yaml
> ^C
[root@master01 ~]# kubectl create cronjob date-job --image=busybox --dry-run=client -o yaml --schedule="*/ * * * *" --bin/sh -c "date; echo "hello from kubernetes cluster" > date-job.yaml
error: unknown flag: --bin/sh
See 'kubectl create cronjob --help' for usage.
[root@master01 ~]# kubectl create cronjob date-job --image=busybox --dry-run=client -o yaml --schedule="*/ * * * *" --bin/sh -c "date; echo hello from kubernetes cluster" > date-job.yamlcreate
error: unknown flag: --bin/sh
See 'kubectl create cronjob --help' for usage.
[root@master01 ~]# kubectl create cronjob date-job --image=busybox --dry-run=client -o yaml --schedule="*/ * * * *" --bin/sh -c "date; echo hello from kubernetes cluster" > date-job.yaml 
[root@master01 ~]# vim date-job.yaml 
"date-job.yaml" 26L, 553C  apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: date-job
spec:
  jobTemplate:
    metadata:creationTimestamp: nullname: date-job
    spec:template:metadata:creationTimestamp: nullspec:containers:- command:- bin/sh- -c- date; echo hello from kubernetes clusterimage: busyboxname: date-jobresources: {}restartPolicy: OnFailure
  schedule: '*/ * * * *'
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           1,1All::wq!"date-job.yaml" 26L, 553C written
[root@master01 ~]# kubectl create -f date-job.yaml 
The CronJob "date-job" is invalid: spec.schedule: Invalid value: "*/ * * * *": failed to parse int from : strconv.Atoi: parsing "": invalid syntax
[root@master01 ~]# kubectl create -f date-job.yaml vim
"date-job.yaml" 26L, 553C  apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: date-job
spec:
  jobTemplate:
    metadata:creationTimestamp: nullname: date-job
    spec:template:metadata:creationTimestamp: nullspec:containers:- command:- bin/sh- -c- date; echo hello from kubernetes clusterimage: busyboxname: date-jobresources: {}restartPolicy: OnFailure
  schedule: '*/ * * * *'
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   4i -- INSERT --25,14All*/ * * * *'3"*/ * * * *'
status: {}4254"
status: {}5^[  25,24All::wq!"date-job.yaml" 26L, 553C written
[root@master01 ~]# vim date-job.yaml kubectl create -f
The CronJob "date-job" is invalid: spec.schedule: Invalid value: "*/ * * * *": failed to parse int from : strconv.Atoi: parsing "": invalid syntax
[root@master01 ~]# kubectl create -f date-job.yaml vim
"date-job.yaml" 26L, 553C  apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: date-job
spec:
  jobTemplate:
    metadata:creationTimestamp: nullname: date-job
    spec:template:metadata:creationTimestamp: nullspec:containers:- command:- bin/sh- -c- date; echo hello from kubernetes clusterimage: busyboxname: date-jobresources: {}restartPolicy: OnFailure
  schedule: "*/ * * * *"
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           25,24All~@k   3~@k   2~@k   1~@k   0~@k   19~@k   8~@k   7~@k   6i -- INSERT --25,16All1 * * * *"7^[  25,16All::wq!"date-job.yaml" 26L, 554C written
[root@master01 ~]# vim date-job.yaml kubectl create -f
cronjob.batch/date-job created
[root@master01 ~]# kubectl hjet get pods
No resources found in default namespace.
[root@master01 ~]# kubectl get podscronjobs
NAME       SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
date-job   */1 * * * *   False     0        <none>          12s
[root@master01 ~]# kubectl get cronjobs 
NAME       SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
date-job   */1 * * * *   False     0        <none>          20s
[root@master01 ~]# kubectl get cronjobs wtch 
-bash: wtch: command not found
[root@master01 ~]# wtch kubectl get cronjobs a
Every 2.0s: kubectl get cronjobsmaster01.airtel.internal.lab: Fri Sep 16 16:40:10 2022NAME   SCHEDULE SUSPEND   ACTIVE   LAST SCHEDULE   AGEdate-job   */1 * * * *   False     010s31s22345677899402121233455677899503131233456788940406122344566788950507122344566789801:0111s 2334556707899901111s233455677899100s2122344566788930301122344566788940402m12234567789910s515123345567789920[root@master01 ~]# kubectl get cj data-jonb -o yaml
Error from server (NotFound): cronjobs.batch "data-job" not found
[root@master01 ~]# kubectl get cj data-job -o yamle
apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: "2022-09-16T11:09:39Z"
  generation: 1
  name: date-job
  namespace: default
  resourceVersion: "591282"
  uid: d407be37-4698-40ac-9751-28de02cf39d4
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: 1
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: date-job
    spec:
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - command:
            - bin/sh
            - -c
            - date; echo hello from kubernetes cluster
            image: busybox
            imagePullPolicy: Always
            name: date-job
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: OnFailure
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 30
  schedule: '*/1 * * * *'
  successfulJobsHistoryLimit: 3
  suspend: false
status:
  lastScheduleTime: "2022-09-16T11:12:00Z"
  lastSuccessfulTime: "2022-09-16T11:12:07Z"
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
date-job-27722110-vg7sh   0/1     Completed   0          2m33s
date-job-27722111-v57p2   0/1     Completed   0          93s
date-job-27722112-gdz58   0/1     Completed   0          33s
[root@master01 ~]# kubectl get podskubectl get podsjobs
NAME                COMPLETIONS   DURATION   AGE
date-job-27722114   1/1           6s         2m55s
date-job-27722115   1/1           6s         115s
date-job-27722116   1/1           6s         55s
[root@master01 ~]# kubectl get jobs jobs jobs jobsl jobso jobsg jobss jobs date-job-27722114
Error from server (NotFound): pods "jobs" not found
[root@master01 ~]# kubectl logs jobs date-job-27722114date-job-27722114date-job-27722114date-job-27722114date-job-27722114date-job-27722114date-job-27722114 date-job-27722114
Error from server (NotFound): pods "date-job-27722114" not found
[root@master01 ~]# kubectl logs date-job-27722114jobs date-job-27722114get jobs
NAME                COMPLETIONS   DURATION   AGE
date-job-27722115   1/1           6s         2m54s
date-job-27722116   1/1           6s         114s
date-job-27722117   1/1           6s         54s
[root@master01 ~]# kubectl get jobslogs date-jojobs date-job-277221145
Error from server (NotFound): pods "jobs" not found
[root@master01 ~]# kubectl logs jobs date-job-27722115date-job-27722115date-job-27722115date-job-27722115date-job-27722115date-job-27722115
Error from server (NotFound): pods "date-job-27722115" not found
[root@master01 ~]# kubectl logs date-job-277221156
Error from server (NotFound): pods "date-job-27722116" not found
[root@master01 ~]# kubectl logs date-job-277221165jobs date-job-27722115get jobs
NAME                COMPLETIONS   DURATION   AGE
date-job-27722116   1/1           6s         2m15s
date-job-27722117   1/1           6s         75s
date-job-27722118   1/1           6s         15s
[root@master01 ~]# kubectl get jobslogs date-job-27722116
Error from server (NotFound): pods "date-job-27722116" not found
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
date-job-27722116-9f4v5   0/1     Completed   0          2m29s
date-job-27722117-h5jzl   0/1     Completed   0          89s
date-job-27722118-jvl4h   0/1     Completed   0          29s
[root@master01 ~]# kubectl get podslogs date-job-27722116-9f4v5
Fri Sep 16 11:09:32 UTC 2022
hello from kubernetes cluster
[root@master01 ~]# [root@master01 ~]# kubectl logs date-job-27722116-9f4v5get podslogs date-job-27722116get jobslogs date-job-277221165jobs date-job-27722115get jobslogs date-jojobs date-job-27722114get jobspodscj date-job -o yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: "2022-09-16T11:09:39Z"
  generation: 1
  name: date-job
  namespace: default
  resourceVersion: "595171"
  uid: d407be37-4698-40ac-9751-28de02cf39d4
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: 1
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: date-job
    spec:
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - command:
            - bin/sh
            - -c
            - date; echo hello from kubernetes cluster
            image: busybox
            imagePullPolicy: Always
            name: date-job
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: OnFailure
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 30
  schedule: '*/1 * * * *'
  successfulJobsHistoryLimit: 3
  suspend: false
status:
  lastScheduleTime: "2022-09-16T11:48:00Z"
  lastSuccessfulTime: "2022-09-16T11:48:06Z"
[root@master01 ~]# kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
date-job-27722146-m7p8z   0/1     Completed   0          2m49s
date-job-27722147-mhhw4   0/1     Completed   0          109s
date-job-27722148-dctpk   0/1     Completed   0          49s
[root@master01 ~]# kubectl delete jobs date-job
Error from server (NotFound): jobs.batch "date-job" not found
[root@master01 ~]# kubectl delete jobs date-job date-job date-job date-job date-jobc date-jobj date-job
cronjob.batch "date-job" deleted
[root@master01 ~]# kubectl delete cj date-jobjobsget pods
No resources found in default namespace.
[root@master01 ~]# kubectl get pvc
No resources found in default namespace.
[root@master01 ~]# kubectl get pvc
NAME           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01       5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     3d2h
nfs-pv02       5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     3d2h
nfs-pv03       10Gi       RWX            Recycle          Available   /nfs-pvc03   nfs                     3d2h
pv-analytics   100Mi      RWX            Retain           Available                                        2d23h
[root@master01 ~]# kubectl delete pv pv-analytics nfs-pv03 nfs-pv02 nfs-pv01
persistentvolume "pv-analytics" deleted
[root@master01 ~]# kubectl kubectl delete pv nfs-pv023
persistentvolume "nfs-pv03" deleted
[root@master01 ~]# kubectl cretecreate pv task-pv-volume --size=10Gi --volume=/mnt/data:ROReadWriteOnce --storage=storageClassName manualmanual=manualstorageClassName=manualstorageClassName=manualstorageClassName=manualstorageClassName=manualstorageClassName=manualstorageClassName=manualstorageClassName=manualtorageClassName=manual --dy-run=client -o yaml
error: unknown flag: --size
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create pv task-pv-volume --size=10Gi --volume=/mnt/data:ReadWriteOnce --storageClassName=manual --dy-run=client -o yamlvim task-pv-volume
"task-pv-volume" [New File]  ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           0,0-1Alli -- INSERT --0,1AllapiVersion: v1kind: Podmetadata:  name: test-pdspec:
  containers:  - image: registry.k8s.io/test-webserver    name: test-container    volumeMounts:
    - mountPath: /test-pd      name: test-volume
  volumes:  - name: test-volume
    hostPath:      # directory location on host
      path: /data      # this field is optional
      type: Directory18,22^[  18,21All::wq!"task-pv-volume" [New] 18L, 359C written
[root@master01 ~]# vim task-pv-volume
"task-pv-volume" 18L, 359C  apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pdname: test-volume
  volumes:
  - name: test-volume
    hostPath:# directory location on hostpath: /data# this field is optionaltype: Directory
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           18,21All~@k   7~@k   6,17~@k   5,21~@k   4,13~@k   3,21~@k   2,10~@k   1,21~@k   0~@k   9,17 ~@k   8,21~@k   7~@k   6,13~@k   5,5 ~@k   6,13~@k   7,21~@k   6,13dd  
~                                                                                                                                                                                           6,3Alldd  
~                                                                                                                                                                                           6,5Alldd  
~                                                                                                                                                                                           6,5All~@k   7~@k   6dd  
~                                                                                                                                                                                           6,5Alldd  
~                                                                                                                                                                                           6,7Alldd  
~                                                                                                                                                                                           6,3All~@k   7~@k   8~@k   7~@k   8~@k   9~@k   10,3~@k   1~@k   2~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   o -- INSERT --13,1All    volumeMounts:
    - mountPath: /test-pd      name: test-volume5,2443,182,221,240,189,24 8,17,2210198765432165,6 4,113,1029 87PersistentVolume233,104,165432109 t10a1s2k3-4p5v6-7v8o9l20u1m2e35,6 6,1178,149,23^[  9,22Alldd  
~                                                                                                                                                                                           9,7All~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --9,17All6543//data4m/data5t/data6/data5n/data6t/data7//data8data710,17^[  10,16Alldd  
~                                                                                                                                                                                           10,7All~@k   9,7 ~@k   8~@k   7~@k   6~@k   5,5~@k   4,7~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   dd  
~                                                                                                                                                                                           1,1Alldd  
~                                                                                                                                                                                           1,1Alldd  
~                                                                                                                                                                                           1,3Alldd  
~                                                                                                                                                                                           1,1Alldd  
~                                                                                                                                                                                           1,3Alldd  
~                                                                                                                                                                                           1,3Alldd  
~                                                                                                                                                                                           1,5Alld 
~                                                                                                                                                                                           1,7All
~                                                                                                                                                                                           1,7Alldd  
~                                                                                                                                                                                           1,5Alldd  
~                                                                                                                                                                                           1,5Alldd  
~                                                                                                                                                                                           1,7Alldd  --No lines in buffer--0,0-1Alldd  dd  dd  dd  dd  dd  dd  dd  i -- INSERT --0,1AllapiVersion: v1kind: PersistentVolumemetadata:  name: task-pv-volume  labels:    type: localspec:  storageClassName: manual  capacity:    storage: 10Gi  accessModes:    - ReadWriteOnce  hostPath:    path: "/mnt/data"28,176543210198765432109,1 8765432^[  2,0-1Alldd  
~                                                                                                                                                                                           2,1All~@k   3,0-1dd  
~                                                                                                                                                                                           3,1All~@k   4,0-1~@k   5,1  ~@k   4,0-1dd  
~                                                                                                                                                                                           4,3All~@k   5,0-1dd  
~                                                                                                                                                                                           5,3All~@k   6,0-1dd  
~                                                                                                                                                                                           6,5All~@k   7,0-1dd  
~                                                                                                                                                                                           7,1All~@k   8,0-1dd  
~                                                                                                                                                                                           8,3All~@k   9,0-1dd  
~                                                                                                                                                                                           9,3All~@k   10,0-1dd  
~                                                                                                                                                                                           10,5All~@k   1,0-1dd  
~                                                                                                                                                                                           11,3All~@k   2,0-1dd  
~                                                                                                                                                                                           12,5All~@k   3,0-1dd  
~                                                                                                                                                                                           13,3All~@k   4,0-1dd  
~                                                                                                                                                                                           14,5All::wq!"task-pv-volume" 15L, 230C written
[root@master01 ~]# kubectl ceate create -f task-pv-volume 
persistentvolume/task-pv-volume created
[root@master01 ~]# kubectl get pv
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01         5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     3d3h
nfs-pv02         5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     3d3h
task-pv-volume   10Gi       RWO            Retain           Available                manual                  6s
[root@master01 ~]# kubectl get pvcreate -f task-pv-volume vim task-pv-volume
"task-pv-volume" 15L, 230C  apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           14,5All::q[root@master01 ~]# kuvim task-pv-volumec-volume
"task-pvc-volume" [New File]  ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           0,0-1Alli -- INSERT --0,1AllapiVersion: v1
kind: PersistentVolumeClaimmetadata:  name: task-pv-claim
spec:  storageClassName: manual  accessModes:
    - ReadWriteOnce  resources:    requests:      storage: 3Gi11,19^[  11,18All::wq!"task-pvc-volume" [New] 11L, 189C written
[root@master01 ~]# kubectl create -f task-pvc-volume 
persistentvolumeclaim/task-pv-claim created
[root@master01 ~]# kubectl get pvc
NAME            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim   Bound    task-pv-volume   10Gi       RWO            manual         5s
[root@master01 ~]# kubectl get pvccreate -f task-pvc-volume vim task-pvc-volume
"task-pvc-volume" 11L, 189C  apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:storage: 3Gi
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           11,18All::q[root@master01 ~]# vim task-pvc-volumekubectl get pvccreate -f task-pvc-volume get pvc
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                   STORAGECLASS   REASON   AGE
nfs-pv01         5Gi        RWX            Recycle          Available   /nfs-pvc01              nfs                     3d3h
nfs-pv02         5Gi        RWX            Recycle          Available   /nfs-pvc02              nfs                     3d3h
task-pv-volume   10Gi       RWO            Retain           Bound       default/task-pv-claim   manual                  2m15s
[root@master01 ~]# kubectl delete pvc task-pvc-volume ume 
Error from server (NotFound): persistentvolumeclaims "task-pvc-volume" not found
[root@master01 ~]# kubectl delete pvc task-pvc-volume 
Error from server (NotFound): persistentvolumeclaims "task-pvc-volume" not found
[root@master01 ~]# kubectl delete pvc task-pvc-volume kubectl delete pvc task-pvc-volume task-pv-volume
Error from server (NotFound): persistentvolumeclaims "task-pv-volume" not found
[root@master01 ~]# kubectl delete pvc task-pv-volumetask-pv-claim
persistentvolumeclaim "task-pv-claim" deleted
[root@master01 ~]# kubectl delete pvc task-pv-claimkubectl delete pvc task-pv-claim task-pv-volume
persistentvolume "task-pv-volume" deleted
[root@master01 ~]# mv task-pv.
task-pvc-volume  task-pv-volume   
[root@master01 ~]# mv task-pv-volume task-pv -volume .yaml
[root@master01 ~]# mv task-pvc-volume task-pvc-volume .yaml
[root@master01 ~]# ll
total 10268
-rw-r--r--  1 root root      284 Sep 13 12:33 admin-pod.yaml
-rw-------. 1 root root     1471 Jun  9 16:46 anaconda-ks.cfg
-rw-r--r--  1 root root      569 Sep 16 12:56 busy-sidecar.yaml
-rw-r--r--  1 root root      554 Sep 16 16:39 date-job.yaml
-rw-r--r--  1 root root      281 Sep 13 14:36 ds-kusc00201.yaml
drwxr-xr-x  3 1000 1000      108 Sep 13 16:28 etcd-v3.3.13-linux-amd64
-rw-r--r--  1 root root 10423953 Dec  7  2021 etcd-v3.3.13-linux-amd64.tar.gz
-rw-r--r--  1 root root      362 Sep 16 16:30 hello.job.yaml
-rw-r--r--  1 root root      353 Sep 15 18:06 multe-container.yaml
-rw-r--r--  1 root root      413 Sep 13 14:22 nfs-pv.yaml
-rw-r--r--  1 root root      219 Sep 15 12:10 nginx1.17.4.yaml
-rw-r--r--  1 root root      228 Sep 16 14:33 nginxNode.yaml
-rw-r--r--  1 root root      270 Sep 14 17:24 nginx-same.yaml
-rw-r--r--  1 root root      176 Sep 13 15:19 nginx.yaml
-rw-r--r--  1 root root        0 Sep 12 09:41 nodes.txt
-rw-r--r--  1 root root      249 Sep 14 16:55 non-root-pod.yaml
-rw-r--r--  1 root root      289 Sep 14 16:47 pod-redis.yaml
-rw-r--r--  1 root root      496 Sep 13 15:06 pod-spec-KUCC00108.yaml
-rw-r--r--  1 root root      143 Sep 14 17:00 policynetwork.yaml
-rw-r--r--  1 root root      184 Sep 13 17:46 pv-analytics.yaml
-rw-r--r--  1 root root      189 Sep 16 17:54 task-pvc-volume.yaml
-rw-r--r--  1 root root      230 Sep 16 17:53 task-pv-volume.yaml
-rw-r--r--  1 root root      438 Sep 16 14:59 webapp-deploy.yaml
-rw-r--r--  1 root root      388 Sep 16 14:46 webapp-replica.yaml
-rw-r--r--  1 root root      338 Sep 13 11:59 web-project-268.yaml
[root@master01 ~]# vim task-pv-volume.yaml 
"task-pv-volume.yaml" 15L, 230C  apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           1,1All::q[root@master01 ~]# kubectl run pod redis-pod --image=Redis --dry-run=client -o yaml ?> redis-pod.yaml
error: Invalid image name "Redis": invalid reference format
[root@master01 ~]# kubectl run pod redis-pod --image=Redis --dry-run=client -o yaml > redis-pod.yamlr
[root@master01 ~]# vim redis-pod.yaml 
"redis-pod.yaml" 17L, 252C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod
  name: pod
spec:
  containers:
  - args:
    - redis-pod
    image: redis
    name: pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,1 ~@k   8::QE492: Not an editor command: Q8,1All::q![root@master01 ~]# vim redis-pod.yaml 
"redis-pod.yaml" 17L, 252C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod
  name: pod
spec:
  containers:
  - args:
    - redis-pod
    image: redis
    name: pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           8,1All~@k   7~@k   6~@k   5~@k   6~@k   7~@k   6~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --6,12All3210R10r1e2d3i4s57,12109 r10e1d2i3s48,6 9,1410,101,1423321r2e3d4i5s6-7p8o9d20^[  13,19All::Wq!E492: Not an editor command: Wq!13,19All::wq!"redis-pod.yaml" 17L, 262C written
[root@master01 ~]# vim redis-pod.yaml kubectl run pod redis-pod --image=redis --dry-run=client -o yaml > redis-pod.yamlRrcreate
error: unknown flag: --image
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create pod redis-pod --image=redis --dry-run=client -o yaml > redis-pod.yamlvim redis-pod.yaml 
"redis-pod.yaml" 0L, 0C  ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           0,0-1All::QE492: Not an editor command: Q0,0-1All::Qq![root@master01 ~]# vim redis-pod.yaml kubectl create pod redis-pod --image=redis --dry-run=client -o yaml > redis-pod.yamlvim redis-pod.yaml kubectl run pod redis-pod --image=redis --dry-run=client -o yaml > redis-pod.yaml
[root@master01 ~]# vm im redis-pod.yaml 
"redis-pod.yaml" 15L, 244C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis-pod
  name: redis-pod
spec:
  containers:
  - image: redis
    name: redis-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   5o -- INSERT --12,5Top12,5AllvolumeMounts:
    - mountPath: /cachename: cache-volum(paste) --e
  volumes:
  - name: cache-volume
    emptyDir: {}--a17,17All{}^[  17,16Allu6 fewer lines; before #1  11 seconds ago
    resources: {}dnsPolicy: ClusterFirst
  restartPolicy: Alwaysstatus: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           11,5Allo -- INSERT --12,5Top12,5All4321volumeMounts:
    - mountPath: /cachename: cach(paste) --e-volume
  volumes:
  - name: cache-volume
    emptyDir: {}{}17,17All{}6^[  16,16All~@k   {}7~@k   {}{}8~@k   {}9~@k   {}8dd  
~                                                                                                                                                                                           18,3Alldd  
~                                                                                                                                                                                           18,3All~@k   7~@k   6~@k   5~@k   4~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --13,23All4321019r20e1d2i3s4-5v6o7l8091304,25432101986543redis-vol01245,116,232101987654321redis-vol0122^[  16,21All^[  ^[  ^[  ^[  ^[  ^[  ::wq!"redis-pod.yaml" 19L, 326C written
[root@master01 ~]# mv redis-pod.yaml redis-pod.yaml -vol.yaml
[root@master01 ~]# kubectl create -f redis-pod-vol.yaml 
pod/redis-pod created
[root@master01 ~]# kubectl get pods
NAME        READY   STATUS              RESTARTS   AGE
redis-pod   0/1     ContainerCreating   0          5s
[root@master01 ~]# kubectl get podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Fri Sep 16 18:05:48 2022NAMEREADY   STATUS    RESTARTS   AGEredis-pod   1/1     Running   010s502[root@master01 ~]# watch kubectl get podsdecrdescribe redis-pod
error: the server doesn't have a resource type "redis-pod"
[root@master01 ~]# kubectl describe redis-podpredis-podoredis-poddredis-podsredis-pod redis-pod
Name:             redis-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             worker01.airtel.internal.lab/172.16.102.23
Start Time:       Fri, 16 Sep 2022 17:59:07 +0530
Labels:           run=redis-pod
Annotations:      <none>
Status:           Running
IP:               10.36.0.1
IPs:
  IP:  10.36.0.1
Containers:
  redis-pod:
    Container ID:   containerd://9551a9758f3c9d4f93808b252efaf6c0299e3dda50d0c5f6f650f22e5ee81773
    Image:          redis
    Image ID:       docker.io/library/redis@sha256:091a7b5de688f283b30a4942280b64cf822bbdab0abfb2d2ce6db989f2d3c3f4
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 16 Sep 2022 17:59:15 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /redis-vol01 from redis-vol01 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-klfrz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  redis-vol01:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-klfrz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  40s    default-scheduler  Successfully assigned default/redis-pod to worker01.airtel.internal.lab
  Normal  Pulling    7m11s  kubelet            Pulling image "redis"
  Normal  Pulled     7m4s   kubelet            Successfully pulled image "redis" in 7.101585345s
  Normal  Created    7m4s   kubelet            Created container redis-pod
  Normal  Started    7m4s   kubelet            Started container redis-pod
[root@master01 ~]# kubectl describe pods redis-podredis-podredis-podredis-podredis-podredis-podredis-podredis-podredis-podredis-podedis-podredis-podredis-podredis-podredis-poderedis-podxredis-poderedis-podcredis-pod redis-pod-redis-podiredis-podtredis-pod redis-pod -- bash touch 
root@redis-pod:/data# root@redis-pod:/data# touch       df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  6.1G   41G  13% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root   47G  6.1G   41G  13% /etc/hosts
shm                   64M     0   64M   0% /dev/shm
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
root@redis-pod:/data# cd /redis-vol01
root@redis-pod:/redis-vol01# touch file.txtfile.txt
root@redis-pod:/redis-vol01# vim file.txt 
bash: vim: command not found
root@redis-pod:/redis-vol01# vim file.txt echo  ""  >This is called the fileThis is called the file" > file.txt 
root@redis-pod:/redis-vol01# echo "This is called the file" > file.txt root@redis-pod:/redis-vol01# 
exit
[root@master01 ~]# kubectl exec -it redis-pod -- bash
root@redis-pod:/data# root@redis-pod:/data# echo "This is called the file" > file.txt root@redis-pod:/data# pwd
/data
root@redis-pod:/data# cd    cd /data/vlo  ol01
bash: cd: /data/vol01: No such file or directory
root@redis-pod:/data# cd /data/vol01  1
bash: cd: /data/vol1: No such file or directory
root@redis-pod:/data# cd /data/vol111111111r1e1d1i1s1-1v1o1l101
root@redis-pod:/redis-vol01# ll
bash: ll: command not found
root@redis-pod:/redis-vol01# ls
file.txt
root@redis-pod:/redis-vol01# 
exit
[root@master01 ~]# kubectl delete odspods redis-pod
pod "redis-pod" deleted
[root@master01 ~]# vim kubectl create -f redis-pod-vol.yaml 
pod/redis-pod created
[root@master01 ~]# kubectl create -f redis-pod-vol.yaml delete pods redis-podexec -it redis-pod -- bash
root@redis-pod:/data# root@redis-pod:/data# cd /redis/vol01
bash: cd: /redis/vol01: No such file or directory
root@redis-pod:/data# cd /redis/vol01
bash: cd: /redis/vol01: No such file or directory
root@redis-pod:/data# cd /redis/vol01     
bash: cd: /redis/: No such file or directory
root@redis-pod:/data# df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  6.1G   41G  13% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root   47G  6.1G   41G  13% /etc/hosts
shm                   64M     0   64M   0% /dev/shm
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
root@redis-pod:/data# 
exit
[root@master01 ~]# kubectl exec -it redis-pod -- bashvim redis-pod-vol.yaml 
"redis-pod-vol.yaml" 19L, 326C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis-pod
  name: redis-pod
spec:
  containers:
  - image: redis
    name: redis-pod
    volumeMounts:
    - mountPath: /redis-vol01name: redis-vol01
  volumes:
  - name: redis-vol01
    emptyDir: {}
  restartPolicy: Always
status: {}
~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           ~                                                                                                                                                                                           1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   20~@k   1~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --13,29All3029876543210198/data/redis294,245,116,227,21^[  17,20All::Wq!wq!"redis-pod-vol.yaml" 19L, 325C written
[root@master01 ~]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
redis-pod   1/1     Running   0          77s
[root@master01 ~]# kubectl delete pod s redis-pod
pod "redis-pod" deleted
[root@master01 ~]# 
[root@master01 ~]# 
[root@master01 ~]# kubectl create -f redis-pod-vol.yaml 
pod/redis-pod created
[root@master01 ~]# kubectl create -f redis-pod-vol.yaml delete pods redis-podget podsvim redis-pod-vol.yaml kubectl exec -it redis-pod -- bash
root@redis-pod:/data# root@redis-pod:/data# pwd
/data
root@redis-pod:/data# l df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  6.1G   41G  13% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root   47G  6.1G   41G  13% /data
shm                   64M     0   64M   0% /dev/shm
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
root@redis-pod:/data# l cd /data/redis/
root@redis-pod:/data/redis# ll
bash: ll: command not found
root@redis-pod:/data/redis# ls
root@redis-pod:/data/redis# touch file.txt
root@redis-pod:/data/redis# echo "" > file.txt "Hello
root@redis-pod:/data/redis# ls
file.txt
root@redis-pod:/data/redis# 
exit
[root@master01 ~]# kubectl exec -it redis-pod -- bashcreate -f redis-pod-vol.yaml delete pods redis-podget podsdelete pods redis-pod
pod "redis-pod" deleted
[root@master01 ~]# kubectl delete pods redis-podexec -it redis-pod -- bashcreate -f redis-pod-vol.yaml exec -it redis-pod -- bashcreate -f redis-pod-vol.yaml 
pod/redis-pod created
[root@master01 ~]# kubectl create -f redis-pod-vol.yaml delete pods redis-podexec -it redis-pod -- bash
error: unable to upgrade connection: container not found ("redis-pod")
[root@master01 ~]# kubectl exec -it redis-pod -- bash
root@redis-pod:/data# root@redis-pod:/data# df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  6.1G   41G  13% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root   47G  6.1G   41G  13% /data
shm                   64M     0   64M   0% /dev/shm
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
root@redis-pod:/data# cd da  /data/redis/
root@redis-pod:/data/redis# ls
root@redis-pod:/data/redis# 
exit
[root@master01 ~]# logout
=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.20 12:52:03 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Authenticating with public key "rsa-key-20220112"
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Fri Sep 16 10:09:41 2022 from 192.168.200.249
[root@master01 ~]# [root@master01 ~]# kubectl exec -it redis-pod -- bashcat redis-pod-vol.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis-pod
  name: redis-pod
spec:
  containers:
  - image: redis
    name: redis-pod
    volumeMounts:
    - mountPath: /data/redis
      name: redis-vol01
  volumes:
  - name: redis-vol01
    emptyDir: {}    
  restartPolicy: Always
status: {}
[root@master01 ~]# cat redis-pod-vol.yaml kubectl exec -it redis-pod -- bash
root@redis-pod:/data# root@redis-pod:/data# ls
redis
root@redis-pod:/data# cd data/redis
bash: cd: data/redis: No such file or directory
root@redis-pod:/data# cd data/redis/
root@redis-pod:/data/redis# df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  6.1G   41G  13% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
/dev/mapper/rl-root   47G  6.1G   41G  13% /data
shm                   64M     0   64M   0% /dev/shm
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
root@redis-pod:/data/redis# ls
root@redis-pod:/data/redis# echo 'This is called the file' > file.txtroot@redis-pod:/data/redis# echo 'This is called the file' > file.txt
root@redis-pod:/data/redis# =~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.09.20 12:54:49 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
Authenticating with public key "rsa-key-20220112"
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Tue Sep 20 12:46:35 2022 from 192.168.200.31
[root@master01 ~]# kubectl exec -it redis-pod -- bash
root@redis-pod:/data# root@redis-pod:/data# cd /data/redis
root@redis-pod:/data/redis# ls
file.txt
root@redis-pod:/data/redis# cat file.txt 
This is called the file
root@redis-pod:/data/redis# 
exit
[root@master01 ~]# logout
c echo 'This is called the file' > file.txt
root@redis-pod:/data/redis# 
exit
[root@master01 ~]# kubectl delete pods redis-pod
pod "redis-pod" deleted
[root@master01 ~]# kubectl get pods
No resources found in default namespace.
[root@master01 ~]# cat task-pv.
task-pvc-volume.yaml  task-pv-volume.yaml   
[root@master01 ~]# cat task-pv-volume.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

[root@master01 ~]# ls
admin-pod.yaml     ds-kusc00201.yaml                multe-container.yaml  nginx-same.yaml    pod-redis.yaml           redis-pod-vol.yaml    webapp-replica.yaml
anaconda-ks.cfg    etcd-v3.3.13-linux-amd64         nfs-pv.yaml           nginx.yaml         pod-spec-KUCC00108.yaml  task-pvc-volume.yaml  web-project-268.yaml
busy-sidecar.yaml  etcd-v3.3.13-linux-amd64.tar.gz  nginx1.17.4.yaml      nodes.txt          policynetwork.yaml       task-pv-volume.yaml
date-job.yaml      hello.job.yaml                   nginxNode.yaml        non-root-pod.yaml  pv-analytics.yaml        webapp-deploy.yaml
[root@master01 ~]# cat task-pv.
task-pvc-volume.yaml  task-pv-volume.yaml   
[root@master01 ~]# cat task-pvkubectl kubectl get pvsc
No resources found in default namespace.
[root@master01 ~]# kubectl get pvc
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01   5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     6d22h
nfs-pv02   5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     6d22h
[root@master01 ~]# kubectl create -f task-pv-volume.yaml 
persistentvolume/task-pv-volume created
[root@master01 ~]# kubectl create -f task-pv-volume.yaml get pv
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS   REASON   AGE
nfs-pv01         5Gi        RWX            Recycle          Available   /nfs-pvc01   nfs                     6d22h
nfs-pv02         5Gi        RWX            Recycle          Available   /nfs-pvc02   nfs                     6d22h
task-pv-volume   10Gi       RWO            Retain           Available                manual                  3s
[root@master01 ~]# vim task-pv-pod.yaml
"task-pv-pod.yaml" [New File]  ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           0,0-1All::q![root@master01 ~]# kubectl crrun -d --name=nginxpox-pod --image=nginx --port 80 --restart=Never --dry-run=client -o yaml > task-pv.-pod.yaml
error: unknown shorthand flag: 'd' in -d
See 'kubectl run --help' for usage.
[root@master01 ~]# kubectl run -d --name=nginx-pod --image=nginx --port 80 --restart=Never --dry-run=client -o yaml > task-pv-pod.yaml create
error: unknown flag: --name
See 'kubectl create --help' for usage.
[root@master01 ~]# kubectl create --name=nginx-pod --image=nginx --port 80 --restart=Never --dry-run=client -o yaml > task-pv-pod.yamlrun -dvim task-pv-pod.yamlkubectl get pvcreate -f task-pv-volume.yaml get pvclscat task-pv-volume.yaml kubectl get podsdelete pods redis-podexec -it redis-pod -- bashcat redis-pod-vol.yaml kubectl exec -it redis-pod -- bashcreate -f redis-pod-vol.yaml delete pods redis-podexec -it redis-pod -- bashcreate -f redis-pod-vol.yaml delete pods redis-podget podsvim redis-pod-vol.yaml kubectl exec -it redis-pod -- bashcreate -f redis-pod-vol.yaml delete pods redis-podexec -it redis-pod -- bashdescribe pods redis-podredis-podwatch kubectl get podscreate -f redis-pod-vol.yaml mv redis-pod.yaml redis-pod-vol.yamlvim redis-pod.yaml kubectl run redis-pod --image=redis --dry-run=client -o yaml > redis-pod.yamlvim redis-pod.yaml kubectl run redis-pod --image=redis --dry-run=client -o yaml > redis-pod.yamlvim redis-pod.yaml mv redis-pod.yaml redis-pod-vol.yamlkubectl create -f redis-pod-vol.yaml get podswatch kubectl describe redis-podpods redis-podexec -it redis-pod -- bashdelete pods redis-podcreate -f redis-pod-vol.yaml exec -it redis-pod -- bashvim redis-pod-vol.yaml kubectl get podsdelete pods redis-podcreate -f redis-pod-vol.yaml exec -it redis-pod -- bashdelete pods redis-podcreate -f redis-pod-vol.yaml exec -it redis-pod -- bashcat redis-pod-vol.yaml kubectl exec -it redis-pod -- bashdelete pods redis-podget podscat task-pv-volume.yaml lskubectl get pvccreate -f task-pv-volume.yaml get pvvim task-pv-pod.yamlkubectl run -d --name=nginx-pod --image=nginx --port 80 --restart=Never --dry-run=client -o yaml >createkubectl create --name=nginx-pod --image=nginx --port 80 --restart=Never --dry-run=client -o yaml > task-pv-pod.yamlrun 
[root@master01 ~]# vim tsask-pv-
task-pv-pod.yaml     task-pv-volume.yaml  
[root@master01 ~]# vim task-pv-pod.yaml 
"task-pv-pod.yaml" 17L, 278C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  containers:
  - image: nginx
    name: nginx-pod
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           1,1All~@k   2~@k   3~@k   4~@k   5~@k   6~@k   7~@k   8~@k   9~@k   10,1~@k   1~@k   2~@k   3~@k   4~@k   2~@k   3~@k   4~@k   5~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9,5 ~@k   8~@k   7~@k   8o -- INSERT --9,9Top9,9All87654321volumes:
    - name: task-pv-storagepers(paste) --persistentVolumeClaim:claimName: task-pv-claim--a12,33All1,290,28^[  10,27All::wq!"task-pv-pod.yaml" 21L, 379C written
[root@master01 ~]# vim task-pv-pod.yaml 
"task-pv-pod.yaml" 21L, 379C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  volumes:
    - name: task-pv-storagepersistentVolumeClaim:claimName: task-pv-claim
  containers:
  - image: nginx
    name: nginx-pod
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           10,27Alli -- INSERT --10,27All87654321019876543task-pv-volume27^[  10,26Allu1 change; before #1  8 seconds agostorage10,27All~@k   1~@k   2~@k   8~@k   9~@k   30~@k   1~@k   2~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   i -- INSERT --12,32All321029876543210task-pv-volume343,14,175,206,117,2418,7All654321volumeMounts:- mountPath: "/us
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}(paste) --r/share/nginx/html"name: task-pv-storage
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}20,32All1918,20198765432109 8767volumeMounts:6volumeMounts:59896789- mountPath: "/usr/share/nginx/html"8- mountPath: "/usr/share/nginx/html"72089101name: task-pv-storage0name: task-pv-storage9 101234567892012345678930^[  20,29All::Wq!wq!"task-pv-pod.yaml" 24L, 471C written
[root@master01 ~]# vim task-pv-pod.yaml 
"task-pv-pod.yaml" 24L, 471C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  volumes:
    - name: task-pv-storagepersistentVolumeClaim:claimName: task-pv-volume
  containers:
  - image: nginx
    name: nginx-pod
    ports:
    - containerPort: 80
    volumeMounts:- mountPath: "/usr/share/nginx/html"name: task-pv-storage
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           20,29All~@k   19~@k   8,17~@k   7,23~@k   6,10~@k   5,19~@k   4,16~@k   3,13~@k   2,29~@k   1,28~@k   0,27~@k   9,10 ~@k   8,5 ~@k   7,17~@k   8,5 ~@k   9,10~@k   10,27~@k   1,28~@k   2,29~@k   3,13~@k   4,16~@k   5~@k   4~@k   3~@k   2~@k   1~@k   0~@k   9 ~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@k   1~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   ~@k   3~@k   2~@k   2~@k   1~@k   0~@k   9,2 ~@k   10,2~@k   9,2 ~@k   1~@k   2~@k   10,2~@k   1~@k   2~@k   3~@k   4i -- INSERT --14,2All3 - image: nginx4 - image: nginx55 name: nginx-pod6 name: nginx-pod7665 ports:6 ports:7765 - containerPort: 806 - containerPort: 807 - containerPort: 808 - containerPort: 80988765 volumeMounts:6 volumeMounts:79 - mountPath: "/usr/share/nginx/html"8 - mountPath: "/usr/share/nginx/html"920 name: task-pv-storage10 name: task-pv-storage1^[  20,10All^[  ^[  ::wq!"task-pv-pod.yaml" 24L, 487C written
[root@master01 ~]# kubectl create -f task-pv-pod.yaml 
error: error parsing task-pv-pod.yaml: error converting YAML to JSON: yaml: line 20: did not find expected '-' indicator
[root@master01 ~]# kubectl create -f task-pv-pod.yaml vim
"task-pv-pod.yaml" 24L, 487C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  volumes:
    - name: task-pv-storagepersistentVolumeClaim:claimName: task-pv-volume
  containers:
    - image: nginxname: nginx-podports:- containerPort: 80volumeMounts:- mountPath: "/usr/share/nginx/html"name: task-pv-storage
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           20,10All::2020,11All^[  ^[  ::Wq! 20,11All~@k   1~@k   0~@k   9 ~@k   8~@k   7~@k   6~@k   5i -- INSERT --21,5All resources: {}6 resources: {}7^[  21,6All::Wq!E492: Not an editor command: Wq!21,6All::wq!"task-pv-pod.yaml" 24L, 489C written
[root@master01 ~]# vim task-pv-pod.yaml kubectl create -f
pod/nginx-pod created
[root@master01 ~]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   0/1     Pending   0          4s
[root@master01 ~]# kubectl get podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Tue Sep 20 13:09:24 2022NAMEREADY   STATUS    RESTARTS   AGEnginx-pod   0/1     Pending   08s610s8230426486208240537597319351537597419310:0164865082104264866082204264977193315375978193415375908250426486100s821:004264861083115375972m1932153759711s9332648620824042648630825042759741932:0153759751931153863m82s2042648610s8230426486219341537597319351537597423:004264865082104264864m82s2053759711s9331537597219341648630825042648640824:004264975193115375975m1s9321537510s823042648620824042648631935153759741935:0153759751941264866m82s2042648610s8230427597219341537597319351538640826:004264865082104264867m1s932153759711s9331537597214042648630825042648640827:005375975193115375978m1s9321538610s823042648620824042648630835153759741938:0153759751941264869m82s2042648610s8230427597219341537597319351537540829:0042648650821042648610m82135793135794246850246820:0025791135719213683024684024795135791:013581024628202468313579413579524682:0024681035739213579313684024685024693:0135791135742024683024684035795135794:014681024658202479313579413585024685:0024691135769213579324684024685035796:013579113678202468302479413579513577:0024681024688213579313579414685024688:0024791135799213583024684024685135799:01357911462082024683024794135795135730:0024681024618203579313579414685024681:0024791135729213583024684024685135792:013579124638202468302579413579513683:0024681024648213579313579424685024684:0035791135759214683024684025795135795:013581024668202468313579413579524686:0024681025779213579313684024685024697:0135791135789224683024684035795135798:014681024698202479313579413575024689:002468103573092135793146840246850247940:0135791135712024683024684135795135791:024681024628202579313579413585024682:0024681135739213579324684024685025793:013579113648202468302479413579513584:0024681024658213579313579414685024685:0024791135769213573024684024685035796:013579113678202468302479413579513587:0024681024689213579313579424685024688:0035791135799213683024684024795135799:01358102464082024683135794135795146850:0024681025719213579313684024685024691:0135791135722024683024684035795135792:013681024638202479313579413575024683:0024681135749213579314684024685025794:013579113658202468302469413579513575:0024681024668203579313579414685024686:0024791135779213583024684024685135797:013579124688202468303579413579513688:0024681024799213579313574024685024689:00357911357509214683024684024795135794:00:013571024618202468313579413579514681:0024681025729[root@master01 ~]# watch kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   0/1     Pending   0          52m
[root@master01 ~]# kubectl get pods pods pods podss podse pods pods podsl podso podsg podss podsnginx-pod
[root@master01 ~]# vim /etc/task-pv
task-pvc-volume.yaml  task-pv-pod.yaml      task-pv-volume.yaml   
[root@master01 ~]# vim task-pv-pod.yaml 
"task-pv-pod.yaml" 24L, 489C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  volumes:
    - name: task-pv-storagepersistentVolumeClaim:claimName: task-pv-volume
  containers:
    - image: nginxname: nginx-podports:- containerPort: 80volumeMounts:- mountPath: "/usr/share/nginx/html"name: task-pv-storageresources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           21,6All::q[root@master01 ~]# kubectl create create -f task-pvc-volume.yaml 
persistentvolumeclaim/task-pv-claim created
[root@master01 ~]# kubectl get pvc
NAME            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim   Bound    task-pv-volume   10Gi       RWO            manual         6s
[root@master01 ~]# kubectl get pvccreate -f task-pvc-volume.yaml vim task-pv-pod
"task-pv-pod.yaml" 24L, 489C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  volumes:
    - name: task-pv-storagepersistentVolumeClaim:claimName: task-pv-volume
  containers:
    - image: nginxname: nginx-podports:- containerPort: 80volumeMounts:- mountPath: "/usr/share/nginx/html"name: task-pv-storageresources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           21,6All~@k   0~@k   19~@k   8~@k   7~@k   6~@k   5~@k   4~@k   3~@k   2~@K   33i -- INSERT --12,33All4321029876543210task-pv-claim33^[  12,32All::wq!"task-pv-pod.yaml" 24L, 488C written
[root@master01 ~]# kubectl get popods
NAME        READY   STATUS    RESTARTS   AGE
nginx-pod   0/1     Pending   0          54m
[root@master01 ~]# kubectl get podsdelete pods nginx-pod
pod "nginx-pod" deleted
[root@master01 ~]# kubectl delete pods nginx-podget podsvim task-pv-pod.yaml kubectl get pvccreate -f task-pvc-volume.yaml vim task-pv-podkubectl create -f task-pvc-volumevim task-pv-podkubectl logs nginx-podget podswatch create -f task-pv-pod.yaml 
pod/nginx-pod created
[root@master01 ~]# kubectl create -f task-pv-pod.yaml delete pods nginx-podget podsvim task-pv-pod.yaml kubectl get pvccreate -f task-pvc-volume.yaml vim task-pv-podkubectl logs nginx-podget podswatch 
Every 2.0s: kubectl get podsmaster01.airtel.internal.lab: Tue Sep 20 14:03:40 2022NAMEREADY   STATUS    RESTARTS   AGEnginx-pod   1/1     Running   05s27[root@master01 ~]# watch kubectl get podskubectl create -f task-pv-pod.yaml delete pods nginx-podget podsvim task-pv-pod.yaml 
"task-pv-pod.yaml" 24L, 488C  apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-pod
  name: nginx-pod
spec:
  volumes:
    - name: task-pv-storagepersistentVolumeClaim:claimName: task-pv-claim
  containers:
    - image: nginxname: nginx-podports:- containerPort: 80volumeMounts:- mountPath: "/usr/share/nginx/html"name: task-pv-storageresources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           ~                                                                                                                                                                           12,32All~@k   1,28~@k   0,27~@k   1,28~@k   2,32~@k   3,13~@k   2,32~@k   3,13~@k   4,18~@k   5,21~@k   6,12~@k   7,27~@k   8,19~@k   9,32::q[root@master01 ~]# kuvvecbeectl exec -it "nginx-pod"nginx-pod -- bash
root@nginx-pod:/# root@nginx-pod:/# df -h 
Filesystem           Size  Used Avail Use% Mounted on
overlay               47G  6.1G   41G  13% /
tmpfs                 64M     0   64M   0% /dev
tmpfs                3.8G     0  3.8G   0% /sys/fs/cgroup
shm                   64M     0   64M   0% /dev/shm
/dev/mapper/rl-root   47G  6.1G   41G  13% /etc/hosts
tmpfs                7.5G   12K  7.5G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                3.8G     0  3.8G   0% /proc/acpi
tmpfs                3.8G     0  3.8G   0% /proc/scsi
tmpfs                3.8G     0  3.8G   0% /sys/firmware
root@nginx-pod:/# cd /usr/share/nginx/html/
root@nginx-pod:/usr/share/nginx/html# ll
bash: ll: command not found
root@nginx-pod:/usr/share/nginx/html# ls 
root@nginx-pod:/usr/share/nginx/html# mount 
overlay on / type overlay (rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/45/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/44/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/43/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/42/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/41/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/40/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/908/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/908/work)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,size=65536k,mode=755)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666)
mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)
sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)
tmpfs on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (ro,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (ro,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/rdma type cgroup (ro,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (ro,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/blkio type cgroup (ro,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (ro,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/devices type cgroup (ro,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/cpuset type cgroup (ro,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/pids type cgroup (ro,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/freezer type cgroup (ro,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/perf_event type cgroup (ro,nosuid,nodev,noexec,relatime,perf_event)
shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k)
/dev/mapper/rl-root on /etc/hosts type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /dev/termination-log type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/hostname type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /etc/resolv.conf type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
/dev/mapper/rl-root on /usr/share/nginx/html type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota)
tmpfs on /run/secrets/kubernetes.io/serviceaccount type tmpfs (ro,relatime,size=7847236k)
proc on /proc/bus type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/fs type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/irq type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sys type proc (ro,nosuid,nodev,noexec,relatime)
proc on /proc/sysrq-trigger type proc (ro,nosuid,nodev,noexec,relatime)
tmpfs on /proc/acpi type tmpfs (ro,relatime)
tmpfs on /proc/kcore type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/keys type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/timer_list type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/sched_debug type tmpfs (rw,nosuid,size=65536k,mode=755)
tmpfs on /proc/scsi type tmpfs (ro,relatime)
tmpfs on /sys/firmware type tmpfs (ro,relatime)
root@nginx-pod:/usr/share/nginx/html# 
exit
[root@master01 ~]# [root@master01 ~]# kubectl get cm
NAME               DATA   AGE
kube-root-ca.crt   1      8d
[root@master01 ~]# kubectl create vmcm myconfigmap --from-literal=appname=myapp
configmap/myconfigmap created
[root@master01 ~]# kubectl get mcm
NAME               DATA   AGE
kube-root-ca.crt   1      8d
myconfigmap        1      5s
[root@master01 ~]# kubectl describe configm myconfigmap
Name:         myconfigmap
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
appname:
----
myapp

BinaryData
====

Events:  <none>
[root@master01 ~]# kubectl get configmaps myconfigmap --from-literal=appname=myapp -o yaml
error: unknown flag: --from-literal
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get configmaps myconfigmap --from-literal=appname=myapp -o yaml
apiVersion: v1
data:
  appname: myapp
kind: ConfigMap
metadata:
  creationTimestamp: "2022-09-20T08:36:50Z"
  name: myconfigmap
  namespace: default
  resourceVersion: "1063996"
  uid: 11f9b84a-2e57-484b-a067-baed15e30940
[root@master01 ~]# kubectl get configmaps myconfigmap -o yaml-from-literal=appname=myapp ---dry-run=client 
error: unknown flag: --from-literal
See 'kubectl get --help' for usage.
[root@master01 ~]# kubectl get configmaps myconfigmap --from-literal=appname=myapp --dry-run=client -o yamlkubectl get events
LAST SEEN   TYPE      REASON             OBJECT          MESSAGE
45m         Normal    Pulling            pod/nginx-pod   Pulling image "nginx"
45m         Normal    Pulled             pod/nginx-pod   Successfully pulled image "nginx" in 2.374457496s
45m         Normal    Created            pod/nginx-pod   Created container nginx-pod
45m         Normal    Started            pod/nginx-pod   Started container nginx-pod
38m         Warning   FailedScheduling   pod/nginx-pod   skip schedule deleting pod: default/nginx-pod
38m         Normal    Scheduled          pod/nginx-pod   Successfully assigned default/nginx-pod to worker01.airtel.internal.lab
[root@master01 ~]# 